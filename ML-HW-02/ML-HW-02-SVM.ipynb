{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7qnhGLbzRdD"
   },
   "source": [
    "# SVM for classification, without and with kernels\n",
    "\n",
    "In this notebook we are going to explore the use of Support Vector Machines (SVMs) for image classification. We are going to use the famous MNIST dataset, that is a dataset of handwritten digits. We get the data from mldata.org, that is a public repository for machine learning data.\n",
    "\n",
    "The dataset consists of 70,000 images of handwritten digits (i.e. 0, 1, ... 9). Each image is 28 pixels by 28 pixels and we can think of it as a vector of 28x28 = 784 numbers. Each number is an integer between 0 and 255. For each image we have the corresponding label (i.e. 0, 1, ..., 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1670443540836,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "TJWtY6mQzRdF"
   },
   "outputs": [],
   "source": [
    "#load the required packages\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1670443541308,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "D3CCzHV9zRdH"
   },
   "outputs": [],
   "source": [
    "#fix your ID (\"numero di matricola\") and the seed for random generator\n",
    "ID = 2082157\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1qdIDy7zRdH"
   },
   "source": [
    "Now let's load the dataset. 'data' contains the input, 'target' contains the label. We normalize the data by dividing each value by 255 so that each value is in [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 54719,
     "status": "ok",
     "timestamp": 1670443596024,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "VEPTLYprzRdH"
   },
   "outputs": [],
   "source": [
    "#load the MNIST dataset and let's normalize the features so that each value is in [0,1]\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "#rescale the data\n",
    "X, y = mnist.data.values / 255., mnist.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdHR-tUizRdI"
   },
   "source": [
    "Now split into training and test. We keep 500 samples in the training set. Make sure that each label is present at least 10 times\n",
    "in training. If it is not, then keep adding permutations to the initial data until this \n",
    "happens.\n",
    "\n",
    "**IMPORTANT**: if you cannot run the SVM with 500 samples or 1000 samples (see below), try with a smaller number of samples (e.g. 200 here and 400 below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2658,
     "status": "ok",
     "timestamp": 1670443598654,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "LOwp2KtzzRdI",
    "outputId": "0636e01a-c079-4d12-818b-7aef79510a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels and frequencies in training dataset: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object),\n",
       " array([45, 55, 52, 54, 50, 51, 47, 45, 49, 52], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random permute the data and split into training and test taking the first 500 data samples as training and the rest as test\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 500\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "print(\"Labels and frequencies in training dataset: \")\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kphf1byFzRdJ"
   },
   "source": [
    "We now provide a function to print an image in the dataset and the corresponding true label given the index of the image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1670443598654,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "bfuT-7ANzRdJ"
   },
   "outputs": [],
   "source": [
    "#function for plotting a digit and printing the corresponding label\n",
    "def plot_digit(X_matrix, labels, index):\n",
    "    print(\"INPUT:\")\n",
    "    plt.imshow(\n",
    "        X_matrix[index].reshape(28, 28),\n",
    "        cmap = plt.cm.gray_r,\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(\"LABEL: %s\" % labels[index])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0uSwgHGzRdJ"
   },
   "source": [
    "As an example, let's print the 100-th image in X_train and the 40,000-th image in X_test and their true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670443598655,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "zrjDDOg3zRdK",
    "outputId": "51108a11-80b1-4682-a5ee-afc7732378a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 2\n",
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 1\n"
     ]
    }
   ],
   "source": [
    "#let's try the plotting function\n",
    "plot_digit(X_train, y_train, 100)\n",
    "plot_digit(X_test, y_test, 40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf5UaApWzRdK"
   },
   "source": [
    "## TO DO 1\n",
    "Run SVM with 5-fold cross validation to pick the best kernel and values of parameters. We provide some potential choice for parameters, but change the grid if needed (e.g. it takes too long). For the SVM for classification use SVC from sklearn.svm; for the grid search we suggest you use GridSearchCV from sklearn.model_selection, but you can implement your own cross-validation for model selection if you prefer.\n",
    "\n",
    "Finally, print the best parameters used as well as the score obtained by the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6568,
     "status": "ok",
     "timestamp": 1670443605218,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "Ioaltt6vzRdK",
    "outputId": "57aa1d4d-c213-4194-f406-9f5a0f588506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator for SVM linear kernel: SVC(C=1, kernel='linear')\n",
      "\n",
      "RESULTS FOR LINEAR KERNEL:\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 1}\n",
      "\n",
      "Score with best parameters:\n",
      "0.842\n",
      "\n",
      "All scores on the grid:\n",
      "Parameters: [{'C': 1}, {'C': 10}, {'C': 100}]\n",
      "Scores: [0.842 0.842 0.842]\n",
      "\n",
      "Best estimator for SVM poly-2 kernel: SVC(C=10, degree=2, gamma=0.01, kernel='poly')\n",
      "\n",
      "RESULTS FOR POLY DEGREE=2 KERNEL:\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 10, 'gamma': 0.01}\n",
      "\n",
      "Score with best parameters:\n",
      "0.8540000000000001\n",
      "\n",
      "All scores on the grid:\n",
      "Parameters: [{'C': 1, 'gamma': 0.01}, {'C': 1, 'gamma': 0.1}, {'C': 1, 'gamma': 1.0}, {'C': 10, 'gamma': 0.01}, {'C': 10, 'gamma': 0.1}, {'C': 10, 'gamma': 1.0}, {'C': 100, 'gamma': 0.01}, {'C': 100, 'gamma': 0.1}, {'C': 100, 'gamma': 1.0}]\n",
      "Scores: [0.832 0.852 0.852 0.854 0.852 0.852 0.852 0.852 0.852]\n",
      "\n",
      "Best estimator for SVM rbf kernel: SVC(C=10, gamma=0.01)\n",
      "\n",
      "RESULTS FOR rbf KERNEL:\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 10, 'gamma': 0.01}\n",
      "\n",
      "Score with best parameters:\n",
      "0.8800000000000001\n",
      "\n",
      "All scores on the grid:\n",
      "Parameters: [{'C': 1, 'gamma': 0.01}, {'C': 1, 'gamma': 0.1}, {'C': 1, 'gamma': 1.0}, {'C': 10, 'gamma': 0.01}, {'C': 10, 'gamma': 0.1}, {'C': 10, 'gamma': 1.0}, {'C': 100, 'gamma': 0.01}, {'C': 100, 'gamma': 0.1}, {'C': 100, 'gamma': 1.0}]\n",
      "Scores: [0.854 0.536 0.112 0.88  0.574 0.112 0.88  0.574 0.112]\n"
     ]
    }
   ],
   "source": [
    "#import SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#import for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#parameters for linear kernel\n",
    "linear_parameters = {'C': [1, 10, 100]}\n",
    "\n",
    "#run linear SVM\n",
    "linear_SVM = SVC(kernel='linear')\n",
    "\n",
    "#find best model using 5-fold CV and train it using all the training data\n",
    "grid_search_CV_linear = GridSearchCV(linear_SVM, linear_parameters, cv=5, return_train_score=False)\n",
    "\n",
    "#fit the grid search with training data\n",
    "grid_search_CV_linear.fit(X_train, y_train)\n",
    "\n",
    "#best SVM model: linear kernel\n",
    "best_linear_SVM = grid_search_CV_linear.best_estimator_\n",
    "print(\"Best estimator for SVM linear kernel: \"+str(best_linear_SVM))\n",
    "\n",
    "print('\\nRESULTS FOR LINEAR KERNEL:\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(grid_search_CV_linear.best_params_)\n",
    "\n",
    "print(\"\\nScore with best parameters:\")\n",
    "print(grid_search_CV_linear.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(\"Parameters: \"+str(grid_search_CV_linear.cv_results_['params']))\n",
    "print(\"Scores: \"+str(grid_search_CV_linear.cv_results_['mean_test_score']))\n",
    "\n",
    "#parameters for poly with degree 2 kernel\n",
    "poly2_parameters = {'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1.]}\n",
    "\n",
    "#run SVM with poly of degree 2 kernel\n",
    "poly2_SVM = SVC(kernel='poly', degree=2)\n",
    "\n",
    "#DO THE SAME AS ABOVE FOR POLYNOMIAL KERNEL WITH DEGREE=2\n",
    "#find best model using 5-fold CV and train it using all the training data\n",
    "grid_search_CV_poly = GridSearchCV(poly2_SVM, poly2_parameters, cv=5, return_train_score=False)\n",
    "\n",
    "#fit grid search with training data\n",
    "grid_search_CV_poly.fit(X_train, y_train)\n",
    "\n",
    "#best SVM model: poly-2 kernel\n",
    "best_poly2_SVM = grid_search_CV_poly.best_estimator_\n",
    "print(\"\\nBest estimator for SVM poly-2 kernel: \"+str(best_poly2_SVM))\n",
    "\n",
    "print('\\nRESULTS FOR POLY DEGREE=2 KERNEL:\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(grid_search_CV_poly.best_params_)\n",
    "\n",
    "print(\"\\nScore with best parameters:\")\n",
    "print(grid_search_CV_poly.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(\"Parameters: \"+str(grid_search_CV_poly.cv_results_['params']))\n",
    "print(\"Scores: \"+str(grid_search_CV_poly.cv_results_['mean_test_score']))\n",
    "\n",
    "#parameters for rbf kernel\n",
    "rbf_parameters = {'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1.]}\n",
    "\n",
    "#run SVM with rbf kernel\n",
    "rbf_SVM = SVC(kernel='rbf')\n",
    "\n",
    "#DO THE SAME AS ABOVE FOR RBF KERNEL\n",
    "#find best model using 5-fold CV and train it using all the training data\n",
    "grid_search_CV_rbf = GridSearchCV(rbf_SVM, rbf_parameters, cv=5, return_train_score=False)\n",
    "\n",
    "#fit grid search with training data\n",
    "grid_search_CV_rbf.fit(X_train, y_train)\n",
    "\n",
    "#best SVM model: rbf kernel\n",
    "best_rbf_SVM = grid_search_CV_rbf.best_estimator_\n",
    "print(\"\\nBest estimator for SVM rbf kernel: \"+str(best_rbf_SVM))\n",
    "\n",
    "print('\\nRESULTS FOR rbf KERNEL:\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(grid_search_CV_rbf.best_params_)\n",
    "\n",
    "print(\"\\nScore with best parameters:\")\n",
    "print(grid_search_CV_rbf.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(\"Parameters: \"+str(grid_search_CV_rbf.cv_results_['params']))\n",
    "print(\"Scores: \"+str(grid_search_CV_rbf.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb2kQJYzRdL"
   },
   "source": [
    "## TO DO 2\n",
    "For the \"best\" SVM kernel and choice of parameters from above, train the model on the entire training set and measure the training error. Also make predictions on the test set and measure the test error. Print the training and the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15459,
     "status": "ok",
     "timestamp": 1670443620667,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "U2zvw537zRdL",
    "outputId": "64e7872f-ae4d-46ed-da36-5829b7d7c657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM training error: 0.000000\n",
      "Best SVM test error: 0.099007\n"
     ]
    }
   ],
   "source": [
    "#get training and test error for the best SVM model from CV\n",
    "\n",
    "#as 'best' SVM model, I consider the one with the highest score from the previous results:\n",
    "#SVM rbf kernel with parameters: C=10, gamma=0.01\n",
    "best_SVM = grid_search_CV_rbf.best_estimator_\n",
    "\n",
    "#fit the model on the entire training set\n",
    "best_SVM.fit(X_train, y_train)\n",
    "\n",
    "#get the training and test error\n",
    "training_error_best_SVM = 1. - best_SVM.score(X_train, y_train)\n",
    "test_error_best_SVM = 1. - best_SVM.score(X_test, y_test)\n",
    "\n",
    "print(\"Best SVM training error: %f\" % training_error_best_SVM)\n",
    "print(\"Best SVM test error: %f\" % test_error_best_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVj7-ZXIzRdL"
   },
   "source": [
    "## Use logistic regression for comparison\n",
    "\n",
    "## TO DO 3\n",
    "\n",
    "Just for comparison let's also use logistic regression, first with the default values of the parameter for regularization and then with cross-validation to fix the value of the parameters. For cross validation, use 5-fold cross validation and the default values of the regularization parameters for the function linear_model.LogisticRegressionCV(...).\n",
    "\n",
    "Note: during training you may receive a \"ConvergenceWarning\" that indicates that the logistic regression solver did not converge to the optimal result. Given the scope of the notebook, we can ignore such warning but in real-world scenarios you should take corrective measures such as increasing the number of training iterations and/or the runtime for training or picking a different optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16036,
     "status": "ok",
     "timestamp": 1670443636690,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "j1F-hZZPzRdL",
    "outputId": "4b7476bc-16d2-45ef-be5d-0772347300f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression training error: 0.000000\n",
      "Best logistic regression test error: 0.147094\n",
      "\n",
      "Best logistic regression training error with CV: 0.002000\n",
      "Best logistic regression test error with CV: 0.147482\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "#simple filter to ignore ConvergenceWarning (introduced only for 'printing' purposes)\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "#logistic regression model from sklearn.linear_model\n",
    "log_regression = linear_model.LogisticRegression()\n",
    "\n",
    "#fit the logistic regression model on the training data\n",
    "log_regression.fit(X_train, y_train)\n",
    "\n",
    "#compute training and test error for model above\n",
    "training_error_log_regression = 1. - log_regression.score(X_train, y_train)\n",
    "test_error_log_regression = 1. - log_regression.score(X_test, y_test)\n",
    "\n",
    "print(\"Best logistic regression training error: %f\" % training_error_log_regression)\n",
    "print(\"Best logistic regression test error: %f\" % test_error_log_regression)\n",
    "\n",
    "#logistic regression with 5-fold CV: you can use use linear_model.LogisticRegressionCV\n",
    "#use 5-fold CV to find the best choice of the parameter, than train the model on the entire training set\n",
    "log_regression_cv = linear_model.LogisticRegressionCV(cv=5, random_state=ID).fit(X_train, y_train)\n",
    "training_error_log_regression_cv = 1. - log_regression_cv.score(X_train, y_train)\n",
    "test_error_log_regression_cv = 1. - log_regression_cv.score(X_test, y_test)\n",
    "\n",
    "print(\"\\nBest logistic regression training error with CV: %f\" % training_error_log_regression_cv)\n",
    "print(\"Best logistic regression test error with CV: %f\" % test_error_log_regression_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pNmjembzRdM"
   },
   "source": [
    "## TO DO 4 \n",
    "Compare and comment the results from SVM and logistic regression.\n",
    "\n",
    "**ANSWER [David Polzoni]:** The following code block shows the performances (in terms of training error and test error) of the different SVM models with linear, poly-2 and rbf kernels. The result shown below is further confirmation of the correct choice of the *best_SVM* as SVM with rbf kernel and parameters $C=10$, $γ=0.01$. As for logistic regression, similar performances are obtained between *log_regression* and *log_regression_cv*, as can be seen below. Comparing *best_SVM* and logistic regression models performances, better results in terms of test error are obtained by *best_SVM*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12816,
     "status": "ok",
     "timestamp": 1670443649494,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "deUxUx8FNzOd",
    "outputId": "c3713b73-ac9b-423b-8d81-228a34a721d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM linear kernel training error: 0.000000\n",
      "Best SVM linear kernel test error: 0.130014\n",
      "\n",
      "Best SVM poly-2 kernel training error: 0.000000\n",
      "Best SVM poly-2 kernel test error: 0.116820\n",
      "\n",
      "Best SVM (rbf kernel) training error: 0.000000\n",
      "Best SVM (rbf kernel) test error: 0.099007\n",
      "\n",
      "Best logistic regression training error: 0.000000\n",
      "Best logistic regression test error: 0.147094\n",
      "\n",
      "Best logistic regression training error with CV: 0.002000\n",
      "Best logistic regression test error with CV: 0.147482\n"
     ]
    }
   ],
   "source": [
    "#comparison between performances of different SVM: linear, poly-2, rbf kernel\n",
    "\n",
    "#SVM: linear kernel --> best_linear_SVM (defined above)\n",
    "training_error_best_linear_SVM = 1. - best_linear_SVM.score(X_train, y_train)\n",
    "test_error_best_linear_SVM = 1. - best_linear_SVM.score(X_test, y_test)\n",
    "print(\"Best SVM linear kernel training error: %f\" % training_error_best_linear_SVM)\n",
    "print(\"Best SVM linear kernel test error: %f\" % test_error_best_linear_SVM)\n",
    "\n",
    "#SVM: poly-2 kernel --> best_poly2_SVM (defined above)\n",
    "training_error_best_poly2_SVM = 1. - best_poly2_SVM.score(X_train, y_train)\n",
    "test_error_best_poly2_SVM = 1. - best_poly2_SVM.score(X_test, y_test)\n",
    "print(\"\\nBest SVM poly-2 kernel training error: %f\" % training_error_best_poly2_SVM)\n",
    "print(\"Best SVM poly-2 kernel test error: %f\" % test_error_best_poly2_SVM)\n",
    "\n",
    "#SVM: rbf kernel --> best_rbf_SVM (defined above) = best_SVM, as it has the highest score\n",
    "print(\"\\nBest SVM (rbf kernel) training error: %f\" % training_error_best_SVM)\n",
    "print(\"Best SVM (rbf kernel) test error: %f\" % test_error_best_SVM)\n",
    "\n",
    "#comparison between performances: log_regression and log_regression_cv\n",
    "\n",
    "#logistic regression\n",
    "print(\"\\nBest logistic regression training error: %f\" % training_error_log_regression)\n",
    "print(\"Best logistic regression test error: %f\" % test_error_log_regression)\n",
    "\n",
    "#logistic regression with cross-validation\n",
    "print(\"\\nBest logistic regression training error with CV: %f\" % training_error_log_regression_cv)\n",
    "print(\"Best logistic regression test error with CV: %f\" % test_error_log_regression_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3v44JFhEzRdM"
   },
   "source": [
    "## TO DO 5\n",
    "Write the code that finds and plots a digit that is misclassified by logistic regression (optimized for the regularization parameter) and correctly classified by the \"best\" SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 15300,
     "status": "ok",
     "timestamp": 1670443664785,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "M8EVvD-MzRdM",
    "outputId": "39aa3c59-ef75-4a3c-e8f9-e8f89860c9ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following digit is correctly classified by the 'best' SVM and misclassified by logistic regression with CV\n",
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 9\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#to find the digits which are misclassified by log_regression_cv:\n",
    "#use np.where() to check for predictions which are NOT equal to the labels previously defined\n",
    "misclassified_log_reg_cv = np.where(y_test != log_regression_cv.predict(X_test))\n",
    "\n",
    "#to find the digits which are correctly classified by best_SVM:\n",
    "#use np.where() to check for predictions which are equal to the labels previously defined\n",
    "classified_best_SVM = np.where(y_test == best_SVM.predict(X_test))\n",
    "\n",
    "#to find the digits which respect the conditions imposed on TO DO 5:\n",
    "#use np.intesects1d() to get the digits indexes which are misclassified by log_regression_cv and classified by best_SVM\n",
    "correct_digits = np.intersect1d(misclassified_log_reg_cv, classified_best_SVM)\n",
    "\n",
    "#randomly choose an index in correct_digits defined above\n",
    "rnd_selected_digit = random.choice(correct_digits)\n",
    "\n",
    "#control to test if the chosen digit is classified by the best_SVM and misclassified by logistic_regression_cv\n",
    "#to test if all the procedure is correct: use reshape(1, -1) to compare the predictions on rnd_selected_digit with the corresponding y_test label\n",
    "if y_test[rnd_selected_digit] == best_SVM.predict(X_test[rnd_selected_digit].reshape(1, -1)) and y_test[rnd_selected_digit] != log_regression_cv.predict(X_test[rnd_selected_digit].reshape(1, -1)):\n",
    "  print(\"The following digit is correctly classified by the 'best' SVM and misclassified by logistic regression with CV\")\n",
    "else:\n",
    "  print(\"Error: the following digit is not among the correct ones!\")\n",
    "\n",
    "#plot the rnd_selected_digit\n",
    "plot_digit(X_test, y_test, rnd_selected_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooo_YQ_rzRdM"
   },
   "source": [
    "## More data\n",
    "Now let's do the same but using 1000 data points for training. \n",
    "\n",
    "## TO DO 6\n",
    "Repeat the entire analysis above using 1000 samples. Of course you can copy the code from above (but no need to copy markdown comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 98617,
     "status": "ok",
     "timestamp": 1670443763374,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "ZnDimQ6o7FPt",
    "outputId": "6209e59d-37ee-4b3b-a531-3c45d429047e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator for SVM linear kernel: SVC(C=1, kernel='linear')\n",
      "\n",
      "RESULTS FOR LINEAR KERNEL:\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 1}\n",
      "\n",
      "Score with best parameters:\n",
      "0.8790000000000001\n",
      "\n",
      "All scores on the grid:\n",
      "Parameters: [{'C': 1}, {'C': 10}, {'C': 100}]\n",
      "Scores: [0.879 0.879 0.879]\n",
      "\n",
      "Best estimator for SVM poly-2 kernel: SVC(C=10, degree=2, gamma=0.01, kernel='poly')\n",
      "\n",
      "RESULTS FOR POLY DEGREE=2 KERNEL:\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 10, 'gamma': 0.01}\n",
      "\n",
      "Score with best parameters:\n",
      "0.899\n",
      "\n",
      "All scores on the grid:\n",
      "Parameters: [{'C': 1, 'gamma': 0.01}, {'C': 1, 'gamma': 0.1}, {'C': 1, 'gamma': 1.0}, {'C': 10, 'gamma': 0.01}, {'C': 10, 'gamma': 0.1}, {'C': 10, 'gamma': 1.0}, {'C': 100, 'gamma': 0.01}, {'C': 100, 'gamma': 0.1}, {'C': 100, 'gamma': 1.0}]\n",
      "Scores: [0.878 0.898 0.898 0.899 0.898 0.898 0.898 0.898 0.898]\n",
      "\n",
      "Best estimator for SVM rbf kernel: SVC(C=100, gamma=0.01)\n",
      "\n",
      "RESULTS FOR rbf KERNEL:\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 100, 'gamma': 0.01}\n",
      "\n",
      "Score with best parameters:\n",
      "0.915\n",
      "\n",
      "All scores on the grid:\n",
      "Parameters: [{'C': 1, 'gamma': 0.01}, {'C': 1, 'gamma': 0.1}, {'C': 1, 'gamma': 1.0}, {'C': 10, 'gamma': 0.01}, {'C': 10, 'gamma': 0.1}, {'C': 10, 'gamma': 1.0}, {'C': 100, 'gamma': 0.01}, {'C': 100, 'gamma': 0.1}, {'C': 100, 'gamma': 1.0}]\n",
      "Scores: [0.904 0.509 0.118 0.914 0.56  0.118 0.915 0.56  0.118]\n",
      "\n",
      "Best SVM training error: 0.000000\n",
      "Best SVM test error: 0.078420\n",
      "\n",
      "Best logistic regression training error: 0.000000\n",
      "Best logistic regression test error: 0.127696\n",
      "Best logistic regression training error with CV: 0.002000\n",
      "Best logistic regression test error with CV: 0.126942\n",
      "\n",
      "The following digit is correctly classified by the 'best' SVM and misclassified by logistic regression with CV\n",
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 1\n"
     ]
    }
   ],
   "source": [
    "#split into training and test taking the first 1000 data samples as training and the rest as test\n",
    "m_training = 1000\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "#SVM: linear, poly-2 and gaussian rbf kernels (GridSearchCV)\n",
    "\n",
    "#parameters for linear kernel\n",
    "linear_parameters = {'C': [1, 10, 100]}\n",
    "\n",
    "#run linear SVM\n",
    "linear_SVM = SVC(kernel='linear')\n",
    "\n",
    "#find best model using 5-fold CV and train it using all the training data\n",
    "grid_search_CV_linear = GridSearchCV(linear_SVM, linear_parameters, cv=5, return_train_score=False)\n",
    "\n",
    "#fit the grid search with training data\n",
    "grid_search_CV_linear.fit(X_train, y_train)\n",
    "\n",
    "#best SVM model: linear kernel\n",
    "best_linear_SVM = grid_search_CV_linear.best_estimator_\n",
    "print(\"Best estimator for SVM linear kernel: \"+str(best_linear_SVM))\n",
    "\n",
    "print('\\nRESULTS FOR LINEAR KERNEL:\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(grid_search_CV_linear.best_params_)\n",
    "\n",
    "print(\"\\nScore with best parameters:\")\n",
    "print(grid_search_CV_linear.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(\"Parameters: \"+str(grid_search_CV_linear.cv_results_['params']))\n",
    "print(\"Scores: \"+str(grid_search_CV_linear.cv_results_['mean_test_score']))\n",
    "\n",
    "#parameters for poly with degree 2 kernel\n",
    "poly2_parameters = {'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1.]}\n",
    "\n",
    "#run SVM with poly of degree 2 kernel\n",
    "poly2_SVM = SVC(kernel='poly', degree=2)\n",
    "\n",
    "#DO THE SAME AS ABOVE FOR POLYNOMIAL KERNEL WITH DEGREE=2\n",
    "#find best model using 5-fold CV and train it using all the training data\n",
    "grid_search_CV_poly = GridSearchCV(poly2_SVM, poly2_parameters, cv=5, return_train_score=False)\n",
    "\n",
    "#fit grid search with training data\n",
    "grid_search_CV_poly.fit(X_train, y_train)\n",
    "\n",
    "#best SVM model: poly-2 kernel\n",
    "best_poly2_SVM = grid_search_CV_poly.best_estimator_\n",
    "print(\"\\nBest estimator for SVM poly-2 kernel: \"+str(best_poly2_SVM))\n",
    "\n",
    "print('\\nRESULTS FOR POLY DEGREE=2 KERNEL:\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(grid_search_CV_poly.best_params_)\n",
    "\n",
    "print(\"\\nScore with best parameters:\")\n",
    "print(grid_search_CV_poly.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(\"Parameters: \"+str(grid_search_CV_poly.cv_results_['params']))\n",
    "print(\"Scores: \"+str(grid_search_CV_poly.cv_results_['mean_test_score']))\n",
    "\n",
    "#parameters for rbf kernel\n",
    "rbf_parameters = {'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1.]}\n",
    "\n",
    "#run SVM with rbf kernel\n",
    "rbf_SVM = SVC(kernel='rbf')\n",
    "\n",
    "#DO THE SAME AS ABOVE FOR RBF KERNEL\n",
    "#find best model using 5-fold CV and train it using all the training data\n",
    "grid_search_CV_rbf = GridSearchCV(rbf_SVM, rbf_parameters, cv=5, return_train_score=False)\n",
    "\n",
    "#fit grid search with training data\n",
    "grid_search_CV_rbf.fit(X_train, y_train)\n",
    "\n",
    "#best SVM model: rbf kernel\n",
    "best_rbf_SVM = grid_search_CV_rbf.best_estimator_\n",
    "print(\"\\nBest estimator for SVM rbf kernel: \"+str(best_rbf_SVM))\n",
    "\n",
    "print('\\nRESULTS FOR rbf KERNEL:\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(grid_search_CV_rbf.best_params_)\n",
    "\n",
    "print(\"\\nScore with best parameters:\")\n",
    "print(grid_search_CV_rbf.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(\"Parameters: \"+str(grid_search_CV_rbf.cv_results_['params']))\n",
    "print(\"Scores: \"+str(grid_search_CV_rbf.cv_results_['mean_test_score']))\n",
    "\n",
    "#choice of the best model in terms of best_score_ parameter \n",
    "\n",
    "#as 'best' SVM model, I consider the one with the highest score from the previous results:\n",
    "#SVM rbf kernel with parameters: C=100, gamma=0.01 (different from above!)\n",
    "best_SVM = grid_search_CV_rbf.best_estimator_\n",
    "\n",
    "#fit the model on the entire training set\n",
    "best_SVM.fit(X_train, y_train)\n",
    "\n",
    "#get the training and test error\n",
    "training_error_best_SVM_1000 = 1. - best_SVM.score(X_train, y_train)\n",
    "test_error_best_SVM_1000 = 1. - best_SVM.score(X_test, y_test)\n",
    "\n",
    "print(\"\\nBest SVM training error: %f\" % training_error_best_SVM_1000)\n",
    "print(\"Best SVM test error: %f\" % test_error_best_SVM_1000)\n",
    "\n",
    "#logistic regression analysis: optimization of hyperparameters with CV\n",
    "\n",
    "#logistic regression model from sklearn.linear_model\n",
    "log_regression = linear_model.LogisticRegression()\n",
    "\n",
    "#fit the logistic regression model on the training data\n",
    "log_regression.fit(X_train, y_train)\n",
    "\n",
    "#compute training and test error for model above\n",
    "training_error_log_regression_1000 = 1. - log_regression.score(X_train, y_train)\n",
    "test_error_log_regression_1000 = 1. - log_regression.score(X_test, y_test)\n",
    "\n",
    "print(\"\\nBest logistic regression training error: %f\" % training_error_log_regression_1000)\n",
    "print(\"Best logistic regression test error: %f\" % test_error_log_regression_1000)\n",
    "\n",
    "#logistic regression with 5-fold CV: you can use use linear_model.LogisticRegressionCV\n",
    "#use 5-fold CV to find the best choice of the parameter, than train the model on the entire training set\n",
    "log_regression_cv = linear_model.LogisticRegressionCV(cv=5, random_state=ID).fit(X_train, y_train)\n",
    "training_error_log_regression_cv_1000 = 1. - log_regression_cv.score(X_train, y_train)\n",
    "test_error_log_regression_cv_1000 = 1. - log_regression_cv.score(X_test, y_test)\n",
    "\n",
    "print(\"Best logistic regression training error with CV: %f\" % training_error_log_regression_cv_1000)\n",
    "print(\"Best logistic regression test error with CV: %f\" % test_error_log_regression_cv_1000)\n",
    "\n",
    "#find and plot a digit that is misclassified by logistic regression (optimized for the regularization parameter)\n",
    "#and correctly classified by the 'best' SVM\n",
    "\n",
    "#to find the digits which are misclassified by log_regression_cv:\n",
    "#use np.where() to check for predictions which are NOT equal to the labels previously defined\n",
    "misclassified_log_reg_cv = np.where(y_test != log_regression_cv.predict(X_test))\n",
    "\n",
    "#to find the digits which are correctly classified by best_SVM:\n",
    "#use np.where() to check for predictions which are equal to the labels previously defined\n",
    "classified_best_SVM = np.where(y_test == best_SVM.predict(X_test))\n",
    "\n",
    "#to find the digits which respect the conditions imposed on TO DO 5:\n",
    "#use np.intesects1d() to get the digits indexes which are misclassified by log_regression_cv and classified by best_SVM\n",
    "correct_digits = np.intersect1d(misclassified_log_reg_cv, classified_best_SVM)\n",
    "\n",
    "#randomly choose an index in correct_digits defined above\n",
    "rnd_selected_digit = random.choice(correct_digits)\n",
    "\n",
    "#control to test if the chosen digit is classified by the best_SVM and misclassified by logistic_regression_cv\n",
    "#to test if all the procedure is correct: use reshape(1, -1) to compare the predictions on rnd_selected_digit with the corresponding y_test label\n",
    "if y_test[rnd_selected_digit] == best_SVM.predict(X_test[rnd_selected_digit].reshape(1, -1)) and y_test[rnd_selected_digit] != log_regression_cv.predict(X_test[rnd_selected_digit].reshape(1, -1)):\n",
    "  print(\"\\nThe following digit is correctly classified by the 'best' SVM and misclassified by logistic regression with CV\")\n",
    "else:\n",
    "  print(\"\\nError: the following digit is not among the correct ones!\")\n",
    "\n",
    "#plot the rnd_selected_digit\n",
    "plot_digit(X_test, y_test, rnd_selected_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYnT0FuizRdN"
   },
   "source": [
    "## TO DO 7\n",
    "Compare and comment on the differences with the results above.\n",
    "\n",
    "**ANSWER [David Polzoni]**: With the increase of training samples ($m_{training}=1000$), the following advantages have been obtained, as expected:\n",
    "\n",
    "* GridSearchCV: higher scores in terms of parameters evaluation;\n",
    "* different set of parameters for the *best_SVM*;\n",
    "* lower training and test error for *best_SVM*;\n",
    "* lower training and test error for logistic regression model with and without CV;\n",
    "* logistic regression: similar performances with and without CV;\n",
    "* better performances in terms of test error for *best_SVM* w.r.t. logistic regression models.\n",
    "\n",
    "In fact, the larger amount of training samples results in better performances for models that are trained with a larger set of training data. The results listed above are shown in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1670443763375,
     "user": {
      "displayName": "David Polzoni",
      "userId": "09670843453542960777"
     },
     "user_tz": -60
    },
    "id": "YImm-uFDzRdN",
    "outputId": "849ef5df-3793-49b4-c879-160032ccb0a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM (m_training = 1000): SVC(C=100, gamma=0.01)\n",
      "\n",
      "Best SVM training error (m_training = 500): 0.000000\n",
      "Best SVM test error (m_training = 500): 0.099007\n",
      "\n",
      "Best SVM training error (m_training = 1000): 0.000000\n",
      "Best SVM test error (m_training = 1000): 0.078420\n",
      "\n",
      "Best logistic regression training error (m_training = 500): 0.000000\n",
      "Best logistic regression test error (m_training = 500): 0.147094\n",
      "Best logistic regression training error with CV (m_training = 500): 0.002000\n",
      "Best logistic regression test error with CV (m_training = 500): 0.147482\n",
      "\n",
      "Best logistic regression training error (m_training = 1000): 0.000000\n",
      "Best logistic regression test error (m_training = 1000): 0.127696\n",
      "Best logistic regression training error with CV (m_training = 1000): 0.002000\n",
      "Best logistic regression test error with CV (m_training = 1000): 0.126942\n"
     ]
    }
   ],
   "source": [
    "#different set of parameters for best SVM: m_training = 1000\n",
    "print(\"Best SVM (m_training = 1000): \"+str(best_SVM))\n",
    "\n",
    "#comparison between performances of best SVMs: m_training = 500 and m_training = 1000\n",
    "print(\"\\nBest SVM training error (m_training = 500): %f\" % training_error_best_SVM)\n",
    "print(\"Best SVM test error (m_training = 500): %f\" % test_error_best_SVM)\n",
    "\n",
    "print(\"\\nBest SVM training error (m_training = 1000): %f\" % training_error_best_SVM_1000)\n",
    "print(\"Best SVM test error (m_training = 1000): %f\" % test_error_best_SVM_1000)\n",
    "\n",
    "#comparison between performances of best logistic regression models with and without CV: m_training = 500 and m_training = 1000\n",
    "print(\"\\nBest logistic regression training error (m_training = 500): %f\" % training_error_log_regression)\n",
    "print(\"Best logistic regression test error (m_training = 500): %f\" % test_error_log_regression)\n",
    "print(\"Best logistic regression training error with CV (m_training = 500): %f\" % training_error_log_regression_cv)\n",
    "print(\"Best logistic regression test error with CV (m_training = 500): %f\" % test_error_log_regression_cv)\n",
    "\n",
    "print(\"\\nBest logistic regression training error (m_training = 1000): %f\" % training_error_log_regression_1000)\n",
    "print(\"Best logistic regression test error (m_training = 1000): %f\" % test_error_log_regression_1000)\n",
    "print(\"Best logistic regression training error with CV (m_training = 1000): %f\" % training_error_log_regression_cv_1000)\n",
    "print(\"Best logistic regression test error with CV (m_training = 1000): %f\" % test_error_log_regression_cv_1000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
