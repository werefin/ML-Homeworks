{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGpf0LfarzHi"
      },
      "source": [
        "# Neural Networks for Classification\n",
        "\n",
        "In this notebook we are going to explore the use of Neural Networks for image classification. We are going to use a dataset of small images of clothes and accessories, the Fashion MNIST. You can find more information regarding the dataset here: https://pravarmahajan.github.io/fashion/.\n",
        "\n",
        "Each instance in the dataset consist of an image, in a format similar to the digit images you have seen in the previous homework and a label. The labels correspond to the type of clothing, as follows:\n",
        "\n",
        "| Label | Description |\n",
        "| --- | --- |\n",
        "| 0 | T-shirt/top |\n",
        "| 1 | Trouser |\n",
        "| 2 | Pullover |\n",
        "| 3 | Dress |\n",
        "| 4 | Coat |\n",
        "| 5 | Sandal |\n",
        "| 6 | Shirt |\n",
        "| 7 | Sneaker |\n",
        "| 8 | Bag |\n",
        "| 9 | Ankle boot |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3v7vhgHrzHu"
      },
      "source": [
        "Let's first load the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3S15AeQtrzHw"
      },
      "outputs": [],
      "source": [
        "#load the required packages\n",
        "%matplotlib inline  \n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou7cWL5przH0"
      },
      "source": [
        "The following is a function to load the data, that we are going to use later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xOIUee25rzH1"
      },
      "outputs": [],
      "source": [
        "#helper function to load Fashion MNIST dataset from disk\n",
        "def load_fashion_mnist(path, kind='train'):\n",
        "    import os\n",
        "    import gzip\n",
        "    import numpy as np\n",
        "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n",
        "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784)\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoL-RPkHrzH3"
      },
      "source": [
        "## TO DO 0\n",
        "Place your ID (\"numero di matricola\") that will be used as seed for random generator. Change the ID number in case you observe unexpected behaviours and want to test if this is due to randomization (e.g. train/test split). If you change the ID number explain here why you have changed it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "GkZx9BzhrzH4"
      },
      "outputs": [],
      "source": [
        "ID = 2082157\n",
        "np.random.seed(ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6tZEgDsrzH5"
      },
      "source": [
        "Now we load the dataset using the function above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3__J_9gSrzH7"
      },
      "outputs": [],
      "source": [
        "#load the fashion MNIST dataset and normalize the features so that each value is in [0,1]\n",
        "X, y = load_fashion_mnist(\"data\")\n",
        "#rescale the data\n",
        "X = X / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGPfmfsQrzH9"
      },
      "source": [
        "Now we split the data into training and test. Make sure that each label is present at least 10 times\n",
        "in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsjQ6aqjrzH_",
        "outputId": "af26eeaf-12b8-42a1-cc2c-60f3b7c2df98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels in training dataset: [0 1 2 3 4 5 6 7 8 9]\n",
            "Frequencies in training dataset: [52 43 46 58 34 64 60 48 53 42]\n"
          ]
        }
      ],
      "source": [
        "#random permute the data and split into training and test taking the first 500 data samples as training and the rest as test\n",
        "permutation = np.random.permutation(X.shape[0])\n",
        "\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "\n",
        "m_training = 500\n",
        "\n",
        "X_train, X_test = X[:m_training], X[m_training:]\n",
        "y_train, y_test = y[:m_training], y[m_training:]\n",
        "\n",
        "labels, freqs = np.unique(y_train, return_counts=True)\n",
        "print(\"Labels in training dataset:\", labels)\n",
        "print(\"Frequencies in training dataset:\", freqs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHr2ICMorzIC"
      },
      "source": [
        "The following function plots an image and the corresponding label, to be used to inspect the data when needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ZVUvPiA0rzID"
      },
      "outputs": [],
      "source": [
        "#function for plotting a image and printing the corresponding label\n",
        "def plot_input(X_matrix, labels, index):\n",
        "    print(\"INPUT:\")\n",
        "    plt.imshow(\n",
        "        X_matrix[index].reshape(28, 28),\n",
        "        cmap = plt.cm.gray_r,\n",
        "        interpolation = \"nearest\"\n",
        "    )\n",
        "    plt.show()\n",
        "    print(\"LABEL: %i\"%labels[index])\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvCAnUxWrzIF"
      },
      "source": [
        "Now let's test the function above and check few images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "Q_lmXOU2rzIG",
        "outputId": "59f386bd-43a3-4852-a20f-6d5f0be944dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LABEL: 5\n",
            "INPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LABEL: 2\n",
            "INPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LABEL: 9\n"
          ]
        }
      ],
      "source": [
        "#let's try the plotting function\n",
        "plot_input(X_train, y_train, 10)\n",
        "plot_input(X_test, y_test, 50)\n",
        "plot_input(X_test, y_test, 300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXN92v1grzIH"
      },
      "source": [
        "### TO DO 1\n",
        "\n",
        "Now use a (feed-forward) Neural Network for prediction. Use the multi-layer perceptron (MLP) classifier MLPClassifier(...) in scikit-learn, with the following parameters: max_iter=300, alpha=1e-4, solver='sgd', tol=1e-4, learning_rate_init=.1, verbose=True, random_state=ID (this last parameter ensures the run is the same even if you run it more than once). The alpha parameter is the regularization parameter for L2 regularization that is used by the MLP in sklearn, and verbose=True allows you to see how loss changes in iterations (note that the loss used by the MLPClassifier may be different from the 0-1 loss, also called accuracy).\n",
        "\n",
        "Then, using the default activation function, we consider four architectures, with different numbers of hidden layers and different sizes. To evaluate the architectures we use the GridSearchCV with a 5-fold cross-validation, and use the results to pick the best architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK6jXC_BrzII",
        "outputId": "35f77652-f81f-4f40-a228-5d46af187d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.32777471\n",
            "Iteration 2, loss = 2.17316215\n",
            "Iteration 3, loss = 2.02401117\n",
            "Iteration 4, loss = 1.86680942\n",
            "Iteration 5, loss = 1.70154171\n",
            "Iteration 6, loss = 1.49444216\n",
            "Iteration 7, loss = 1.35672469\n",
            "Iteration 8, loss = 1.30989955\n",
            "Iteration 9, loss = 1.03556787\n",
            "Iteration 10, loss = 0.99275043\n",
            "Iteration 11, loss = 0.97195151\n",
            "Iteration 12, loss = 0.78254896\n",
            "Iteration 13, loss = 0.76782784\n",
            "Iteration 14, loss = 0.78115537\n",
            "Iteration 15, loss = 0.76178625\n",
            "Iteration 16, loss = 0.69344369\n",
            "Iteration 17, loss = 0.61003288\n",
            "Iteration 18, loss = 0.66234546\n",
            "Iteration 19, loss = 0.56591744\n",
            "Iteration 20, loss = 0.60732606\n",
            "Iteration 21, loss = 0.47960518\n",
            "Iteration 22, loss = 0.43269299\n",
            "Iteration 23, loss = 0.45132729\n",
            "Iteration 24, loss = 0.47644375\n",
            "Iteration 25, loss = 0.55576308\n",
            "Iteration 26, loss = 0.44747095\n",
            "Iteration 27, loss = 0.46521609\n",
            "Iteration 28, loss = 0.33587867\n",
            "Iteration 29, loss = 0.40025748\n",
            "Iteration 30, loss = 0.65923644\n",
            "Iteration 31, loss = 0.35593686\n",
            "Iteration 32, loss = 0.29298473\n",
            "Iteration 33, loss = 0.27682263\n",
            "Iteration 34, loss = 0.25142224\n",
            "Iteration 35, loss = 0.26271832\n",
            "Iteration 36, loss = 0.26691689\n",
            "Iteration 37, loss = 0.23718623\n",
            "Iteration 38, loss = 0.20669416\n",
            "Iteration 39, loss = 0.19838960\n",
            "Iteration 40, loss = 0.22350927\n",
            "Iteration 41, loss = 0.18267391\n",
            "Iteration 42, loss = 0.18272457\n",
            "Iteration 43, loss = 0.19788684\n",
            "Iteration 44, loss = 0.22143880\n",
            "Iteration 45, loss = 0.26677813\n",
            "Iteration 46, loss = 0.37694845\n",
            "Iteration 47, loss = 0.58933020\n",
            "Iteration 48, loss = 0.45900471\n",
            "Iteration 49, loss = 0.20494756\n",
            "Iteration 50, loss = 0.16864233\n",
            "Iteration 51, loss = 0.14902218\n",
            "Iteration 52, loss = 0.12465024\n",
            "Iteration 53, loss = 0.11774443\n",
            "Iteration 54, loss = 0.11066326\n",
            "Iteration 55, loss = 0.10648955\n",
            "Iteration 56, loss = 0.09893065\n",
            "Iteration 57, loss = 0.10087739\n",
            "Iteration 58, loss = 0.10113108\n",
            "Iteration 59, loss = 0.09722206\n",
            "Iteration 60, loss = 0.11255270\n",
            "Iteration 61, loss = 0.08264619\n",
            "Iteration 62, loss = 0.07441514\n",
            "Iteration 63, loss = 0.07181157\n",
            "Iteration 64, loss = 0.06991936\n",
            "Iteration 65, loss = 0.06698326\n",
            "Iteration 66, loss = 0.07139922\n",
            "Iteration 67, loss = 0.11643907\n",
            "Iteration 68, loss = 0.08138563\n",
            "Iteration 69, loss = 0.06374640\n",
            "Iteration 70, loss = 0.06433638\n",
            "Iteration 71, loss = 0.06453692\n",
            "Iteration 72, loss = 0.05404986\n",
            "Iteration 73, loss = 0.04870773\n",
            "Iteration 74, loss = 0.04882957\n",
            "Iteration 75, loss = 0.04476937\n",
            "Iteration 76, loss = 0.04978116\n",
            "Iteration 77, loss = 0.04629032\n",
            "Iteration 78, loss = 0.05270984\n",
            "Iteration 79, loss = 0.04134369\n",
            "Iteration 80, loss = 0.04188034\n",
            "Iteration 81, loss = 0.03946933\n",
            "Iteration 82, loss = 0.04442507\n",
            "Iteration 83, loss = 0.04296323\n",
            "Iteration 84, loss = 0.03983067\n",
            "Iteration 85, loss = 0.03202722\n",
            "Iteration 86, loss = 0.03198484\n",
            "Iteration 87, loss = 0.03488033\n",
            "Iteration 88, loss = 0.03541143\n",
            "Iteration 89, loss = 0.02834797\n",
            "Iteration 90, loss = 0.02764425\n",
            "Iteration 91, loss = 0.02831156\n",
            "Iteration 92, loss = 0.02842210\n",
            "Iteration 93, loss = 0.02587182\n",
            "Iteration 94, loss = 0.02496359\n",
            "Iteration 95, loss = 0.02397062\n",
            "Iteration 96, loss = 0.02364429\n",
            "Iteration 97, loss = 0.02286669\n",
            "Iteration 98, loss = 0.02240537\n",
            "Iteration 99, loss = 0.02323136\n",
            "Iteration 100, loss = 0.02169303\n",
            "Iteration 101, loss = 0.02118635\n",
            "Iteration 102, loss = 0.02080540\n",
            "Iteration 103, loss = 0.01996338\n",
            "Iteration 104, loss = 0.01921764\n",
            "Iteration 105, loss = 0.02106620\n",
            "Iteration 106, loss = 0.01995117\n",
            "Iteration 107, loss = 0.01799747\n",
            "Iteration 108, loss = 0.01755229\n",
            "Iteration 109, loss = 0.01741297\n",
            "Iteration 110, loss = 0.01834780\n",
            "Iteration 111, loss = 0.01664448\n",
            "Iteration 112, loss = 0.01649127\n",
            "Iteration 113, loss = 0.01587685\n",
            "Iteration 114, loss = 0.01552951\n",
            "Iteration 115, loss = 0.01590988\n",
            "Iteration 116, loss = 0.01557777\n",
            "Iteration 117, loss = 0.01585571\n",
            "Iteration 118, loss = 0.01486239\n",
            "Iteration 119, loss = 0.01428018\n",
            "Iteration 120, loss = 0.01428122\n",
            "Iteration 121, loss = 0.01366292\n",
            "Iteration 122, loss = 0.01331686\n",
            "Iteration 123, loss = 0.01311156\n",
            "Iteration 124, loss = 0.01296070\n",
            "Iteration 125, loss = 0.01264323\n",
            "Iteration 126, loss = 0.01247623\n",
            "Iteration 127, loss = 0.01220478\n",
            "Iteration 128, loss = 0.01203612\n",
            "Iteration 129, loss = 0.01180180\n",
            "Iteration 130, loss = 0.01167911\n",
            "Iteration 131, loss = 0.01147851\n",
            "Iteration 132, loss = 0.01133647\n",
            "Iteration 133, loss = 0.01108358\n",
            "Iteration 134, loss = 0.01133949\n",
            "Iteration 135, loss = 0.01090514\n",
            "Iteration 136, loss = 0.01069479\n",
            "Iteration 137, loss = 0.01049952\n",
            "Iteration 138, loss = 0.01080074\n",
            "Iteration 139, loss = 0.01020943\n",
            "Iteration 140, loss = 0.01006078\n",
            "Iteration 141, loss = 0.01003142\n",
            "Iteration 142, loss = 0.00981857\n",
            "Iteration 143, loss = 0.00967112\n",
            "Iteration 144, loss = 0.00954199\n",
            "Iteration 145, loss = 0.00927991\n",
            "Iteration 146, loss = 0.00919288\n",
            "Iteration 147, loss = 0.00907597\n",
            "Iteration 148, loss = 0.00891713\n",
            "Iteration 149, loss = 0.00882220\n",
            "Iteration 150, loss = 0.00886773\n",
            "Iteration 151, loss = 0.00860930\n",
            "Iteration 152, loss = 0.00845969\n",
            "Iteration 153, loss = 0.00851856\n",
            "Iteration 154, loss = 0.00834913\n",
            "Iteration 155, loss = 0.00828714\n",
            "Iteration 156, loss = 0.00813772\n",
            "Iteration 157, loss = 0.00797033\n",
            "Iteration 158, loss = 0.00788669\n",
            "Iteration 159, loss = 0.00776756\n",
            "Iteration 160, loss = 0.00776385\n",
            "Iteration 161, loss = 0.00770534\n",
            "Iteration 162, loss = 0.00754492\n",
            "Iteration 163, loss = 0.00746195\n",
            "Iteration 164, loss = 0.00736699\n",
            "Iteration 165, loss = 0.00730458\n",
            "Iteration 166, loss = 0.00753278\n",
            "Iteration 167, loss = 0.00717237\n",
            "Iteration 168, loss = 0.00711700\n",
            "Iteration 169, loss = 0.00707671\n",
            "Iteration 170, loss = 0.00689807\n",
            "Iteration 171, loss = 0.00688620\n",
            "Iteration 172, loss = 0.00674433\n",
            "Iteration 173, loss = 0.00676965\n",
            "Iteration 174, loss = 0.00664897\n",
            "Iteration 175, loss = 0.00652347\n",
            "Iteration 176, loss = 0.00647736\n",
            "Iteration 177, loss = 0.00646137\n",
            "Iteration 178, loss = 0.00650576\n",
            "Iteration 179, loss = 0.00632730\n",
            "Iteration 180, loss = 0.00619204\n",
            "Iteration 181, loss = 0.00614975\n",
            "Iteration 182, loss = 0.00609337\n",
            "Iteration 183, loss = 0.00604409\n",
            "Iteration 184, loss = 0.00598250\n",
            "Iteration 185, loss = 0.00593349\n",
            "Iteration 186, loss = 0.00586193\n",
            "Iteration 187, loss = 0.00585793\n",
            "Iteration 188, loss = 0.00578528\n",
            "Iteration 189, loss = 0.00569989\n",
            "Iteration 190, loss = 0.00564820\n",
            "Iteration 191, loss = 0.00563472\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.32692021\n",
            "Iteration 2, loss = 2.20261466\n",
            "Iteration 3, loss = 2.04516677\n",
            "Iteration 4, loss = 1.89272442\n",
            "Iteration 5, loss = 1.74212502\n",
            "Iteration 6, loss = 1.50698877\n",
            "Iteration 7, loss = 1.30665627\n",
            "Iteration 8, loss = 1.15856733\n",
            "Iteration 9, loss = 1.09289208\n",
            "Iteration 10, loss = 0.97218557\n",
            "Iteration 11, loss = 0.93781929\n",
            "Iteration 12, loss = 0.85271783\n",
            "Iteration 13, loss = 0.80061244\n",
            "Iteration 14, loss = 0.66888914\n",
            "Iteration 15, loss = 0.57248086\n",
            "Iteration 16, loss = 0.54604784\n",
            "Iteration 17, loss = 0.49366571\n",
            "Iteration 18, loss = 0.44406502\n",
            "Iteration 19, loss = 0.46300098\n",
            "Iteration 20, loss = 0.40266088\n",
            "Iteration 21, loss = 0.40817018\n",
            "Iteration 22, loss = 0.35109749\n",
            "Iteration 23, loss = 0.33894487\n",
            "Iteration 24, loss = 0.34551167\n",
            "Iteration 25, loss = 0.28979602\n",
            "Iteration 26, loss = 0.27212198\n",
            "Iteration 27, loss = 0.25381450\n",
            "Iteration 28, loss = 0.23714355\n",
            "Iteration 29, loss = 0.23804205\n",
            "Iteration 30, loss = 0.21685493\n",
            "Iteration 31, loss = 0.21089105\n",
            "Iteration 32, loss = 0.24008249\n",
            "Iteration 33, loss = 0.23789738\n",
            "Iteration 34, loss = 0.25309153\n",
            "Iteration 35, loss = 0.22394972\n",
            "Iteration 36, loss = 0.15900680\n",
            "Iteration 37, loss = 0.14008978\n",
            "Iteration 38, loss = 0.13913081\n",
            "Iteration 39, loss = 0.15005835\n",
            "Iteration 40, loss = 0.13492509\n",
            "Iteration 41, loss = 0.11394520\n",
            "Iteration 42, loss = 0.11511591\n",
            "Iteration 43, loss = 0.15843448\n",
            "Iteration 44, loss = 0.11585464\n",
            "Iteration 45, loss = 0.13602301\n",
            "Iteration 46, loss = 0.14897350\n",
            "Iteration 47, loss = 0.19605819\n",
            "Iteration 48, loss = 0.16092716\n",
            "Iteration 49, loss = 0.11867476\n",
            "Iteration 50, loss = 0.10963411\n",
            "Iteration 51, loss = 0.08998454\n",
            "Iteration 52, loss = 0.06791650\n",
            "Iteration 53, loss = 0.06804383\n",
            "Iteration 54, loss = 0.06106841\n",
            "Iteration 55, loss = 0.05699418\n",
            "Iteration 56, loss = 0.05531461\n",
            "Iteration 57, loss = 0.05911986\n",
            "Iteration 58, loss = 0.05537817\n",
            "Iteration 59, loss = 0.04757924\n",
            "Iteration 60, loss = 0.04448457\n",
            "Iteration 61, loss = 0.04396023\n",
            "Iteration 62, loss = 0.04166385\n",
            "Iteration 63, loss = 0.03848953\n",
            "Iteration 64, loss = 0.03697806\n",
            "Iteration 65, loss = 0.03570367\n",
            "Iteration 66, loss = 0.03376178\n",
            "Iteration 67, loss = 0.03276753\n",
            "Iteration 68, loss = 0.03108776\n",
            "Iteration 69, loss = 0.03012310\n",
            "Iteration 70, loss = 0.02939747\n",
            "Iteration 71, loss = 0.02815325\n",
            "Iteration 72, loss = 0.02648209\n",
            "Iteration 73, loss = 0.02654785\n",
            "Iteration 74, loss = 0.02485469\n",
            "Iteration 75, loss = 0.02367601\n",
            "Iteration 76, loss = 0.02304082\n",
            "Iteration 77, loss = 0.02236443\n",
            "Iteration 78, loss = 0.02187963\n",
            "Iteration 79, loss = 0.02096218\n",
            "Iteration 80, loss = 0.02066157\n",
            "Iteration 81, loss = 0.02064077\n",
            "Iteration 82, loss = 0.01983000\n",
            "Iteration 83, loss = 0.01972503\n",
            "Iteration 84, loss = 0.01818415\n",
            "Iteration 85, loss = 0.01742450\n",
            "Iteration 86, loss = 0.01701858\n",
            "Iteration 87, loss = 0.01688273\n",
            "Iteration 88, loss = 0.01635256\n",
            "Iteration 89, loss = 0.01575797\n",
            "Iteration 90, loss = 0.01529739\n",
            "Iteration 91, loss = 0.01521763\n",
            "Iteration 92, loss = 0.01557584\n",
            "Iteration 93, loss = 0.01453654\n",
            "Iteration 94, loss = 0.01382500\n",
            "Iteration 95, loss = 0.01349833\n",
            "Iteration 96, loss = 0.01354245\n",
            "Iteration 97, loss = 0.01323616\n",
            "Iteration 98, loss = 0.01272971\n",
            "Iteration 99, loss = 0.01286460\n",
            "Iteration 100, loss = 0.01220416\n",
            "Iteration 101, loss = 0.01191289\n",
            "Iteration 102, loss = 0.01162969\n",
            "Iteration 103, loss = 0.01158439\n",
            "Iteration 104, loss = 0.01147252\n",
            "Iteration 105, loss = 0.01104351\n",
            "Iteration 106, loss = 0.01080466\n",
            "Iteration 107, loss = 0.01063855\n",
            "Iteration 108, loss = 0.01041667\n",
            "Iteration 109, loss = 0.01039464\n",
            "Iteration 110, loss = 0.01027412\n",
            "Iteration 111, loss = 0.01006102\n",
            "Iteration 112, loss = 0.00978373\n",
            "Iteration 113, loss = 0.00952472\n",
            "Iteration 114, loss = 0.00954359\n",
            "Iteration 115, loss = 0.00922781\n",
            "Iteration 116, loss = 0.00906451\n",
            "Iteration 117, loss = 0.00910889\n",
            "Iteration 118, loss = 0.00879271\n",
            "Iteration 119, loss = 0.00868621\n",
            "Iteration 120, loss = 0.00928153\n",
            "Iteration 121, loss = 0.00856764\n",
            "Iteration 122, loss = 0.00841098\n",
            "Iteration 123, loss = 0.00829864\n",
            "Iteration 124, loss = 0.00800860\n",
            "Iteration 125, loss = 0.00790805\n",
            "Iteration 126, loss = 0.00778625\n",
            "Iteration 127, loss = 0.00773305\n",
            "Iteration 128, loss = 0.00761504\n",
            "Iteration 129, loss = 0.00755949\n",
            "Iteration 130, loss = 0.00744381\n",
            "Iteration 131, loss = 0.00730977\n",
            "Iteration 132, loss = 0.00720575\n",
            "Iteration 133, loss = 0.00710941\n",
            "Iteration 134, loss = 0.00710360\n",
            "Iteration 135, loss = 0.00697275\n",
            "Iteration 136, loss = 0.00690298\n",
            "Iteration 137, loss = 0.00684772\n",
            "Iteration 138, loss = 0.00672131\n",
            "Iteration 139, loss = 0.00661230\n",
            "Iteration 140, loss = 0.00660305\n",
            "Iteration 141, loss = 0.00642936\n",
            "Iteration 142, loss = 0.00636295\n",
            "Iteration 143, loss = 0.00631653\n",
            "Iteration 144, loss = 0.00635365\n",
            "Iteration 145, loss = 0.00613143\n",
            "Iteration 146, loss = 0.00610758\n",
            "Iteration 147, loss = 0.00600170\n",
            "Iteration 148, loss = 0.00597070\n",
            "Iteration 149, loss = 0.00588302\n",
            "Iteration 150, loss = 0.00581691\n",
            "Iteration 151, loss = 0.00573839\n",
            "Iteration 152, loss = 0.00573230\n",
            "Iteration 153, loss = 0.00565718\n",
            "Iteration 154, loss = 0.00555786\n",
            "Iteration 155, loss = 0.00558608\n",
            "Iteration 156, loss = 0.00546957\n",
            "Iteration 157, loss = 0.00541607\n",
            "Iteration 158, loss = 0.00534694\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.34349742\n",
            "Iteration 2, loss = 2.17677085\n",
            "Iteration 3, loss = 2.04180724\n",
            "Iteration 4, loss = 1.88574965\n",
            "Iteration 5, loss = 1.71423380\n",
            "Iteration 6, loss = 1.51573005\n",
            "Iteration 7, loss = 1.37305427\n",
            "Iteration 8, loss = 1.24606633\n",
            "Iteration 9, loss = 1.08232584\n",
            "Iteration 10, loss = 1.03039823\n",
            "Iteration 11, loss = 1.01502436\n",
            "Iteration 12, loss = 0.96208913\n",
            "Iteration 13, loss = 0.84278678\n",
            "Iteration 14, loss = 0.82803665\n",
            "Iteration 15, loss = 0.76210759\n",
            "Iteration 16, loss = 0.65136948\n",
            "Iteration 17, loss = 0.63774092\n",
            "Iteration 18, loss = 0.64445181\n",
            "Iteration 19, loss = 0.59224927\n",
            "Iteration 20, loss = 0.57773042\n",
            "Iteration 21, loss = 0.49440022\n",
            "Iteration 22, loss = 0.51632067\n",
            "Iteration 23, loss = 0.46609234\n",
            "Iteration 24, loss = 0.41842062\n",
            "Iteration 25, loss = 0.41538841\n",
            "Iteration 26, loss = 0.49318032\n",
            "Iteration 27, loss = 0.60363999\n",
            "Iteration 28, loss = 0.43635165\n",
            "Iteration 29, loss = 0.39562577\n",
            "Iteration 30, loss = 0.40020569\n",
            "Iteration 31, loss = 0.33292807\n",
            "Iteration 32, loss = 0.37850449\n",
            "Iteration 33, loss = 0.27543733\n",
            "Iteration 34, loss = 0.26213592\n",
            "Iteration 35, loss = 0.36601244\n",
            "Iteration 36, loss = 0.36415627\n",
            "Iteration 37, loss = 0.24961043\n",
            "Iteration 38, loss = 0.23514090\n",
            "Iteration 39, loss = 0.36634065\n",
            "Iteration 40, loss = 0.23051747\n",
            "Iteration 41, loss = 0.19199842\n",
            "Iteration 42, loss = 0.19068941\n",
            "Iteration 43, loss = 0.23442269\n",
            "Iteration 44, loss = 0.17894364\n",
            "Iteration 45, loss = 0.14906225\n",
            "Iteration 46, loss = 0.25172196\n",
            "Iteration 47, loss = 0.33967285\n",
            "Iteration 48, loss = 0.22836110\n",
            "Iteration 49, loss = 0.13210495\n",
            "Iteration 50, loss = 0.12149964\n",
            "Iteration 51, loss = 0.13213268\n",
            "Iteration 52, loss = 0.11555803\n",
            "Iteration 53, loss = 0.10912024\n",
            "Iteration 54, loss = 0.11657112\n",
            "Iteration 55, loss = 0.09192604\n",
            "Iteration 56, loss = 0.09155853\n",
            "Iteration 57, loss = 0.08761990\n",
            "Iteration 58, loss = 0.08090731\n",
            "Iteration 59, loss = 0.08622236\n",
            "Iteration 60, loss = 0.09334120\n",
            "Iteration 61, loss = 0.08681571\n",
            "Iteration 62, loss = 0.10064058\n",
            "Iteration 63, loss = 0.07640246\n",
            "Iteration 64, loss = 0.06428129\n",
            "Iteration 65, loss = 0.07589785\n",
            "Iteration 66, loss = 0.08267793\n",
            "Iteration 67, loss = 0.08323092\n",
            "Iteration 68, loss = 0.05383032\n",
            "Iteration 69, loss = 0.05970407\n",
            "Iteration 70, loss = 0.05060344\n",
            "Iteration 71, loss = 0.04907070\n",
            "Iteration 72, loss = 0.05018582\n",
            "Iteration 73, loss = 0.04631550\n",
            "Iteration 74, loss = 0.04626994\n",
            "Iteration 75, loss = 0.04308411\n",
            "Iteration 76, loss = 0.04191870\n",
            "Iteration 77, loss = 0.04195215\n",
            "Iteration 78, loss = 0.04087423\n",
            "Iteration 79, loss = 0.03799250\n",
            "Iteration 80, loss = 0.03631893\n",
            "Iteration 81, loss = 0.03548031\n",
            "Iteration 82, loss = 0.03535396\n",
            "Iteration 83, loss = 0.03421393\n",
            "Iteration 84, loss = 0.03263510\n",
            "Iteration 85, loss = 0.03158205\n",
            "Iteration 86, loss = 0.03083864\n",
            "Iteration 87, loss = 0.03060619\n",
            "Iteration 88, loss = 0.02969288\n",
            "Iteration 89, loss = 0.02896251\n",
            "Iteration 90, loss = 0.02781139\n",
            "Iteration 91, loss = 0.02735113\n",
            "Iteration 92, loss = 0.02673515\n",
            "Iteration 93, loss = 0.02668999\n",
            "Iteration 94, loss = 0.02576168\n",
            "Iteration 95, loss = 0.02511715\n",
            "Iteration 96, loss = 0.02571133\n",
            "Iteration 97, loss = 0.02483189\n",
            "Iteration 98, loss = 0.02416153\n",
            "Iteration 99, loss = 0.02345858\n",
            "Iteration 100, loss = 0.02271779\n",
            "Iteration 101, loss = 0.02235115\n",
            "Iteration 102, loss = 0.02203976\n",
            "Iteration 103, loss = 0.02192937\n",
            "Iteration 104, loss = 0.02122822\n",
            "Iteration 105, loss = 0.02067176\n",
            "Iteration 106, loss = 0.02023861\n",
            "Iteration 107, loss = 0.01971456\n",
            "Iteration 108, loss = 0.01939548\n",
            "Iteration 109, loss = 0.01929374\n",
            "Iteration 110, loss = 0.01863765\n",
            "Iteration 111, loss = 0.01843100\n",
            "Iteration 112, loss = 0.01813905\n",
            "Iteration 113, loss = 0.01776954\n",
            "Iteration 114, loss = 0.01755086\n",
            "Iteration 115, loss = 0.01711474\n",
            "Iteration 116, loss = 0.01709959\n",
            "Iteration 117, loss = 0.01673178\n",
            "Iteration 118, loss = 0.01636692\n",
            "Iteration 119, loss = 0.01613410\n",
            "Iteration 120, loss = 0.01597087\n",
            "Iteration 121, loss = 0.01571504\n",
            "Iteration 122, loss = 0.01527976\n",
            "Iteration 123, loss = 0.01500887\n",
            "Iteration 124, loss = 0.01479138\n",
            "Iteration 125, loss = 0.01459602\n",
            "Iteration 126, loss = 0.01443819\n",
            "Iteration 127, loss = 0.01410712\n",
            "Iteration 128, loss = 0.01396148\n",
            "Iteration 129, loss = 0.01383895\n",
            "Iteration 130, loss = 0.01352636\n",
            "Iteration 131, loss = 0.01339479\n",
            "Iteration 132, loss = 0.01327004\n",
            "Iteration 133, loss = 0.01297223\n",
            "Iteration 134, loss = 0.01285243\n",
            "Iteration 135, loss = 0.01269964\n",
            "Iteration 136, loss = 0.01235491\n",
            "Iteration 137, loss = 0.01223141\n",
            "Iteration 138, loss = 0.01207646\n",
            "Iteration 139, loss = 0.01203132\n",
            "Iteration 140, loss = 0.01178442\n",
            "Iteration 141, loss = 0.01159542\n",
            "Iteration 142, loss = 0.01146932\n",
            "Iteration 143, loss = 0.01134245\n",
            "Iteration 144, loss = 0.01124035\n",
            "Iteration 145, loss = 0.01098234\n",
            "Iteration 146, loss = 0.01092852\n",
            "Iteration 147, loss = 0.01088021\n",
            "Iteration 148, loss = 0.01059682\n",
            "Iteration 149, loss = 0.01042409\n",
            "Iteration 150, loss = 0.01030816\n",
            "Iteration 151, loss = 0.01024057\n",
            "Iteration 152, loss = 0.01006014\n",
            "Iteration 153, loss = 0.00997842\n",
            "Iteration 154, loss = 0.00984117\n",
            "Iteration 155, loss = 0.00970053\n",
            "Iteration 156, loss = 0.00960333\n",
            "Iteration 157, loss = 0.00950854\n",
            "Iteration 158, loss = 0.00933031\n",
            "Iteration 159, loss = 0.00934952\n",
            "Iteration 160, loss = 0.00914861\n",
            "Iteration 161, loss = 0.00905802\n",
            "Iteration 162, loss = 0.00902940\n",
            "Iteration 163, loss = 0.00896594\n",
            "Iteration 164, loss = 0.00877359\n",
            "Iteration 165, loss = 0.00870712\n",
            "Iteration 166, loss = 0.00857907\n",
            "Iteration 167, loss = 0.00850232\n",
            "Iteration 168, loss = 0.00840642\n",
            "Iteration 169, loss = 0.00832324\n",
            "Iteration 170, loss = 0.00822168\n",
            "Iteration 171, loss = 0.00817493\n",
            "Iteration 172, loss = 0.00811372\n",
            "Iteration 173, loss = 0.00799763\n",
            "Iteration 174, loss = 0.00787108\n",
            "Iteration 175, loss = 0.00783609\n",
            "Iteration 176, loss = 0.00771501\n",
            "Iteration 177, loss = 0.00762655\n",
            "Iteration 178, loss = 0.00757834\n",
            "Iteration 179, loss = 0.00747197\n",
            "Iteration 180, loss = 0.00740475\n",
            "Iteration 181, loss = 0.00738286\n",
            "Iteration 182, loss = 0.00727103\n",
            "Iteration 183, loss = 0.00721200\n",
            "Iteration 184, loss = 0.00714188\n",
            "Iteration 185, loss = 0.00709455\n",
            "Iteration 186, loss = 0.00699717\n",
            "Iteration 187, loss = 0.00696723\n",
            "Iteration 188, loss = 0.00692205\n",
            "Iteration 189, loss = 0.00680955\n",
            "Iteration 190, loss = 0.00676912\n",
            "Iteration 191, loss = 0.00670703\n",
            "Iteration 192, loss = 0.00661837\n",
            "Iteration 193, loss = 0.00655474\n",
            "Iteration 194, loss = 0.00651802\n",
            "Iteration 195, loss = 0.00645836\n",
            "Iteration 196, loss = 0.00641591\n",
            "Iteration 197, loss = 0.00639127\n",
            "Iteration 198, loss = 0.00632799\n",
            "Iteration 199, loss = 0.00622592\n",
            "Iteration 200, loss = 0.00628374\n",
            "Iteration 201, loss = 0.00613571\n",
            "Iteration 202, loss = 0.00612004\n",
            "Iteration 203, loss = 0.00604387\n",
            "Iteration 204, loss = 0.00598136\n",
            "Iteration 205, loss = 0.00593296\n",
            "Iteration 206, loss = 0.00588789\n",
            "Iteration 207, loss = 0.00583899\n",
            "Iteration 208, loss = 0.00580168\n",
            "Iteration 209, loss = 0.00575067\n",
            "Iteration 210, loss = 0.00571348\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.33978676\n",
            "Iteration 2, loss = 2.18885406\n",
            "Iteration 3, loss = 2.05591380\n",
            "Iteration 4, loss = 1.89494994\n",
            "Iteration 5, loss = 1.74030122\n",
            "Iteration 6, loss = 1.56688712\n",
            "Iteration 7, loss = 1.41237349\n",
            "Iteration 8, loss = 1.27204858\n",
            "Iteration 9, loss = 1.13936175\n",
            "Iteration 10, loss = 1.11469190\n",
            "Iteration 11, loss = 1.17046973\n",
            "Iteration 12, loss = 1.01285533\n",
            "Iteration 13, loss = 0.85386090\n",
            "Iteration 14, loss = 0.79513366\n",
            "Iteration 15, loss = 0.76027637\n",
            "Iteration 16, loss = 0.69905482\n",
            "Iteration 17, loss = 0.67262124\n",
            "Iteration 18, loss = 0.65159460\n",
            "Iteration 19, loss = 0.60743806\n",
            "Iteration 20, loss = 0.64696352\n",
            "Iteration 21, loss = 0.58020989\n",
            "Iteration 22, loss = 0.60112040\n",
            "Iteration 23, loss = 0.56183431\n",
            "Iteration 24, loss = 0.51859827\n",
            "Iteration 25, loss = 0.49132129\n",
            "Iteration 26, loss = 0.46284742\n",
            "Iteration 27, loss = 0.60568479\n",
            "Iteration 28, loss = 0.85808771\n",
            "Iteration 29, loss = 0.65977572\n",
            "Iteration 30, loss = 0.56042365\n",
            "Iteration 31, loss = 0.42642398\n",
            "Iteration 32, loss = 0.40550196\n",
            "Iteration 33, loss = 0.37630306\n",
            "Iteration 34, loss = 0.38825233\n",
            "Iteration 35, loss = 0.37754767\n",
            "Iteration 36, loss = 0.50614254\n",
            "Iteration 37, loss = 0.52121297\n",
            "Iteration 38, loss = 0.32218661\n",
            "Iteration 39, loss = 0.31555263\n",
            "Iteration 40, loss = 0.34845872\n",
            "Iteration 41, loss = 0.38118170\n",
            "Iteration 42, loss = 0.36280703\n",
            "Iteration 43, loss = 0.32719538\n",
            "Iteration 44, loss = 0.26430934\n",
            "Iteration 45, loss = 0.24786590\n",
            "Iteration 46, loss = 0.24080340\n",
            "Iteration 47, loss = 0.23567634\n",
            "Iteration 48, loss = 0.23578200\n",
            "Iteration 49, loss = 0.23161120\n",
            "Iteration 50, loss = 0.35525360\n",
            "Iteration 51, loss = 0.29990027\n",
            "Iteration 52, loss = 0.29810653\n",
            "Iteration 53, loss = 0.21163327\n",
            "Iteration 54, loss = 0.20884026\n",
            "Iteration 55, loss = 0.18335266\n",
            "Iteration 56, loss = 0.17526229\n",
            "Iteration 57, loss = 0.27308645\n",
            "Iteration 58, loss = 0.38412018\n",
            "Iteration 59, loss = 0.37661896\n",
            "Iteration 60, loss = 0.24554472\n",
            "Iteration 61, loss = 0.22967665\n",
            "Iteration 62, loss = 0.15228979\n",
            "Iteration 63, loss = 0.14780008\n",
            "Iteration 64, loss = 0.14224222\n",
            "Iteration 65, loss = 0.15940694\n",
            "Iteration 66, loss = 0.15266707\n",
            "Iteration 67, loss = 0.13472273\n",
            "Iteration 68, loss = 0.16763177\n",
            "Iteration 69, loss = 0.13730511\n",
            "Iteration 70, loss = 0.12974555\n",
            "Iteration 71, loss = 0.13765518\n",
            "Iteration 72, loss = 0.75469763\n",
            "Iteration 73, loss = 1.68986535\n",
            "Iteration 74, loss = 0.76551332\n",
            "Iteration 75, loss = 0.55870600\n",
            "Iteration 76, loss = 0.52310731\n",
            "Iteration 77, loss = 0.54632797\n",
            "Iteration 78, loss = 0.46559068\n",
            "Iteration 79, loss = 0.37999546\n",
            "Iteration 80, loss = 0.35593536\n",
            "Iteration 81, loss = 0.34536539\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.34331222\n",
            "Iteration 2, loss = 2.19238787\n",
            "Iteration 3, loss = 2.05662362\n",
            "Iteration 4, loss = 1.88970554\n",
            "Iteration 5, loss = 1.76845181\n",
            "Iteration 6, loss = 1.62727567\n",
            "Iteration 7, loss = 1.43369532\n",
            "Iteration 8, loss = 1.31128818\n",
            "Iteration 9, loss = 1.15207208\n",
            "Iteration 10, loss = 1.03519909\n",
            "Iteration 11, loss = 0.93663404\n",
            "Iteration 12, loss = 0.87257705\n",
            "Iteration 13, loss = 0.78173619\n",
            "Iteration 14, loss = 0.74924714\n",
            "Iteration 15, loss = 0.87869161\n",
            "Iteration 16, loss = 0.86774097\n",
            "Iteration 17, loss = 0.61848489\n",
            "Iteration 18, loss = 0.59544800\n",
            "Iteration 19, loss = 0.53949849\n",
            "Iteration 20, loss = 0.53712599\n",
            "Iteration 21, loss = 0.48962312\n",
            "Iteration 22, loss = 0.56397811\n",
            "Iteration 23, loss = 0.55292783\n",
            "Iteration 24, loss = 0.42737767\n",
            "Iteration 25, loss = 0.40147058\n",
            "Iteration 26, loss = 0.39815275\n",
            "Iteration 27, loss = 0.34713606\n",
            "Iteration 28, loss = 0.34559537\n",
            "Iteration 29, loss = 0.36763655\n",
            "Iteration 30, loss = 0.37225913\n",
            "Iteration 31, loss = 0.37334383\n",
            "Iteration 32, loss = 0.31037526\n",
            "Iteration 33, loss = 0.27107376\n",
            "Iteration 34, loss = 0.24159176\n",
            "Iteration 35, loss = 0.24174549\n",
            "Iteration 36, loss = 0.23481476\n",
            "Iteration 37, loss = 0.21941451\n",
            "Iteration 38, loss = 0.19910997\n",
            "Iteration 39, loss = 0.18221372\n",
            "Iteration 40, loss = 0.18956596\n",
            "Iteration 41, loss = 0.19621762\n",
            "Iteration 42, loss = 0.17670900\n",
            "Iteration 43, loss = 0.15116276\n",
            "Iteration 44, loss = 0.17051771\n",
            "Iteration 45, loss = 0.17751489\n",
            "Iteration 46, loss = 0.20371857\n",
            "Iteration 47, loss = 0.36746067\n",
            "Iteration 48, loss = 0.51278427\n",
            "Iteration 49, loss = 0.27943344\n",
            "Iteration 50, loss = 0.14481171\n",
            "Iteration 51, loss = 0.12361317\n",
            "Iteration 52, loss = 0.11883782\n",
            "Iteration 53, loss = 0.10959329\n",
            "Iteration 54, loss = 0.10011506\n",
            "Iteration 55, loss = 0.09356961\n",
            "Iteration 56, loss = 0.09098619\n",
            "Iteration 57, loss = 0.08529991\n",
            "Iteration 58, loss = 0.08109070\n",
            "Iteration 59, loss = 0.07911801\n",
            "Iteration 60, loss = 0.07595597\n",
            "Iteration 61, loss = 0.07201438\n",
            "Iteration 62, loss = 0.07068077\n",
            "Iteration 63, loss = 0.07292916\n",
            "Iteration 64, loss = 0.06099377\n",
            "Iteration 65, loss = 0.05851023\n",
            "Iteration 66, loss = 0.06415831\n",
            "Iteration 67, loss = 0.06210233\n",
            "Iteration 68, loss = 0.05204688\n",
            "Iteration 69, loss = 0.05033245\n",
            "Iteration 70, loss = 0.04640917\n",
            "Iteration 71, loss = 0.04622249\n",
            "Iteration 72, loss = 0.04401728\n",
            "Iteration 73, loss = 0.04233377\n",
            "Iteration 74, loss = 0.04360959\n",
            "Iteration 75, loss = 0.03974401\n",
            "Iteration 76, loss = 0.03560271\n",
            "Iteration 77, loss = 0.03423914\n",
            "Iteration 78, loss = 0.03331437\n",
            "Iteration 79, loss = 0.03245987\n",
            "Iteration 80, loss = 0.03099620\n",
            "Iteration 81, loss = 0.03019564\n",
            "Iteration 82, loss = 0.03008650\n",
            "Iteration 83, loss = 0.02869087\n",
            "Iteration 84, loss = 0.02760731\n",
            "Iteration 85, loss = 0.02689963\n",
            "Iteration 86, loss = 0.02585555\n",
            "Iteration 87, loss = 0.02463091\n",
            "Iteration 88, loss = 0.02365751\n",
            "Iteration 89, loss = 0.02306344\n",
            "Iteration 90, loss = 0.02284371\n",
            "Iteration 91, loss = 0.02244395\n",
            "Iteration 92, loss = 0.02146006\n",
            "Iteration 93, loss = 0.02207920\n",
            "Iteration 94, loss = 0.02078746\n",
            "Iteration 95, loss = 0.02038831\n",
            "Iteration 96, loss = 0.01956143\n",
            "Iteration 97, loss = 0.01865391\n",
            "Iteration 98, loss = 0.01819626\n",
            "Iteration 99, loss = 0.01779769\n",
            "Iteration 100, loss = 0.01747787\n",
            "Iteration 101, loss = 0.01732793\n",
            "Iteration 102, loss = 0.01689788\n",
            "Iteration 103, loss = 0.01642460\n",
            "Iteration 104, loss = 0.01579314\n",
            "Iteration 105, loss = 0.01602965\n",
            "Iteration 106, loss = 0.01538089\n",
            "Iteration 107, loss = 0.01486628\n",
            "Iteration 108, loss = 0.01528245\n",
            "Iteration 109, loss = 0.01455467\n",
            "Iteration 110, loss = 0.01401974\n",
            "Iteration 111, loss = 0.01383832\n",
            "Iteration 112, loss = 0.01349721\n",
            "Iteration 113, loss = 0.01321575\n",
            "Iteration 114, loss = 0.01316104\n",
            "Iteration 115, loss = 0.01296117\n",
            "Iteration 116, loss = 0.01250796\n",
            "Iteration 117, loss = 0.01231403\n",
            "Iteration 118, loss = 0.01227597\n",
            "Iteration 119, loss = 0.01180352\n",
            "Iteration 120, loss = 0.01167296\n",
            "Iteration 121, loss = 0.01159310\n",
            "Iteration 122, loss = 0.01141228\n",
            "Iteration 123, loss = 0.01113038\n",
            "Iteration 124, loss = 0.01088455\n",
            "Iteration 125, loss = 0.01078211\n",
            "Iteration 126, loss = 0.01052567\n",
            "Iteration 127, loss = 0.01052929\n",
            "Iteration 128, loss = 0.01048211\n",
            "Iteration 129, loss = 0.01013837\n",
            "Iteration 130, loss = 0.01002678\n",
            "Iteration 131, loss = 0.00976904\n",
            "Iteration 132, loss = 0.00973211\n",
            "Iteration 133, loss = 0.00951477\n",
            "Iteration 134, loss = 0.00934871\n",
            "Iteration 135, loss = 0.00930514\n",
            "Iteration 136, loss = 0.00908147\n",
            "Iteration 137, loss = 0.00896617\n",
            "Iteration 138, loss = 0.00888084\n",
            "Iteration 139, loss = 0.00877809\n",
            "Iteration 140, loss = 0.00860965\n",
            "Iteration 141, loss = 0.00856571\n",
            "Iteration 142, loss = 0.00843650\n",
            "Iteration 143, loss = 0.00836131\n",
            "Iteration 144, loss = 0.00821809\n",
            "Iteration 145, loss = 0.00807195\n",
            "Iteration 146, loss = 0.00798150\n",
            "Iteration 147, loss = 0.00793051\n",
            "Iteration 148, loss = 0.00779810\n",
            "Iteration 149, loss = 0.00775692\n",
            "Iteration 150, loss = 0.00763936\n",
            "Iteration 151, loss = 0.00753751\n",
            "Iteration 152, loss = 0.00743348\n",
            "Iteration 153, loss = 0.00738526\n",
            "Iteration 154, loss = 0.00731287\n",
            "Iteration 155, loss = 0.00723760\n",
            "Iteration 156, loss = 0.00723642\n",
            "Iteration 157, loss = 0.00708332\n",
            "Iteration 158, loss = 0.00697887\n",
            "Iteration 159, loss = 0.00688127\n",
            "Iteration 160, loss = 0.00683142\n",
            "Iteration 161, loss = 0.00672067\n",
            "Iteration 162, loss = 0.00670422\n",
            "Iteration 163, loss = 0.00668943\n",
            "Iteration 164, loss = 0.00653712\n",
            "Iteration 165, loss = 0.00645775\n",
            "Iteration 166, loss = 0.00645929\n",
            "Iteration 167, loss = 0.00639614\n",
            "Iteration 168, loss = 0.00630957\n",
            "Iteration 169, loss = 0.00625097\n",
            "Iteration 170, loss = 0.00614684\n",
            "Iteration 171, loss = 0.00612312\n",
            "Iteration 172, loss = 0.00606893\n",
            "Iteration 173, loss = 0.00598315\n",
            "Iteration 174, loss = 0.00594520\n",
            "Iteration 175, loss = 0.00586795\n",
            "Iteration 176, loss = 0.00581921\n",
            "Iteration 177, loss = 0.00575964\n",
            "Iteration 178, loss = 0.00570091\n",
            "Iteration 179, loss = 0.00565167\n",
            "Iteration 180, loss = 0.00559708\n",
            "Iteration 181, loss = 0.00556352\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.29324985\n",
            "Iteration 2, loss = 1.70616128\n",
            "Iteration 3, loss = 1.41310976\n",
            "Iteration 4, loss = 1.05957777\n",
            "Iteration 5, loss = 1.11326663\n",
            "Iteration 6, loss = 0.72912787\n",
            "Iteration 7, loss = 0.68743897\n",
            "Iteration 8, loss = 0.73642239\n",
            "Iteration 9, loss = 0.57922399\n",
            "Iteration 10, loss = 0.55806562\n",
            "Iteration 11, loss = 0.51199353\n",
            "Iteration 12, loss = 0.43767096\n",
            "Iteration 13, loss = 0.38705617\n",
            "Iteration 14, loss = 0.37947864\n",
            "Iteration 15, loss = 0.38756096\n",
            "Iteration 16, loss = 0.41976045\n",
            "Iteration 17, loss = 0.31684160\n",
            "Iteration 18, loss = 0.27266409\n",
            "Iteration 19, loss = 0.29532633\n",
            "Iteration 20, loss = 0.25489317\n",
            "Iteration 21, loss = 0.28795170\n",
            "Iteration 22, loss = 0.20257056\n",
            "Iteration 23, loss = 0.18521893\n",
            "Iteration 24, loss = 0.17653385\n",
            "Iteration 25, loss = 0.20272920\n",
            "Iteration 26, loss = 0.19090278\n",
            "Iteration 27, loss = 0.15917159\n",
            "Iteration 28, loss = 0.15372343\n",
            "Iteration 29, loss = 0.14306730\n",
            "Iteration 30, loss = 0.13511631\n",
            "Iteration 31, loss = 0.12397968\n",
            "Iteration 32, loss = 0.11261819\n",
            "Iteration 33, loss = 0.09850128\n",
            "Iteration 34, loss = 0.10045819\n",
            "Iteration 35, loss = 0.09412374\n",
            "Iteration 36, loss = 0.11094361\n",
            "Iteration 37, loss = 0.13088287\n",
            "Iteration 38, loss = 0.11075759\n",
            "Iteration 39, loss = 0.08856621\n",
            "Iteration 40, loss = 0.10043268\n",
            "Iteration 41, loss = 0.07524809\n",
            "Iteration 42, loss = 0.06047638\n",
            "Iteration 43, loss = 0.05666874\n",
            "Iteration 44, loss = 0.05361576\n",
            "Iteration 45, loss = 0.05160297\n",
            "Iteration 46, loss = 0.04755129\n",
            "Iteration 47, loss = 0.04599060\n",
            "Iteration 48, loss = 0.04390400\n",
            "Iteration 49, loss = 0.04072589\n",
            "Iteration 50, loss = 0.04018349\n",
            "Iteration 51, loss = 0.04220476\n",
            "Iteration 52, loss = 0.03994070\n",
            "Iteration 53, loss = 0.03531195\n",
            "Iteration 54, loss = 0.03462324\n",
            "Iteration 55, loss = 0.03429354\n",
            "Iteration 56, loss = 0.03023690\n",
            "Iteration 57, loss = 0.02899691\n",
            "Iteration 58, loss = 0.02752855\n",
            "Iteration 59, loss = 0.02624229\n",
            "Iteration 60, loss = 0.02563059\n",
            "Iteration 61, loss = 0.02621460\n",
            "Iteration 62, loss = 0.02675163\n",
            "Iteration 63, loss = 0.02875599\n",
            "Iteration 64, loss = 0.02755460\n",
            "Iteration 65, loss = 0.02174227\n",
            "Iteration 66, loss = 0.02084135\n",
            "Iteration 67, loss = 0.02010939\n",
            "Iteration 68, loss = 0.01939490\n",
            "Iteration 69, loss = 0.01869280\n",
            "Iteration 70, loss = 0.01824752\n",
            "Iteration 71, loss = 0.01810644\n",
            "Iteration 72, loss = 0.01792188\n",
            "Iteration 73, loss = 0.01711748\n",
            "Iteration 74, loss = 0.01672992\n",
            "Iteration 75, loss = 0.01626314\n",
            "Iteration 76, loss = 0.01627647\n",
            "Iteration 77, loss = 0.01599390\n",
            "Iteration 78, loss = 0.01491029\n",
            "Iteration 79, loss = 0.01439464\n",
            "Iteration 80, loss = 0.01394342\n",
            "Iteration 81, loss = 0.01360745\n",
            "Iteration 82, loss = 0.01333884\n",
            "Iteration 83, loss = 0.01316754\n",
            "Iteration 84, loss = 0.01292051\n",
            "Iteration 85, loss = 0.01241429\n",
            "Iteration 86, loss = 0.01231512\n",
            "Iteration 87, loss = 0.01204662\n",
            "Iteration 88, loss = 0.01204233\n",
            "Iteration 89, loss = 0.01179522\n",
            "Iteration 90, loss = 0.01120106\n",
            "Iteration 91, loss = 0.01098326\n",
            "Iteration 92, loss = 0.01080797\n",
            "Iteration 93, loss = 0.01060510\n",
            "Iteration 94, loss = 0.01059833\n",
            "Iteration 95, loss = 0.01021665\n",
            "Iteration 96, loss = 0.00996998\n",
            "Iteration 97, loss = 0.00996265\n",
            "Iteration 98, loss = 0.00966797\n",
            "Iteration 99, loss = 0.00957568\n",
            "Iteration 100, loss = 0.00949753\n",
            "Iteration 101, loss = 0.00921948\n",
            "Iteration 102, loss = 0.00902564\n",
            "Iteration 103, loss = 0.00907944\n",
            "Iteration 104, loss = 0.00869673\n",
            "Iteration 105, loss = 0.00861824\n",
            "Iteration 106, loss = 0.00850352\n",
            "Iteration 107, loss = 0.00831896\n",
            "Iteration 108, loss = 0.00821396\n",
            "Iteration 109, loss = 0.00805595\n",
            "Iteration 110, loss = 0.00794587\n",
            "Iteration 111, loss = 0.00785581\n",
            "Iteration 112, loss = 0.00776754\n",
            "Iteration 113, loss = 0.00781498\n",
            "Iteration 114, loss = 0.00752804\n",
            "Iteration 115, loss = 0.00743621\n",
            "Iteration 116, loss = 0.00736101\n",
            "Iteration 117, loss = 0.00719967\n",
            "Iteration 118, loss = 0.00709220\n",
            "Iteration 119, loss = 0.00704440\n",
            "Iteration 120, loss = 0.00692308\n",
            "Iteration 121, loss = 0.00693222\n",
            "Iteration 122, loss = 0.00676888\n",
            "Iteration 123, loss = 0.00666336\n",
            "Iteration 124, loss = 0.00665390\n",
            "Iteration 125, loss = 0.00648648\n",
            "Iteration 126, loss = 0.00642339\n",
            "Iteration 127, loss = 0.00650599\n",
            "Iteration 128, loss = 0.00633344\n",
            "Iteration 129, loss = 0.00619040\n",
            "Iteration 130, loss = 0.00611715\n",
            "Iteration 131, loss = 0.00605673\n",
            "Iteration 132, loss = 0.00598341\n",
            "Iteration 133, loss = 0.00589591\n",
            "Iteration 134, loss = 0.00583009\n",
            "Iteration 135, loss = 0.00580617\n",
            "Iteration 136, loss = 0.00572251\n",
            "Iteration 137, loss = 0.00567365\n",
            "Iteration 138, loss = 0.00558354\n",
            "Iteration 139, loss = 0.00552491\n",
            "Iteration 140, loss = 0.00546771\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.35162665\n",
            "Iteration 2, loss = 1.91677482\n",
            "Iteration 3, loss = 1.38508753\n",
            "Iteration 4, loss = 1.11990439\n",
            "Iteration 5, loss = 0.92340529\n",
            "Iteration 6, loss = 0.84604040\n",
            "Iteration 7, loss = 0.75712192\n",
            "Iteration 8, loss = 0.78983878\n",
            "Iteration 9, loss = 1.13351841\n",
            "Iteration 10, loss = 0.83478679\n",
            "Iteration 11, loss = 0.58113290\n",
            "Iteration 12, loss = 0.53006810\n",
            "Iteration 13, loss = 0.49171076\n",
            "Iteration 14, loss = 0.43445158\n",
            "Iteration 15, loss = 0.45625272\n",
            "Iteration 16, loss = 0.41106014\n",
            "Iteration 17, loss = 0.34054985\n",
            "Iteration 18, loss = 0.31963474\n",
            "Iteration 19, loss = 0.28407466\n",
            "Iteration 20, loss = 0.26242305\n",
            "Iteration 21, loss = 0.25810356\n",
            "Iteration 22, loss = 0.27322302\n",
            "Iteration 23, loss = 0.23479189\n",
            "Iteration 24, loss = 0.20863750\n",
            "Iteration 25, loss = 0.37133305\n",
            "Iteration 26, loss = 0.20250860\n",
            "Iteration 27, loss = 0.20165467\n",
            "Iteration 28, loss = 0.15785886\n",
            "Iteration 29, loss = 0.14913791\n",
            "Iteration 30, loss = 0.14076778\n",
            "Iteration 31, loss = 0.13338071\n",
            "Iteration 32, loss = 0.13998820\n",
            "Iteration 33, loss = 0.12256474\n",
            "Iteration 34, loss = 0.10438197\n",
            "Iteration 35, loss = 0.10159198\n",
            "Iteration 36, loss = 0.09047437\n",
            "Iteration 37, loss = 0.10212057\n",
            "Iteration 38, loss = 0.09533774\n",
            "Iteration 39, loss = 0.09139622\n",
            "Iteration 40, loss = 0.10292753\n",
            "Iteration 41, loss = 0.13056085\n",
            "Iteration 42, loss = 0.07281636\n",
            "Iteration 43, loss = 0.06756727\n",
            "Iteration 44, loss = 0.05853642\n",
            "Iteration 45, loss = 0.05840409\n",
            "Iteration 46, loss = 0.05155042\n",
            "Iteration 47, loss = 0.04920747\n",
            "Iteration 48, loss = 0.04932648\n",
            "Iteration 49, loss = 0.04014686\n",
            "Iteration 50, loss = 0.03873032\n",
            "Iteration 51, loss = 0.04227139\n",
            "Iteration 52, loss = 0.04592878\n",
            "Iteration 53, loss = 0.03464110\n",
            "Iteration 54, loss = 0.03353352\n",
            "Iteration 55, loss = 0.03118873\n",
            "Iteration 56, loss = 0.02849521\n",
            "Iteration 57, loss = 0.02700447\n",
            "Iteration 58, loss = 0.02716861\n",
            "Iteration 59, loss = 0.02572979\n",
            "Iteration 60, loss = 0.02487532\n",
            "Iteration 61, loss = 0.02423654\n",
            "Iteration 62, loss = 0.02378385\n",
            "Iteration 63, loss = 0.02213591\n",
            "Iteration 64, loss = 0.02063424\n",
            "Iteration 65, loss = 0.02016113\n",
            "Iteration 66, loss = 0.02099711\n",
            "Iteration 67, loss = 0.01975084\n",
            "Iteration 68, loss = 0.01825992\n",
            "Iteration 69, loss = 0.01744999\n",
            "Iteration 70, loss = 0.01696413\n",
            "Iteration 71, loss = 0.01706913\n",
            "Iteration 72, loss = 0.01705353\n",
            "Iteration 73, loss = 0.01564051\n",
            "Iteration 74, loss = 0.01519633\n",
            "Iteration 75, loss = 0.01506756\n",
            "Iteration 76, loss = 0.01449630\n",
            "Iteration 77, loss = 0.01386999\n",
            "Iteration 78, loss = 0.01392555\n",
            "Iteration 79, loss = 0.01342445\n",
            "Iteration 80, loss = 0.01286279\n",
            "Iteration 81, loss = 0.01279616\n",
            "Iteration 82, loss = 0.01240683\n",
            "Iteration 83, loss = 0.01219801\n",
            "Iteration 84, loss = 0.01221600\n",
            "Iteration 85, loss = 0.01161524\n",
            "Iteration 86, loss = 0.01129294\n",
            "Iteration 87, loss = 0.01116506\n",
            "Iteration 88, loss = 0.01108385\n",
            "Iteration 89, loss = 0.01071861\n",
            "Iteration 90, loss = 0.01044450\n",
            "Iteration 91, loss = 0.01050600\n",
            "Iteration 92, loss = 0.01016819\n",
            "Iteration 93, loss = 0.00988078\n",
            "Iteration 94, loss = 0.00972132\n",
            "Iteration 95, loss = 0.00954878\n",
            "Iteration 96, loss = 0.00936693\n",
            "Iteration 97, loss = 0.00930721\n",
            "Iteration 98, loss = 0.00906527\n",
            "Iteration 99, loss = 0.00890511\n",
            "Iteration 100, loss = 0.00884773\n",
            "Iteration 101, loss = 0.00861838\n",
            "Iteration 102, loss = 0.00854455\n",
            "Iteration 103, loss = 0.00834178\n",
            "Iteration 104, loss = 0.00825553\n",
            "Iteration 105, loss = 0.00816049\n",
            "Iteration 106, loss = 0.00823150\n",
            "Iteration 107, loss = 0.00789033\n",
            "Iteration 108, loss = 0.00776369\n",
            "Iteration 109, loss = 0.00763180\n",
            "Iteration 110, loss = 0.00755721\n",
            "Iteration 111, loss = 0.00738576\n",
            "Iteration 112, loss = 0.00734484\n",
            "Iteration 113, loss = 0.00726747\n",
            "Iteration 114, loss = 0.00710958\n",
            "Iteration 115, loss = 0.00723736\n",
            "Iteration 116, loss = 0.00695335\n",
            "Iteration 117, loss = 0.00686801\n",
            "Iteration 118, loss = 0.00675861\n",
            "Iteration 119, loss = 0.00667133\n",
            "Iteration 120, loss = 0.00666269\n",
            "Iteration 121, loss = 0.00649161\n",
            "Iteration 122, loss = 0.00641419\n",
            "Iteration 123, loss = 0.00638618\n",
            "Iteration 124, loss = 0.00627662\n",
            "Iteration 125, loss = 0.00626837\n",
            "Iteration 126, loss = 0.00613008\n",
            "Iteration 127, loss = 0.00607379\n",
            "Iteration 128, loss = 0.00598559\n",
            "Iteration 129, loss = 0.00592629\n",
            "Iteration 130, loss = 0.00581484\n",
            "Iteration 131, loss = 0.00580049\n",
            "Iteration 132, loss = 0.00570596\n",
            "Iteration 133, loss = 0.00564086\n",
            "Iteration 134, loss = 0.00555898\n",
            "Iteration 135, loss = 0.00554382\n",
            "Iteration 136, loss = 0.00546773\n",
            "Iteration 137, loss = 0.00540493\n",
            "Iteration 138, loss = 0.00532511\n",
            "Iteration 139, loss = 0.00527095\n",
            "Iteration 140, loss = 0.00524332\n",
            "Iteration 141, loss = 0.00516411\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.33769388\n",
            "Iteration 2, loss = 2.03142490\n",
            "Iteration 3, loss = 1.44578508\n",
            "Iteration 4, loss = 1.09399194\n",
            "Iteration 5, loss = 0.87318128\n",
            "Iteration 6, loss = 0.87876515\n",
            "Iteration 7, loss = 0.84688805\n",
            "Iteration 8, loss = 0.80838813\n",
            "Iteration 9, loss = 0.66861454\n",
            "Iteration 10, loss = 0.67493179\n",
            "Iteration 11, loss = 0.55453392\n",
            "Iteration 12, loss = 0.48222190\n",
            "Iteration 13, loss = 0.45814760\n",
            "Iteration 14, loss = 0.41437037\n",
            "Iteration 15, loss = 0.44300338\n",
            "Iteration 16, loss = 0.56540788\n",
            "Iteration 17, loss = 0.37104915\n",
            "Iteration 18, loss = 0.31572379\n",
            "Iteration 19, loss = 0.29602502\n",
            "Iteration 20, loss = 0.27445449\n",
            "Iteration 21, loss = 0.24676856\n",
            "Iteration 22, loss = 0.26805932\n",
            "Iteration 23, loss = 0.22125504\n",
            "Iteration 24, loss = 0.20714810\n",
            "Iteration 25, loss = 0.23601708\n",
            "Iteration 26, loss = 0.28874255\n",
            "Iteration 27, loss = 0.19151620\n",
            "Iteration 28, loss = 0.15445509\n",
            "Iteration 29, loss = 0.14833761\n",
            "Iteration 30, loss = 0.13036790\n",
            "Iteration 31, loss = 0.13610170\n",
            "Iteration 32, loss = 0.11793357\n",
            "Iteration 33, loss = 0.11253978\n",
            "Iteration 34, loss = 0.11835984\n",
            "Iteration 35, loss = 0.11490406\n",
            "Iteration 36, loss = 0.15755869\n",
            "Iteration 37, loss = 0.11460129\n",
            "Iteration 38, loss = 0.09638315\n",
            "Iteration 39, loss = 0.09739247\n",
            "Iteration 40, loss = 0.07692125\n",
            "Iteration 41, loss = 0.07043478\n",
            "Iteration 42, loss = 0.06877869\n",
            "Iteration 43, loss = 0.05972528\n",
            "Iteration 44, loss = 0.06384356\n",
            "Iteration 45, loss = 0.05960574\n",
            "Iteration 46, loss = 0.04788921\n",
            "Iteration 47, loss = 0.04662441\n",
            "Iteration 48, loss = 0.04618822\n",
            "Iteration 49, loss = 0.04410236\n",
            "Iteration 50, loss = 0.03993342\n",
            "Iteration 51, loss = 0.04573905\n",
            "Iteration 52, loss = 0.04642542\n",
            "Iteration 53, loss = 0.03451834\n",
            "Iteration 54, loss = 0.03253282\n",
            "Iteration 55, loss = 0.03069136\n",
            "Iteration 56, loss = 0.03143181\n",
            "Iteration 57, loss = 0.02955988\n",
            "Iteration 58, loss = 0.02964601\n",
            "Iteration 59, loss = 0.02803023\n",
            "Iteration 60, loss = 0.02868362\n",
            "Iteration 61, loss = 0.02416498\n",
            "Iteration 62, loss = 0.02334050\n",
            "Iteration 63, loss = 0.02254366\n",
            "Iteration 64, loss = 0.02210197\n",
            "Iteration 65, loss = 0.02123470\n",
            "Iteration 66, loss = 0.02069164\n",
            "Iteration 67, loss = 0.01948398\n",
            "Iteration 68, loss = 0.01933221\n",
            "Iteration 69, loss = 0.01829413\n",
            "Iteration 70, loss = 0.01794775\n",
            "Iteration 71, loss = 0.01734905\n",
            "Iteration 72, loss = 0.01699305\n",
            "Iteration 73, loss = 0.01646577\n",
            "Iteration 74, loss = 0.01584697\n",
            "Iteration 75, loss = 0.01563019\n",
            "Iteration 76, loss = 0.01521618\n",
            "Iteration 77, loss = 0.01470908\n",
            "Iteration 78, loss = 0.01462692\n",
            "Iteration 79, loss = 0.01424670\n",
            "Iteration 80, loss = 0.01373483\n",
            "Iteration 81, loss = 0.01337840\n",
            "Iteration 82, loss = 0.01314185\n",
            "Iteration 83, loss = 0.01288122\n",
            "Iteration 84, loss = 0.01277308\n",
            "Iteration 85, loss = 0.01235841\n",
            "Iteration 86, loss = 0.01230990\n",
            "Iteration 87, loss = 0.01205196\n",
            "Iteration 88, loss = 0.01197587\n",
            "Iteration 89, loss = 0.01133210\n",
            "Iteration 90, loss = 0.01109923\n",
            "Iteration 91, loss = 0.01107852\n",
            "Iteration 92, loss = 0.01072519\n",
            "Iteration 93, loss = 0.01055121\n",
            "Iteration 94, loss = 0.01027680\n",
            "Iteration 95, loss = 0.01008283\n",
            "Iteration 96, loss = 0.00989989\n",
            "Iteration 97, loss = 0.00997300\n",
            "Iteration 98, loss = 0.00966151\n",
            "Iteration 99, loss = 0.00937014\n",
            "Iteration 100, loss = 0.00922762\n",
            "Iteration 101, loss = 0.00911338\n",
            "Iteration 102, loss = 0.00898479\n",
            "Iteration 103, loss = 0.00902881\n",
            "Iteration 104, loss = 0.00871741\n",
            "Iteration 105, loss = 0.00869388\n",
            "Iteration 106, loss = 0.00842265\n",
            "Iteration 107, loss = 0.00826772\n",
            "Iteration 108, loss = 0.00814237\n",
            "Iteration 109, loss = 0.00805134\n",
            "Iteration 110, loss = 0.00798306\n",
            "Iteration 111, loss = 0.00791660\n",
            "Iteration 112, loss = 0.00768149\n",
            "Iteration 113, loss = 0.00768544\n",
            "Iteration 114, loss = 0.00755123\n",
            "Iteration 115, loss = 0.00750238\n",
            "Iteration 116, loss = 0.00744837\n",
            "Iteration 117, loss = 0.00717869\n",
            "Iteration 118, loss = 0.00706550\n",
            "Iteration 119, loss = 0.00697737\n",
            "Iteration 120, loss = 0.00688989\n",
            "Iteration 121, loss = 0.00685567\n",
            "Iteration 122, loss = 0.00670782\n",
            "Iteration 123, loss = 0.00668743\n",
            "Iteration 124, loss = 0.00654794\n",
            "Iteration 125, loss = 0.00645609\n",
            "Iteration 126, loss = 0.00646650\n",
            "Iteration 127, loss = 0.00632456\n",
            "Iteration 128, loss = 0.00623823\n",
            "Iteration 129, loss = 0.00620188\n",
            "Iteration 130, loss = 0.00609069\n",
            "Iteration 131, loss = 0.00604117\n",
            "Iteration 132, loss = 0.00597461\n",
            "Iteration 133, loss = 0.00587614\n",
            "Iteration 134, loss = 0.00581209\n",
            "Iteration 135, loss = 0.00575970\n",
            "Iteration 136, loss = 0.00569546\n",
            "Iteration 137, loss = 0.00565999\n",
            "Iteration 138, loss = 0.00559949\n",
            "Iteration 139, loss = 0.00552689\n",
            "Iteration 140, loss = 0.00547853\n",
            "Iteration 141, loss = 0.00545837\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.35534027\n",
            "Iteration 2, loss = 1.88171656\n",
            "Iteration 3, loss = 1.48112628\n",
            "Iteration 4, loss = 1.23774334\n",
            "Iteration 5, loss = 0.99617590\n",
            "Iteration 6, loss = 0.82208974\n",
            "Iteration 7, loss = 0.75297022\n",
            "Iteration 8, loss = 0.77757860\n",
            "Iteration 9, loss = 0.74366549\n",
            "Iteration 10, loss = 0.59522227\n",
            "Iteration 11, loss = 0.54487184\n",
            "Iteration 12, loss = 0.53506968\n",
            "Iteration 13, loss = 0.53927956\n",
            "Iteration 14, loss = 0.47964433\n",
            "Iteration 15, loss = 0.41542659\n",
            "Iteration 16, loss = 0.37905545\n",
            "Iteration 17, loss = 0.34367600\n",
            "Iteration 18, loss = 0.32647339\n",
            "Iteration 19, loss = 0.32891336\n",
            "Iteration 20, loss = 0.30198299\n",
            "Iteration 21, loss = 0.27987492\n",
            "Iteration 22, loss = 0.24609851\n",
            "Iteration 23, loss = 0.26169439\n",
            "Iteration 24, loss = 0.20945589\n",
            "Iteration 25, loss = 0.18879013\n",
            "Iteration 26, loss = 0.25441889\n",
            "Iteration 27, loss = 0.22502079\n",
            "Iteration 28, loss = 0.17913209\n",
            "Iteration 29, loss = 0.16474251\n",
            "Iteration 30, loss = 0.14523114\n",
            "Iteration 31, loss = 0.13515604\n",
            "Iteration 32, loss = 0.13360606\n",
            "Iteration 33, loss = 0.37266618\n",
            "Iteration 34, loss = 0.16621381\n",
            "Iteration 35, loss = 0.19943017\n",
            "Iteration 36, loss = 0.21508245\n",
            "Iteration 37, loss = 0.13816403\n",
            "Iteration 38, loss = 0.09550568\n",
            "Iteration 39, loss = 0.08509168\n",
            "Iteration 40, loss = 0.08291514\n",
            "Iteration 41, loss = 0.07353906\n",
            "Iteration 42, loss = 0.07450617\n",
            "Iteration 43, loss = 0.07380434\n",
            "Iteration 44, loss = 0.06794291\n",
            "Iteration 45, loss = 0.06173802\n",
            "Iteration 46, loss = 0.05647070\n",
            "Iteration 47, loss = 0.05085925\n",
            "Iteration 48, loss = 0.04960961\n",
            "Iteration 49, loss = 0.04636446\n",
            "Iteration 50, loss = 0.04237533\n",
            "Iteration 51, loss = 0.04237347\n",
            "Iteration 52, loss = 0.04268549\n",
            "Iteration 53, loss = 0.04020682\n",
            "Iteration 54, loss = 0.03598143\n",
            "Iteration 55, loss = 0.03510025\n",
            "Iteration 56, loss = 0.03762891\n",
            "Iteration 57, loss = 0.03187541\n",
            "Iteration 58, loss = 0.03062376\n",
            "Iteration 59, loss = 0.02849870\n",
            "Iteration 60, loss = 0.03008278\n",
            "Iteration 61, loss = 0.02806896\n",
            "Iteration 62, loss = 0.02533071\n",
            "Iteration 63, loss = 0.02421181\n",
            "Iteration 64, loss = 0.02468843\n",
            "Iteration 65, loss = 0.02429546\n",
            "Iteration 66, loss = 0.02310550\n",
            "Iteration 67, loss = 0.02177552\n",
            "Iteration 68, loss = 0.02102915\n",
            "Iteration 69, loss = 0.01984699\n",
            "Iteration 70, loss = 0.01919061\n",
            "Iteration 71, loss = 0.01875079\n",
            "Iteration 72, loss = 0.01813070\n",
            "Iteration 73, loss = 0.01747126\n",
            "Iteration 74, loss = 0.01703001\n",
            "Iteration 75, loss = 0.01651017\n",
            "Iteration 76, loss = 0.01604940\n",
            "Iteration 77, loss = 0.01563335\n",
            "Iteration 78, loss = 0.01552511\n",
            "Iteration 79, loss = 0.01509643\n",
            "Iteration 80, loss = 0.01492251\n",
            "Iteration 81, loss = 0.01455055\n",
            "Iteration 82, loss = 0.01407964\n",
            "Iteration 83, loss = 0.01403733\n",
            "Iteration 84, loss = 0.01332705\n",
            "Iteration 85, loss = 0.01296996\n",
            "Iteration 86, loss = 0.01264027\n",
            "Iteration 87, loss = 0.01231752\n",
            "Iteration 88, loss = 0.01228891\n",
            "Iteration 89, loss = 0.01199175\n",
            "Iteration 90, loss = 0.01183500\n",
            "Iteration 91, loss = 0.01176502\n",
            "Iteration 92, loss = 0.01124767\n",
            "Iteration 93, loss = 0.01103695\n",
            "Iteration 94, loss = 0.01081814\n",
            "Iteration 95, loss = 0.01119361\n",
            "Iteration 96, loss = 0.01043800\n",
            "Iteration 97, loss = 0.01016803\n",
            "Iteration 98, loss = 0.01020042\n",
            "Iteration 99, loss = 0.01007400\n",
            "Iteration 100, loss = 0.00979805\n",
            "Iteration 101, loss = 0.00945622\n",
            "Iteration 102, loss = 0.00932779\n",
            "Iteration 103, loss = 0.00932437\n",
            "Iteration 104, loss = 0.00931647\n",
            "Iteration 105, loss = 0.00891358\n",
            "Iteration 106, loss = 0.00885588\n",
            "Iteration 107, loss = 0.00866222\n",
            "Iteration 108, loss = 0.00855863\n",
            "Iteration 109, loss = 0.00841295\n",
            "Iteration 110, loss = 0.00822161\n",
            "Iteration 111, loss = 0.00810680\n",
            "Iteration 112, loss = 0.00799115\n",
            "Iteration 113, loss = 0.00786166\n",
            "Iteration 114, loss = 0.00774809\n",
            "Iteration 115, loss = 0.00791384\n",
            "Iteration 116, loss = 0.00807118\n",
            "Iteration 117, loss = 0.00745536\n",
            "Iteration 118, loss = 0.00735393\n",
            "Iteration 119, loss = 0.00723454\n",
            "Iteration 120, loss = 0.00712583\n",
            "Iteration 121, loss = 0.00701789\n",
            "Iteration 122, loss = 0.00699822\n",
            "Iteration 123, loss = 0.00686816\n",
            "Iteration 124, loss = 0.00681532\n",
            "Iteration 125, loss = 0.00670578\n",
            "Iteration 126, loss = 0.00665245\n",
            "Iteration 127, loss = 0.00662861\n",
            "Iteration 128, loss = 0.00652504\n",
            "Iteration 129, loss = 0.00646750\n",
            "Iteration 130, loss = 0.00634247\n",
            "Iteration 131, loss = 0.00623898\n",
            "Iteration 132, loss = 0.00618264\n",
            "Iteration 133, loss = 0.00610658\n",
            "Iteration 134, loss = 0.00602224\n",
            "Iteration 135, loss = 0.00598386\n",
            "Iteration 136, loss = 0.00588163\n",
            "Iteration 137, loss = 0.00584472\n",
            "Iteration 138, loss = 0.00573728\n",
            "Iteration 139, loss = 0.00572932\n",
            "Iteration 140, loss = 0.00562839\n",
            "Iteration 141, loss = 0.00567147\n",
            "Iteration 142, loss = 0.00549316\n",
            "Iteration 143, loss = 0.00544522\n",
            "Iteration 144, loss = 0.00539051\n",
            "Iteration 145, loss = 0.00533007\n",
            "Iteration 146, loss = 0.00527744\n",
            "Iteration 147, loss = 0.00523447\n",
            "Iteration 148, loss = 0.00519074\n",
            "Iteration 149, loss = 0.00531754\n",
            "Iteration 150, loss = 0.00507245\n",
            "Iteration 151, loss = 0.00503968\n",
            "Iteration 152, loss = 0.00497720\n",
            "Iteration 153, loss = 0.00493859\n",
            "Iteration 154, loss = 0.00488080\n",
            "Iteration 155, loss = 0.00486134\n",
            "Iteration 156, loss = 0.00486967\n",
            "Iteration 157, loss = 0.00473390\n",
            "Iteration 158, loss = 0.00474658\n",
            "Iteration 159, loss = 0.00470663\n",
            "Iteration 160, loss = 0.00462055\n",
            "Iteration 161, loss = 0.00468181\n",
            "Iteration 162, loss = 0.00454531\n",
            "Iteration 163, loss = 0.00450429\n",
            "Iteration 164, loss = 0.00444527\n",
            "Iteration 165, loss = 0.00441463\n",
            "Iteration 166, loss = 0.00438806\n",
            "Iteration 167, loss = 0.00433253\n",
            "Iteration 168, loss = 0.00430008\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.42815270\n",
            "Iteration 2, loss = 1.91529373\n",
            "Iteration 3, loss = 1.44193237\n",
            "Iteration 4, loss = 1.12817299\n",
            "Iteration 5, loss = 0.98601683\n",
            "Iteration 6, loss = 0.97588432\n",
            "Iteration 7, loss = 0.83219188\n",
            "Iteration 8, loss = 0.74375625\n",
            "Iteration 9, loss = 0.78795746\n",
            "Iteration 10, loss = 0.58634014\n",
            "Iteration 11, loss = 0.56117250\n",
            "Iteration 12, loss = 0.58595091\n",
            "Iteration 13, loss = 0.49557386\n",
            "Iteration 14, loss = 0.49920650\n",
            "Iteration 15, loss = 0.44067425\n",
            "Iteration 16, loss = 0.36700409\n",
            "Iteration 17, loss = 0.36073366\n",
            "Iteration 18, loss = 0.31107183\n",
            "Iteration 19, loss = 0.28865932\n",
            "Iteration 20, loss = 0.27399931\n",
            "Iteration 21, loss = 0.25128373\n",
            "Iteration 22, loss = 0.22247443\n",
            "Iteration 23, loss = 0.22611983\n",
            "Iteration 24, loss = 0.19705916\n",
            "Iteration 25, loss = 0.21877036\n",
            "Iteration 26, loss = 0.65447327\n",
            "Iteration 27, loss = 0.39145783\n",
            "Iteration 28, loss = 0.23596707\n",
            "Iteration 29, loss = 0.18273405\n",
            "Iteration 30, loss = 0.16322756\n",
            "Iteration 31, loss = 0.14953605\n",
            "Iteration 32, loss = 0.14534222\n",
            "Iteration 33, loss = 0.14373059\n",
            "Iteration 34, loss = 0.11652239\n",
            "Iteration 35, loss = 0.11936755\n",
            "Iteration 36, loss = 0.10828030\n",
            "Iteration 37, loss = 0.09595300\n",
            "Iteration 38, loss = 0.08948701\n",
            "Iteration 39, loss = 0.08579923\n",
            "Iteration 40, loss = 0.09910988\n",
            "Iteration 41, loss = 0.08079608\n",
            "Iteration 42, loss = 0.06914188\n",
            "Iteration 43, loss = 0.07243953\n",
            "Iteration 44, loss = 0.06903847\n",
            "Iteration 45, loss = 0.06909743\n",
            "Iteration 46, loss = 0.05344682\n",
            "Iteration 47, loss = 0.05128945\n",
            "Iteration 48, loss = 0.04944072\n",
            "Iteration 49, loss = 0.04768018\n",
            "Iteration 50, loss = 0.04247264\n",
            "Iteration 51, loss = 0.04189634\n",
            "Iteration 52, loss = 0.04422449\n",
            "Iteration 53, loss = 0.04900998\n",
            "Iteration 54, loss = 0.03598181\n",
            "Iteration 55, loss = 0.03468959\n",
            "Iteration 56, loss = 0.03731217\n",
            "Iteration 57, loss = 0.03297442\n",
            "Iteration 58, loss = 0.03068108\n",
            "Iteration 59, loss = 0.02962734\n",
            "Iteration 60, loss = 0.02878290\n",
            "Iteration 61, loss = 0.02628209\n",
            "Iteration 62, loss = 0.02501891\n",
            "Iteration 63, loss = 0.02405089\n",
            "Iteration 64, loss = 0.02376494\n",
            "Iteration 65, loss = 0.02428919\n",
            "Iteration 66, loss = 0.02357933\n",
            "Iteration 67, loss = 0.02139325\n",
            "Iteration 68, loss = 0.02181559\n",
            "Iteration 69, loss = 0.02016194\n",
            "Iteration 70, loss = 0.01965286\n",
            "Iteration 71, loss = 0.01860953\n",
            "Iteration 72, loss = 0.01838341\n",
            "Iteration 73, loss = 0.01765331\n",
            "Iteration 74, loss = 0.01724441\n",
            "Iteration 75, loss = 0.01673120\n",
            "Iteration 76, loss = 0.01618566\n",
            "Iteration 77, loss = 0.01593058\n",
            "Iteration 78, loss = 0.01583472\n",
            "Iteration 79, loss = 0.01514028\n",
            "Iteration 80, loss = 0.01539624\n",
            "Iteration 81, loss = 0.01471309\n",
            "Iteration 82, loss = 0.01433805\n",
            "Iteration 83, loss = 0.01451357\n",
            "Iteration 84, loss = 0.01360840\n",
            "Iteration 85, loss = 0.01316991\n",
            "Iteration 86, loss = 0.01283450\n",
            "Iteration 87, loss = 0.01259228\n",
            "Iteration 88, loss = 0.01228791\n",
            "Iteration 89, loss = 0.01200494\n",
            "Iteration 90, loss = 0.01182087\n",
            "Iteration 91, loss = 0.01216390\n",
            "Iteration 92, loss = 0.01145604\n",
            "Iteration 93, loss = 0.01118222\n",
            "Iteration 94, loss = 0.01093574\n",
            "Iteration 95, loss = 0.01084966\n",
            "Iteration 96, loss = 0.01054653\n",
            "Iteration 97, loss = 0.01037922\n",
            "Iteration 98, loss = 0.01053723\n",
            "Iteration 99, loss = 0.01000365\n",
            "Iteration 100, loss = 0.00989153\n",
            "Iteration 101, loss = 0.00964768\n",
            "Iteration 102, loss = 0.00963076\n",
            "Iteration 103, loss = 0.00955786\n",
            "Iteration 104, loss = 0.00959833\n",
            "Iteration 105, loss = 0.00908269\n",
            "Iteration 106, loss = 0.00894696\n",
            "Iteration 107, loss = 0.00887754\n",
            "Iteration 108, loss = 0.00879504\n",
            "Iteration 109, loss = 0.00858211\n",
            "Iteration 110, loss = 0.00836341\n",
            "Iteration 111, loss = 0.00827929\n",
            "Iteration 112, loss = 0.00829966\n",
            "Iteration 113, loss = 0.00805424\n",
            "Iteration 114, loss = 0.00789516\n",
            "Iteration 115, loss = 0.00794454\n",
            "Iteration 116, loss = 0.00793086\n",
            "Iteration 117, loss = 0.00761834\n",
            "Iteration 118, loss = 0.00748052\n",
            "Iteration 119, loss = 0.00734512\n",
            "Iteration 120, loss = 0.00726728\n",
            "Iteration 121, loss = 0.00717358\n",
            "Iteration 122, loss = 0.00713520\n",
            "Iteration 123, loss = 0.00705368\n",
            "Iteration 124, loss = 0.00699228\n",
            "Iteration 125, loss = 0.00683567\n",
            "Iteration 126, loss = 0.00676337\n",
            "Iteration 127, loss = 0.00667840\n",
            "Iteration 128, loss = 0.00661275\n",
            "Iteration 129, loss = 0.00655601\n",
            "Iteration 130, loss = 0.00646281\n",
            "Iteration 131, loss = 0.00643071\n",
            "Iteration 132, loss = 0.00626605\n",
            "Iteration 133, loss = 0.00622221\n",
            "Iteration 134, loss = 0.00612746\n",
            "Iteration 135, loss = 0.00604128\n",
            "Iteration 136, loss = 0.00598491\n",
            "Iteration 137, loss = 0.00600465\n",
            "Iteration 138, loss = 0.00586559\n",
            "Iteration 139, loss = 0.00586222\n",
            "Iteration 140, loss = 0.00572594\n",
            "Iteration 141, loss = 0.00578347\n",
            "Iteration 142, loss = 0.00560376\n",
            "Iteration 143, loss = 0.00554268\n",
            "Iteration 144, loss = 0.00551908\n",
            "Iteration 145, loss = 0.00546745\n",
            "Iteration 146, loss = 0.00537876\n",
            "Iteration 147, loss = 0.00533040\n",
            "Iteration 148, loss = 0.00528708\n",
            "Iteration 149, loss = 0.00529621\n",
            "Iteration 150, loss = 0.00518732\n",
            "Iteration 151, loss = 0.00511025\n",
            "Iteration 152, loss = 0.00509760\n",
            "Iteration 153, loss = 0.00504561\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.44770069\n",
            "Iteration 2, loss = 2.29425100\n",
            "Iteration 3, loss = 2.17317050\n",
            "Iteration 4, loss = 2.11670413\n",
            "Iteration 5, loss = 2.51181989\n",
            "Iteration 6, loss = 2.15646576\n",
            "Iteration 7, loss = 2.09613906\n",
            "Iteration 8, loss = 1.91524519\n",
            "Iteration 9, loss = 1.85474761\n",
            "Iteration 10, loss = 2.23276430\n",
            "Iteration 11, loss = 2.19347505\n",
            "Iteration 12, loss = 2.17401161\n",
            "Iteration 13, loss = 2.15363425\n",
            "Iteration 14, loss = 2.12820107\n",
            "Iteration 15, loss = 2.08841995\n",
            "Iteration 16, loss = 2.02766406\n",
            "Iteration 17, loss = 1.95368087\n",
            "Iteration 18, loss = 1.84881197\n",
            "Iteration 19, loss = 1.78091585\n",
            "Iteration 20, loss = 1.71321195\n",
            "Iteration 21, loss = 1.68193338\n",
            "Iteration 22, loss = 1.65090827\n",
            "Iteration 23, loss = 1.63735652\n",
            "Iteration 24, loss = 1.61995731\n",
            "Iteration 25, loss = 1.59927416\n",
            "Iteration 26, loss = 1.54376579\n",
            "Iteration 27, loss = 1.51132761\n",
            "Iteration 28, loss = 1.49570612\n",
            "Iteration 29, loss = 1.47312968\n",
            "Iteration 30, loss = 1.43848341\n",
            "Iteration 31, loss = 1.41505934\n",
            "Iteration 32, loss = 1.38968047\n",
            "Iteration 33, loss = 1.36440918\n",
            "Iteration 34, loss = 1.35016780\n",
            "Iteration 35, loss = 1.33160461\n",
            "Iteration 36, loss = 1.32240676\n",
            "Iteration 37, loss = 1.30260979\n",
            "Iteration 38, loss = 1.26144138\n",
            "Iteration 39, loss = 1.25924004\n",
            "Iteration 40, loss = 1.23262398\n",
            "Iteration 41, loss = 1.21456189\n",
            "Iteration 42, loss = 1.18648822\n",
            "Iteration 43, loss = 1.18784699\n",
            "Iteration 44, loss = 1.24938424\n",
            "Iteration 45, loss = 1.16249231\n",
            "Iteration 46, loss = 1.13302081\n",
            "Iteration 47, loss = 1.11116600\n",
            "Iteration 48, loss = 1.09529339\n",
            "Iteration 49, loss = 1.07244703\n",
            "Iteration 50, loss = 1.06271100\n",
            "Iteration 51, loss = 1.04782964\n",
            "Iteration 52, loss = 1.05089869\n",
            "Iteration 53, loss = 1.06769073\n",
            "Iteration 54, loss = 1.12742449\n",
            "Iteration 55, loss = 1.07732305\n",
            "Iteration 56, loss = 1.04890892\n",
            "Iteration 57, loss = 0.99128661\n",
            "Iteration 58, loss = 0.96025416\n",
            "Iteration 59, loss = 0.92839753\n",
            "Iteration 60, loss = 0.91340322\n",
            "Iteration 61, loss = 0.92425545\n",
            "Iteration 62, loss = 0.86990397\n",
            "Iteration 63, loss = 0.87664258\n",
            "Iteration 64, loss = 0.86830885\n",
            "Iteration 65, loss = 0.85625857\n",
            "Iteration 66, loss = 0.83657485\n",
            "Iteration 67, loss = 0.83040357\n",
            "Iteration 68, loss = 0.80525851\n",
            "Iteration 69, loss = 0.80611461\n",
            "Iteration 70, loss = 0.79874489\n",
            "Iteration 71, loss = 0.78073899\n",
            "Iteration 72, loss = 0.77132151\n",
            "Iteration 73, loss = 0.76656161\n",
            "Iteration 74, loss = 0.76481394\n",
            "Iteration 75, loss = 0.76001348\n",
            "Iteration 76, loss = 0.74745151\n",
            "Iteration 77, loss = 0.74210697\n",
            "Iteration 78, loss = 0.74097659\n",
            "Iteration 79, loss = 0.73772948\n",
            "Iteration 80, loss = 0.73472589\n",
            "Iteration 81, loss = 0.73033636\n",
            "Iteration 82, loss = 0.72124377\n",
            "Iteration 83, loss = 0.72389146\n",
            "Iteration 84, loss = 0.71670042\n",
            "Iteration 85, loss = 0.71897385\n",
            "Iteration 86, loss = 0.70776355\n",
            "Iteration 87, loss = 0.72342130\n",
            "Iteration 88, loss = 0.71429647\n",
            "Iteration 89, loss = 0.75201164\n",
            "Iteration 90, loss = 0.70395731\n",
            "Iteration 91, loss = 0.69205502\n",
            "Iteration 92, loss = 0.68693181\n",
            "Iteration 93, loss = 0.68544722\n",
            "Iteration 94, loss = 0.68431313\n",
            "Iteration 95, loss = 0.73279076\n",
            "Iteration 96, loss = 0.89678515\n",
            "Iteration 97, loss = 1.34708539\n",
            "Iteration 98, loss = 1.35779571\n",
            "Iteration 99, loss = 1.25977858\n",
            "Iteration 100, loss = 1.25770958\n",
            "Iteration 101, loss = 1.13787506\n",
            "Iteration 102, loss = 2.03360732\n",
            "Iteration 103, loss = 2.01968455\n",
            "Iteration 104, loss = 1.86250015\n",
            "Iteration 105, loss = 1.45737719\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.43946956\n",
            "Iteration 2, loss = 2.29250831\n",
            "Iteration 3, loss = 2.20463618\n",
            "Iteration 4, loss = 2.33164729\n",
            "Iteration 5, loss = 2.14679601\n",
            "Iteration 6, loss = 2.09174161\n",
            "Iteration 7, loss = 2.01402579\n",
            "Iteration 8, loss = 1.92419521\n",
            "Iteration 9, loss = 1.81286975\n",
            "Iteration 10, loss = 1.77532437\n",
            "Iteration 11, loss = 1.68905198\n",
            "Iteration 12, loss = 1.56600273\n",
            "Iteration 13, loss = 1.51871294\n",
            "Iteration 14, loss = 1.35301773\n",
            "Iteration 15, loss = 1.27377132\n",
            "Iteration 16, loss = 1.55495415\n",
            "Iteration 17, loss = 1.35389574\n",
            "Iteration 18, loss = 1.43100945\n",
            "Iteration 19, loss = 1.10217445\n",
            "Iteration 20, loss = 1.00744366\n",
            "Iteration 21, loss = 1.00429190\n",
            "Iteration 22, loss = 0.98868836\n",
            "Iteration 23, loss = 0.96657786\n",
            "Iteration 24, loss = 1.21913368\n",
            "Iteration 25, loss = 0.94039086\n",
            "Iteration 26, loss = 0.83880370\n",
            "Iteration 27, loss = 0.78201964\n",
            "Iteration 28, loss = 0.78598073\n",
            "Iteration 29, loss = 0.87812575\n",
            "Iteration 30, loss = 0.95987877\n",
            "Iteration 31, loss = 0.71873151\n",
            "Iteration 32, loss = 0.68257535\n",
            "Iteration 33, loss = 0.62572050\n",
            "Iteration 34, loss = 0.58505615\n",
            "Iteration 35, loss = 0.57817455\n",
            "Iteration 36, loss = 0.71779135\n",
            "Iteration 37, loss = 1.73722077\n",
            "Iteration 38, loss = 0.95291057\n",
            "Iteration 39, loss = 0.85283821\n",
            "Iteration 40, loss = 0.98228840\n",
            "Iteration 41, loss = 0.87536498\n",
            "Iteration 42, loss = 0.88952772\n",
            "Iteration 43, loss = 0.63849666\n",
            "Iteration 44, loss = 0.57867015\n",
            "Iteration 45, loss = 0.70803532\n",
            "Iteration 46, loss = 0.65062503\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.44847261\n",
            "Iteration 2, loss = 2.31453088\n",
            "Iteration 3, loss = 2.21571121\n",
            "Iteration 4, loss = 2.08550876\n",
            "Iteration 5, loss = 2.14410649\n",
            "Iteration 6, loss = 2.65720275\n",
            "Iteration 7, loss = 2.20349610\n",
            "Iteration 8, loss = 2.61577123\n",
            "Iteration 9, loss = 2.33012450\n",
            "Iteration 10, loss = 2.27632051\n",
            "Iteration 11, loss = 2.22632238\n",
            "Iteration 12, loss = 2.18672595\n",
            "Iteration 13, loss = 2.12217359\n",
            "Iteration 14, loss = 2.07162664\n",
            "Iteration 15, loss = 2.01257584\n",
            "Iteration 16, loss = 1.98824615\n",
            "Iteration 17, loss = 1.89160592\n",
            "Iteration 18, loss = 1.98014588\n",
            "Iteration 19, loss = 1.97689278\n",
            "Iteration 20, loss = 1.94989461\n",
            "Iteration 21, loss = 1.93314991\n",
            "Iteration 22, loss = 1.92209484\n",
            "Iteration 23, loss = 1.90033170\n",
            "Iteration 24, loss = 1.88614505\n",
            "Iteration 25, loss = 1.87780725\n",
            "Iteration 26, loss = 1.86272294\n",
            "Iteration 27, loss = 1.84109864\n",
            "Iteration 28, loss = 1.82524057\n",
            "Iteration 29, loss = 1.82443025\n",
            "Iteration 30, loss = 1.82449429\n",
            "Iteration 31, loss = 1.76740594\n",
            "Iteration 32, loss = 1.74670636\n",
            "Iteration 33, loss = 1.68653269\n",
            "Iteration 34, loss = 1.67164132\n",
            "Iteration 35, loss = 1.63439228\n",
            "Iteration 36, loss = 1.59379284\n",
            "Iteration 37, loss = 1.61415221\n",
            "Iteration 38, loss = 1.63775696\n",
            "Iteration 39, loss = 1.60955108\n",
            "Iteration 40, loss = 1.59542002\n",
            "Iteration 41, loss = 1.57582280\n",
            "Iteration 42, loss = 1.55751331\n",
            "Iteration 43, loss = 1.54160644\n",
            "Iteration 44, loss = 1.50667758\n",
            "Iteration 45, loss = 1.49126239\n",
            "Iteration 46, loss = 1.48227155\n",
            "Iteration 47, loss = 1.45700980\n",
            "Iteration 48, loss = 1.48136576\n",
            "Iteration 49, loss = 1.42714681\n",
            "Iteration 50, loss = 1.44157526\n",
            "Iteration 51, loss = 1.44242564\n",
            "Iteration 52, loss = 1.39646771\n",
            "Iteration 53, loss = 1.39499214\n",
            "Iteration 54, loss = 1.40489740\n",
            "Iteration 55, loss = 1.37946300\n",
            "Iteration 56, loss = 1.37839992\n",
            "Iteration 57, loss = 1.37667217\n",
            "Iteration 58, loss = 1.35901630\n",
            "Iteration 59, loss = 1.34089785\n",
            "Iteration 60, loss = 1.33760501\n",
            "Iteration 61, loss = 1.33685751\n",
            "Iteration 62, loss = 1.32832099\n",
            "Iteration 63, loss = 1.31248780\n",
            "Iteration 64, loss = 1.31190484\n",
            "Iteration 65, loss = 1.30085079\n",
            "Iteration 66, loss = 1.29802018\n",
            "Iteration 67, loss = 1.29058966\n",
            "Iteration 68, loss = 1.28970736\n",
            "Iteration 69, loss = 1.27137237\n",
            "Iteration 70, loss = 1.26273725\n",
            "Iteration 71, loss = 1.25717664\n",
            "Iteration 72, loss = 1.25665699\n",
            "Iteration 73, loss = 1.25309777\n",
            "Iteration 74, loss = 1.24183180\n",
            "Iteration 75, loss = 1.23640152\n",
            "Iteration 76, loss = 1.24035094\n",
            "Iteration 77, loss = 1.23529461\n",
            "Iteration 78, loss = 1.22630850\n",
            "Iteration 79, loss = 1.22491416\n",
            "Iteration 80, loss = 1.22423685\n",
            "Iteration 81, loss = 1.22618819\n",
            "Iteration 82, loss = 1.21462966\n",
            "Iteration 83, loss = 1.21241534\n",
            "Iteration 84, loss = 1.21314653\n",
            "Iteration 85, loss = 1.20501532\n",
            "Iteration 86, loss = 1.20013128\n",
            "Iteration 87, loss = 1.19993637\n",
            "Iteration 88, loss = 1.20149166\n",
            "Iteration 89, loss = 1.17818360\n",
            "Iteration 90, loss = 1.17802554\n",
            "Iteration 91, loss = 1.17498432\n",
            "Iteration 92, loss = 1.17120099\n",
            "Iteration 93, loss = 1.16982676\n",
            "Iteration 94, loss = 1.16897824\n",
            "Iteration 95, loss = 1.16656185\n",
            "Iteration 96, loss = 1.16533748\n",
            "Iteration 97, loss = 1.16196149\n",
            "Iteration 98, loss = 1.16141039\n",
            "Iteration 99, loss = 1.15833416\n",
            "Iteration 100, loss = 1.15863383\n",
            "Iteration 101, loss = 1.15752595\n",
            "Iteration 102, loss = 1.15459695\n",
            "Iteration 103, loss = 1.15301860\n",
            "Iteration 104, loss = 1.15323201\n",
            "Iteration 105, loss = 1.15074002\n",
            "Iteration 106, loss = 1.14988273\n",
            "Iteration 107, loss = 1.15108293\n",
            "Iteration 108, loss = 1.14848628\n",
            "Iteration 109, loss = 1.14765557\n",
            "Iteration 110, loss = 1.14938351\n",
            "Iteration 111, loss = 1.14764928\n",
            "Iteration 112, loss = 1.14738995\n",
            "Iteration 113, loss = 1.14490729\n",
            "Iteration 114, loss = 1.14565588\n",
            "Iteration 115, loss = 1.14415279\n",
            "Iteration 116, loss = 1.14252679\n",
            "Iteration 117, loss = 1.14199960\n",
            "Iteration 118, loss = 1.14095895\n",
            "Iteration 119, loss = 1.14132152\n",
            "Iteration 120, loss = 1.14083959\n",
            "Iteration 121, loss = 1.13939346\n",
            "Iteration 122, loss = 1.13984815\n",
            "Iteration 123, loss = 1.13871883\n",
            "Iteration 124, loss = 1.13770232\n",
            "Iteration 125, loss = 1.13673792\n",
            "Iteration 126, loss = 1.13742839\n",
            "Iteration 127, loss = 1.13947237\n",
            "Iteration 128, loss = 1.13714825\n",
            "Iteration 129, loss = 1.13564207\n",
            "Iteration 130, loss = 1.13436158\n",
            "Iteration 131, loss = 1.13409488\n",
            "Iteration 132, loss = 1.13640571\n",
            "Iteration 133, loss = 1.13273953\n",
            "Iteration 134, loss = 1.13292214\n",
            "Iteration 135, loss = 1.13268491\n",
            "Iteration 136, loss = 1.13285076\n",
            "Iteration 137, loss = 1.13521934\n",
            "Iteration 138, loss = 1.13161111\n",
            "Iteration 139, loss = 1.13096270\n",
            "Iteration 140, loss = 1.12945863\n",
            "Iteration 141, loss = 1.12950202\n",
            "Iteration 142, loss = 1.12901835\n",
            "Iteration 143, loss = 1.12713729\n",
            "Iteration 144, loss = 1.12935273\n",
            "Iteration 145, loss = 1.12811504\n",
            "Iteration 146, loss = 1.12444870\n",
            "Iteration 147, loss = 1.12469509\n",
            "Iteration 148, loss = 1.12817037\n",
            "Iteration 149, loss = 1.12385329\n",
            "Iteration 150, loss = 1.12217300\n",
            "Iteration 151, loss = 1.12544219\n",
            "Iteration 152, loss = 1.12122469\n",
            "Iteration 153, loss = 1.12021985\n",
            "Iteration 154, loss = 1.12198501\n",
            "Iteration 155, loss = 1.11924656\n",
            "Iteration 156, loss = 1.11897088\n",
            "Iteration 157, loss = 1.12152464\n",
            "Iteration 158, loss = 1.11941380\n",
            "Iteration 159, loss = 1.11842654\n",
            "Iteration 160, loss = 1.11910776\n",
            "Iteration 161, loss = 1.11746661\n",
            "Iteration 162, loss = 1.11980705\n",
            "Iteration 163, loss = 1.11628604\n",
            "Iteration 164, loss = 1.11661104\n",
            "Iteration 165, loss = 1.11590969\n",
            "Iteration 166, loss = 1.11911259\n",
            "Iteration 167, loss = 1.11606406\n",
            "Iteration 168, loss = 1.11690299\n",
            "Iteration 169, loss = 1.11099043\n",
            "Iteration 170, loss = 1.10648509\n",
            "Iteration 171, loss = 1.10570426\n",
            "Iteration 172, loss = 1.10580452\n",
            "Iteration 173, loss = 1.10569892\n",
            "Iteration 174, loss = 1.10530263\n",
            "Iteration 175, loss = 1.10507589\n",
            "Iteration 176, loss = 1.10465776\n",
            "Iteration 177, loss = 1.10419907\n",
            "Iteration 178, loss = 1.10384681\n",
            "Iteration 179, loss = 1.10373158\n",
            "Iteration 180, loss = 1.10351496\n",
            "Iteration 181, loss = 1.10354100\n",
            "Iteration 182, loss = 1.10342674\n",
            "Iteration 183, loss = 1.10332302\n",
            "Iteration 184, loss = 1.10304588\n",
            "Iteration 185, loss = 1.10283877\n",
            "Iteration 186, loss = 1.10235463\n",
            "Iteration 187, loss = 1.10237012\n",
            "Iteration 188, loss = 1.10274837\n",
            "Iteration 189, loss = 1.10200663\n",
            "Iteration 190, loss = 1.10219334\n",
            "Iteration 191, loss = 1.10198202\n",
            "Iteration 192, loss = 1.10181795\n",
            "Iteration 193, loss = 1.10208604\n",
            "Iteration 194, loss = 1.10150618\n",
            "Iteration 195, loss = 1.10164563\n",
            "Iteration 196, loss = 1.10117180\n",
            "Iteration 197, loss = 1.10122364\n",
            "Iteration 198, loss = 1.10092033\n",
            "Iteration 199, loss = 1.10122228\n",
            "Iteration 200, loss = 1.10090051\n",
            "Iteration 201, loss = 1.10115890\n",
            "Iteration 202, loss = 1.10078548\n",
            "Iteration 203, loss = 1.10113051\n",
            "Iteration 204, loss = 1.10035681\n",
            "Iteration 205, loss = 1.10026272\n",
            "Iteration 206, loss = 1.10008732\n",
            "Iteration 207, loss = 1.10041028\n",
            "Iteration 208, loss = 1.09986977\n",
            "Iteration 209, loss = 1.10004957\n",
            "Iteration 210, loss = 1.09951084\n",
            "Iteration 211, loss = 1.09870413\n",
            "Iteration 212, loss = 1.09842198\n",
            "Iteration 213, loss = 1.09807385\n",
            "Iteration 214, loss = 1.09794780\n",
            "Iteration 215, loss = 1.09787947\n",
            "Iteration 216, loss = 1.09737256\n",
            "Iteration 217, loss = 1.09723270\n",
            "Iteration 218, loss = 1.09705912\n",
            "Iteration 219, loss = 1.09685427\n",
            "Iteration 220, loss = 1.09662089\n",
            "Iteration 221, loss = 1.09694975\n",
            "Iteration 222, loss = 1.09652806\n",
            "Iteration 223, loss = 1.09645877\n",
            "Iteration 224, loss = 1.09613337\n",
            "Iteration 225, loss = 1.09613191\n",
            "Iteration 226, loss = 1.09613112\n",
            "Iteration 227, loss = 1.09581167\n",
            "Iteration 228, loss = 1.09564190\n",
            "Iteration 229, loss = 1.09543571\n",
            "Iteration 230, loss = 1.09524417\n",
            "Iteration 231, loss = 1.09514391\n",
            "Iteration 232, loss = 1.09512214\n",
            "Iteration 233, loss = 1.09497472\n",
            "Iteration 234, loss = 1.09474890\n",
            "Iteration 235, loss = 1.09479230\n",
            "Iteration 236, loss = 1.09465200\n",
            "Iteration 237, loss = 1.09449678\n",
            "Iteration 238, loss = 1.09422946\n",
            "Iteration 239, loss = 1.09406945\n",
            "Iteration 240, loss = 1.09392371\n",
            "Iteration 241, loss = 1.09397903\n",
            "Iteration 242, loss = 1.09366866\n",
            "Iteration 243, loss = 1.09371030\n",
            "Iteration 244, loss = 1.09354597\n",
            "Iteration 245, loss = 1.09342090\n",
            "Iteration 246, loss = 1.09311698\n",
            "Iteration 247, loss = 1.09299370\n",
            "Iteration 248, loss = 1.09291301\n",
            "Iteration 249, loss = 1.09257090\n",
            "Iteration 250, loss = 1.09285959\n",
            "Iteration 251, loss = 1.09262595\n",
            "Iteration 252, loss = 1.09239617\n",
            "Iteration 253, loss = 1.09226566\n",
            "Iteration 254, loss = 1.09215974\n",
            "Iteration 255, loss = 1.09185732\n",
            "Iteration 256, loss = 1.09183509\n",
            "Iteration 257, loss = 1.09163454\n",
            "Iteration 258, loss = 1.09145287\n",
            "Iteration 259, loss = 1.09126068\n",
            "Iteration 260, loss = 1.09102272\n",
            "Iteration 261, loss = 1.09081257\n",
            "Iteration 262, loss = 1.09058876\n",
            "Iteration 263, loss = 1.08989592\n",
            "Iteration 264, loss = 1.09292318\n",
            "Iteration 265, loss = 2.20420504\n",
            "Iteration 266, loss = 3.75671367\n",
            "Iteration 267, loss = 3.85624827\n",
            "Iteration 268, loss = 3.81215865\n",
            "Iteration 269, loss = 3.75054797\n",
            "Iteration 270, loss = 3.67558983\n",
            "Iteration 271, loss = 3.59387634\n",
            "Iteration 272, loss = 3.50772200\n",
            "Iteration 273, loss = 3.41968569\n",
            "Iteration 274, loss = 3.32264641\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.44312691\n",
            "Iteration 2, loss = 2.38572672\n",
            "Iteration 3, loss = 2.25101266\n",
            "Iteration 4, loss = 2.15127880\n",
            "Iteration 5, loss = 2.05212637\n",
            "Iteration 6, loss = 1.93849634\n",
            "Iteration 7, loss = 1.84680760\n",
            "Iteration 8, loss = 1.76642892\n",
            "Iteration 9, loss = 1.69303013\n",
            "Iteration 10, loss = 1.62319467\n",
            "Iteration 11, loss = 1.55850776\n",
            "Iteration 12, loss = 1.48347267\n",
            "Iteration 13, loss = 1.40860330\n",
            "Iteration 14, loss = 1.32079198\n",
            "Iteration 15, loss = 1.22740616\n",
            "Iteration 16, loss = 1.14442240\n",
            "Iteration 17, loss = 1.05695257\n",
            "Iteration 18, loss = 0.97288469\n",
            "Iteration 19, loss = 0.90797480\n",
            "Iteration 20, loss = 0.86178465\n",
            "Iteration 21, loss = 0.79337436\n",
            "Iteration 22, loss = 0.78571797\n",
            "Iteration 23, loss = 0.82016215\n",
            "Iteration 24, loss = 1.17933108\n",
            "Iteration 25, loss = 2.19983739\n",
            "Iteration 26, loss = 1.14126885\n",
            "Iteration 27, loss = 0.94342366\n",
            "Iteration 28, loss = 0.83758342\n",
            "Iteration 29, loss = 0.78201743\n",
            "Iteration 30, loss = 0.74932351\n",
            "Iteration 31, loss = 0.73471533\n",
            "Iteration 32, loss = 0.69184251\n",
            "Iteration 33, loss = 0.66142882\n",
            "Iteration 34, loss = 0.62781609\n",
            "Iteration 35, loss = 0.59539713\n",
            "Iteration 36, loss = 0.56885467\n",
            "Iteration 37, loss = 0.56027534\n",
            "Iteration 38, loss = 0.53964647\n",
            "Iteration 39, loss = 0.53335875\n",
            "Iteration 40, loss = 0.50923770\n",
            "Iteration 41, loss = 0.50399517\n",
            "Iteration 42, loss = 0.50023410\n",
            "Iteration 43, loss = 0.47871751\n",
            "Iteration 44, loss = 0.47886423\n",
            "Iteration 45, loss = 0.46679477\n",
            "Iteration 46, loss = 0.48700040\n",
            "Iteration 47, loss = 0.43162688\n",
            "Iteration 48, loss = 0.42243210\n",
            "Iteration 49, loss = 0.41486013\n",
            "Iteration 50, loss = 0.40134298\n",
            "Iteration 51, loss = 0.41507800\n",
            "Iteration 52, loss = 0.38286973\n",
            "Iteration 53, loss = 0.42466200\n",
            "Iteration 54, loss = 0.41545766\n",
            "Iteration 55, loss = 0.39306779\n",
            "Iteration 56, loss = 0.36027260\n",
            "Iteration 57, loss = 0.36947502\n",
            "Iteration 58, loss = 0.44732692\n",
            "Iteration 59, loss = 0.37259245\n",
            "Iteration 60, loss = 0.36253728\n",
            "Iteration 61, loss = 0.33090998\n",
            "Iteration 62, loss = 0.36891489\n",
            "Iteration 63, loss = 0.38081303\n",
            "Iteration 64, loss = 0.31152417\n",
            "Iteration 65, loss = 0.30847041\n",
            "Iteration 66, loss = 0.33130487\n",
            "Iteration 67, loss = 0.28342428\n",
            "Iteration 68, loss = 0.27429268\n",
            "Iteration 69, loss = 0.27920959\n",
            "Iteration 70, loss = 0.32612098\n",
            "Iteration 71, loss = 0.41127904\n",
            "Iteration 72, loss = 0.27887002\n",
            "Iteration 73, loss = 0.26206319\n",
            "Iteration 74, loss = 0.29826507\n",
            "Iteration 75, loss = 0.36967678\n",
            "Iteration 76, loss = 0.36256389\n",
            "Iteration 77, loss = 0.29209006\n",
            "Iteration 78, loss = 0.37547742\n",
            "Iteration 79, loss = 0.28849248\n",
            "Iteration 80, loss = 0.24139175\n",
            "Iteration 81, loss = 0.23287747\n",
            "Iteration 82, loss = 0.22890381\n",
            "Iteration 83, loss = 0.23054344\n",
            "Iteration 84, loss = 0.30288678\n",
            "Iteration 85, loss = 0.74155209\n",
            "Iteration 86, loss = 1.34926067\n",
            "Iteration 87, loss = 2.30782178\n",
            "Iteration 88, loss = 4.06516249\n",
            "Iteration 89, loss = 3.06081528\n",
            "Iteration 90, loss = 2.26439250\n",
            "Iteration 91, loss = 1.84890299\n",
            "Iteration 92, loss = 1.73427810\n",
            "Iteration 93, loss = 1.71592034\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.43965927\n",
            "Iteration 2, loss = 2.37247102\n",
            "Iteration 3, loss = 2.24895527\n",
            "Iteration 4, loss = 2.14572903\n",
            "Iteration 5, loss = 2.05560448\n",
            "Iteration 6, loss = 1.95416232\n",
            "Iteration 7, loss = 1.87983779\n",
            "Iteration 8, loss = 1.79824775\n",
            "Iteration 9, loss = 1.71054652\n",
            "Iteration 10, loss = 1.63138324\n",
            "Iteration 11, loss = 1.53090870\n",
            "Iteration 12, loss = 1.42273720\n",
            "Iteration 13, loss = 1.32006287\n",
            "Iteration 14, loss = 1.22085011\n",
            "Iteration 15, loss = 1.10730523\n",
            "Iteration 16, loss = 1.00871553\n",
            "Iteration 17, loss = 0.97828415\n",
            "Iteration 18, loss = 0.95321778\n",
            "Iteration 19, loss = 0.79207664\n",
            "Iteration 20, loss = 0.75225725\n",
            "Iteration 21, loss = 0.70945049\n",
            "Iteration 22, loss = 0.71183682\n",
            "Iteration 23, loss = 0.73897768\n",
            "Iteration 24, loss = 0.62685259\n",
            "Iteration 25, loss = 0.59689382\n",
            "Iteration 26, loss = 0.68764902\n",
            "Iteration 27, loss = 0.67222223\n",
            "Iteration 28, loss = 0.54146092\n",
            "Iteration 29, loss = 0.53575067\n",
            "Iteration 30, loss = 0.54170807\n",
            "Iteration 31, loss = 0.54835652\n",
            "Iteration 32, loss = 0.59465108\n",
            "Iteration 33, loss = 0.59127225\n",
            "Iteration 34, loss = 0.65806791\n",
            "Iteration 35, loss = 0.65687645\n",
            "Iteration 36, loss = 0.45232932\n",
            "Iteration 37, loss = 0.43559365\n",
            "Iteration 38, loss = 0.48239273\n",
            "Iteration 39, loss = 0.51660989\n",
            "Iteration 40, loss = 0.59409367\n",
            "Iteration 41, loss = 0.48594594\n",
            "Iteration 42, loss = 0.39819009\n",
            "Iteration 43, loss = 0.36197833\n",
            "Iteration 44, loss = 0.36163331\n",
            "Iteration 45, loss = 0.34711320\n",
            "Iteration 46, loss = 0.36273338\n",
            "Iteration 47, loss = 0.55057020\n",
            "Iteration 48, loss = 3.94428298\n",
            "Iteration 49, loss = 2.60812394\n",
            "Iteration 50, loss = 2.21099008\n",
            "Iteration 51, loss = 2.14516198\n",
            "Iteration 52, loss = 1.92527113\n",
            "Iteration 53, loss = 1.73266998\n",
            "Iteration 54, loss = 1.57253715\n",
            "Iteration 55, loss = 2.08208356\n",
            "Iteration 56, loss = 1.53810069\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.24357629\n",
            "Iteration 2, loss = 1.76082846\n",
            "Iteration 3, loss = 1.43112319\n",
            "Iteration 4, loss = 1.68391214\n",
            "Iteration 5, loss = 2.11656364\n",
            "Iteration 6, loss = 1.46476155\n",
            "Iteration 7, loss = 1.24697526\n",
            "Iteration 8, loss = 1.06744646\n",
            "Iteration 9, loss = 0.92982149\n",
            "Iteration 10, loss = 0.78752253\n",
            "Iteration 11, loss = 0.75872403\n",
            "Iteration 12, loss = 0.76684469\n",
            "Iteration 13, loss = 0.70552848\n",
            "Iteration 14, loss = 0.57421312\n",
            "Iteration 15, loss = 0.52465352\n",
            "Iteration 16, loss = 0.56371386\n",
            "Iteration 17, loss = 0.46039767\n",
            "Iteration 18, loss = 0.42061735\n",
            "Iteration 19, loss = 0.39583645\n",
            "Iteration 20, loss = 0.38192707\n",
            "Iteration 21, loss = 0.31590344\n",
            "Iteration 22, loss = 0.32358138\n",
            "Iteration 23, loss = 0.48345461\n",
            "Iteration 24, loss = 0.30919372\n",
            "Iteration 25, loss = 0.28674786\n",
            "Iteration 26, loss = 0.35377802\n",
            "Iteration 27, loss = 0.27335744\n",
            "Iteration 28, loss = 0.24114536\n",
            "Iteration 29, loss = 0.20596861\n",
            "Iteration 30, loss = 0.18855264\n",
            "Iteration 31, loss = 0.19471024\n",
            "Iteration 32, loss = 0.23974420\n",
            "Iteration 33, loss = 0.51943167\n",
            "Iteration 34, loss = 0.34550047\n",
            "Iteration 35, loss = 0.20679728\n",
            "Iteration 36, loss = 0.16156022\n",
            "Iteration 37, loss = 0.15069953\n",
            "Iteration 38, loss = 0.13659590\n",
            "Iteration 39, loss = 0.13892604\n",
            "Iteration 40, loss = 0.10857586\n",
            "Iteration 41, loss = 0.09494400\n",
            "Iteration 42, loss = 0.09261053\n",
            "Iteration 43, loss = 0.14263403\n",
            "Iteration 44, loss = 0.09067986\n",
            "Iteration 45, loss = 0.19955268\n",
            "Iteration 46, loss = 0.34260456\n",
            "Iteration 47, loss = 0.26267260\n",
            "Iteration 48, loss = 0.09366680\n",
            "Iteration 49, loss = 0.09414218\n",
            "Iteration 50, loss = 0.07307609\n",
            "Iteration 51, loss = 0.06540880\n",
            "Iteration 52, loss = 0.06840032\n",
            "Iteration 53, loss = 0.05222541\n",
            "Iteration 54, loss = 0.04682876\n",
            "Iteration 55, loss = 0.04376680\n",
            "Iteration 56, loss = 0.04034185\n",
            "Iteration 57, loss = 0.03297840\n",
            "Iteration 58, loss = 0.03396749\n",
            "Iteration 59, loss = 0.03160231\n",
            "Iteration 60, loss = 0.02362232\n",
            "Iteration 61, loss = 0.02262949\n",
            "Iteration 62, loss = 0.02195374\n",
            "Iteration 63, loss = 0.02116850\n",
            "Iteration 64, loss = 0.03031783\n",
            "Iteration 65, loss = 0.01498284\n",
            "Iteration 66, loss = 0.01697156\n",
            "Iteration 67, loss = 0.01363396\n",
            "Iteration 68, loss = 0.01320572\n",
            "Iteration 69, loss = 0.01284284\n",
            "Iteration 70, loss = 0.01082891\n",
            "Iteration 71, loss = 0.00997723\n",
            "Iteration 72, loss = 0.00938861\n",
            "Iteration 73, loss = 0.00878532\n",
            "Iteration 74, loss = 0.00834113\n",
            "Iteration 75, loss = 0.00777802\n",
            "Iteration 76, loss = 0.00778534\n",
            "Iteration 77, loss = 0.00679515\n",
            "Iteration 78, loss = 0.00644038\n",
            "Iteration 79, loss = 0.00676012\n",
            "Iteration 80, loss = 0.00629466\n",
            "Iteration 81, loss = 0.00566766\n",
            "Iteration 82, loss = 0.00532198\n",
            "Iteration 83, loss = 0.00512288\n",
            "Iteration 84, loss = 0.00494926\n",
            "Iteration 85, loss = 0.00476182\n",
            "Iteration 86, loss = 0.00473952\n",
            "Iteration 87, loss = 0.00455255\n",
            "Iteration 88, loss = 0.00466397\n",
            "Iteration 89, loss = 0.00422585\n",
            "Iteration 90, loss = 0.00400655\n",
            "Iteration 91, loss = 0.00398621\n",
            "Iteration 92, loss = 0.00374366\n",
            "Iteration 93, loss = 0.00365209\n",
            "Iteration 94, loss = 0.00351859\n",
            "Iteration 95, loss = 0.00342167\n",
            "Iteration 96, loss = 0.00334611\n",
            "Iteration 97, loss = 0.00327910\n",
            "Iteration 98, loss = 0.00326933\n",
            "Iteration 99, loss = 0.00310029\n",
            "Iteration 100, loss = 0.00303061\n",
            "Iteration 101, loss = 0.00294572\n",
            "Iteration 102, loss = 0.00287917\n",
            "Iteration 103, loss = 0.00284280\n",
            "Iteration 104, loss = 0.00275473\n",
            "Iteration 105, loss = 0.00276878\n",
            "Iteration 106, loss = 0.00264182\n",
            "Iteration 107, loss = 0.00260702\n",
            "Iteration 108, loss = 0.00256187\n",
            "Iteration 109, loss = 0.00251030\n",
            "Iteration 110, loss = 0.00250924\n",
            "Iteration 111, loss = 0.00239537\n",
            "Iteration 112, loss = 0.00237424\n",
            "Iteration 113, loss = 0.00234025\n",
            "Iteration 114, loss = 0.00227010\n",
            "Iteration 115, loss = 0.00222250\n",
            "Iteration 116, loss = 0.00218509\n",
            "Iteration 117, loss = 0.00215267\n",
            "Iteration 118, loss = 0.00211305\n",
            "Iteration 119, loss = 0.00208461\n",
            "Iteration 120, loss = 0.00204811\n",
            "Iteration 121, loss = 0.00207592\n",
            "Iteration 122, loss = 0.00199154\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.25403254\n",
            "Iteration 2, loss = 1.80399509\n",
            "Iteration 3, loss = 1.77755875\n",
            "Iteration 4, loss = 1.29875249\n",
            "Iteration 5, loss = 1.09028013\n",
            "Iteration 6, loss = 0.88877467\n",
            "Iteration 7, loss = 0.80446893\n",
            "Iteration 8, loss = 1.04738268\n",
            "Iteration 9, loss = 0.86806329\n",
            "Iteration 10, loss = 0.68106717\n",
            "Iteration 11, loss = 0.60364452\n",
            "Iteration 12, loss = 0.54918268\n",
            "Iteration 13, loss = 0.49351323\n",
            "Iteration 14, loss = 0.54182982\n",
            "Iteration 15, loss = 0.43381748\n",
            "Iteration 16, loss = 0.42490982\n",
            "Iteration 17, loss = 0.35708595\n",
            "Iteration 18, loss = 0.31660028\n",
            "Iteration 19, loss = 0.29003081\n",
            "Iteration 20, loss = 0.27899810\n",
            "Iteration 21, loss = 0.25104094\n",
            "Iteration 22, loss = 0.26311834\n",
            "Iteration 23, loss = 0.50785086\n",
            "Iteration 24, loss = 0.46148059\n",
            "Iteration 25, loss = 0.29000411\n",
            "Iteration 26, loss = 0.22552532\n",
            "Iteration 27, loss = 0.19724779\n",
            "Iteration 28, loss = 0.17102117\n",
            "Iteration 29, loss = 0.16259395\n",
            "Iteration 30, loss = 0.18074778\n",
            "Iteration 31, loss = 0.23823699\n",
            "Iteration 32, loss = 0.12523779\n",
            "Iteration 33, loss = 0.11480576\n",
            "Iteration 34, loss = 0.13412325\n",
            "Iteration 35, loss = 0.11651863\n",
            "Iteration 36, loss = 0.09375399\n",
            "Iteration 37, loss = 0.08451689\n",
            "Iteration 38, loss = 0.08231897\n",
            "Iteration 39, loss = 0.07095266\n",
            "Iteration 40, loss = 0.06902425\n",
            "Iteration 41, loss = 0.05746100\n",
            "Iteration 42, loss = 0.07054152\n",
            "Iteration 43, loss = 0.08160387\n",
            "Iteration 44, loss = 0.11756240\n",
            "Iteration 45, loss = 0.06673359\n",
            "Iteration 46, loss = 0.05893365\n",
            "Iteration 47, loss = 0.04905473\n",
            "Iteration 48, loss = 0.05071869\n",
            "Iteration 49, loss = 0.02880077\n",
            "Iteration 50, loss = 0.02719892\n",
            "Iteration 51, loss = 0.02890488\n",
            "Iteration 52, loss = 0.01925380\n",
            "Iteration 53, loss = 0.01742443\n",
            "Iteration 54, loss = 0.01616490\n",
            "Iteration 55, loss = 0.01426817\n",
            "Iteration 56, loss = 0.01319404\n",
            "Iteration 57, loss = 0.01208844\n",
            "Iteration 58, loss = 0.01148565\n",
            "Iteration 59, loss = 0.01081676\n",
            "Iteration 60, loss = 0.00974247\n",
            "Iteration 61, loss = 0.00971246\n",
            "Iteration 62, loss = 0.00885811\n",
            "Iteration 63, loss = 0.00838947\n",
            "Iteration 64, loss = 0.00780089\n",
            "Iteration 65, loss = 0.00749096\n",
            "Iteration 66, loss = 0.00704678\n",
            "Iteration 67, loss = 0.00672946\n",
            "Iteration 68, loss = 0.00641234\n",
            "Iteration 69, loss = 0.00615287\n",
            "Iteration 70, loss = 0.00594781\n",
            "Iteration 71, loss = 0.00590513\n",
            "Iteration 72, loss = 0.00556458\n",
            "Iteration 73, loss = 0.00543454\n",
            "Iteration 74, loss = 0.00516411\n",
            "Iteration 75, loss = 0.00500151\n",
            "Iteration 76, loss = 0.00479183\n",
            "Iteration 77, loss = 0.00475590\n",
            "Iteration 78, loss = 0.00450300\n",
            "Iteration 79, loss = 0.00440858\n",
            "Iteration 80, loss = 0.00426234\n",
            "Iteration 81, loss = 0.00411899\n",
            "Iteration 82, loss = 0.00400540\n",
            "Iteration 83, loss = 0.00394116\n",
            "Iteration 84, loss = 0.00380244\n",
            "Iteration 85, loss = 0.00371017\n",
            "Iteration 86, loss = 0.00361879\n",
            "Iteration 87, loss = 0.00352100\n",
            "Iteration 88, loss = 0.00348069\n",
            "Iteration 89, loss = 0.00340725\n",
            "Iteration 90, loss = 0.00330393\n",
            "Iteration 91, loss = 0.00324563\n",
            "Iteration 92, loss = 0.00318627\n",
            "Iteration 93, loss = 0.00312245\n",
            "Iteration 94, loss = 0.00305061\n",
            "Iteration 95, loss = 0.00297222\n",
            "Iteration 96, loss = 0.00290990\n",
            "Iteration 97, loss = 0.00287809\n",
            "Iteration 98, loss = 0.00281474\n",
            "Iteration 99, loss = 0.00274585\n",
            "Iteration 100, loss = 0.00269165\n",
            "Iteration 101, loss = 0.00266183\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.23473150\n",
            "Iteration 2, loss = 1.77086880\n",
            "Iteration 3, loss = 1.66040947\n",
            "Iteration 4, loss = 1.18718563\n",
            "Iteration 5, loss = 1.05157157\n",
            "Iteration 6, loss = 1.11527094\n",
            "Iteration 7, loss = 0.88901695\n",
            "Iteration 8, loss = 1.10440155\n",
            "Iteration 9, loss = 0.71476228\n",
            "Iteration 10, loss = 0.64112444\n",
            "Iteration 11, loss = 0.55497864\n",
            "Iteration 12, loss = 0.51579675\n",
            "Iteration 13, loss = 0.47853808\n",
            "Iteration 14, loss = 0.47645919\n",
            "Iteration 15, loss = 0.69409695\n",
            "Iteration 16, loss = 0.59135259\n",
            "Iteration 17, loss = 0.43023770\n",
            "Iteration 18, loss = 0.38483557\n",
            "Iteration 19, loss = 0.34224419\n",
            "Iteration 20, loss = 0.32654147\n",
            "Iteration 21, loss = 0.32235913\n",
            "Iteration 22, loss = 0.33906880\n",
            "Iteration 23, loss = 0.24281789\n",
            "Iteration 24, loss = 0.26322595\n",
            "Iteration 25, loss = 0.27016378\n",
            "Iteration 26, loss = 0.32837289\n",
            "Iteration 27, loss = 0.25926972\n",
            "Iteration 28, loss = 0.21801723\n",
            "Iteration 29, loss = 0.24722530\n",
            "Iteration 30, loss = 0.16757323\n",
            "Iteration 31, loss = 0.16536974\n",
            "Iteration 32, loss = 0.12642259\n",
            "Iteration 33, loss = 0.14128455\n",
            "Iteration 34, loss = 0.13084619\n",
            "Iteration 35, loss = 0.09817210\n",
            "Iteration 36, loss = 0.08656702\n",
            "Iteration 37, loss = 0.07112116\n",
            "Iteration 38, loss = 0.12178480\n",
            "Iteration 39, loss = 0.25751358\n",
            "Iteration 40, loss = 0.18985687\n",
            "Iteration 41, loss = 0.09135372\n",
            "Iteration 42, loss = 0.06713850\n",
            "Iteration 43, loss = 0.05832327\n",
            "Iteration 44, loss = 0.05188384\n",
            "Iteration 45, loss = 0.05633931\n",
            "Iteration 46, loss = 0.05851737\n",
            "Iteration 47, loss = 0.05416058\n",
            "Iteration 48, loss = 0.03173107\n",
            "Iteration 49, loss = 0.02558984\n",
            "Iteration 50, loss = 0.02301396\n",
            "Iteration 51, loss = 0.02207710\n",
            "Iteration 52, loss = 0.01901808\n",
            "Iteration 53, loss = 0.01734398\n",
            "Iteration 54, loss = 0.01485671\n",
            "Iteration 55, loss = 0.01362457\n",
            "Iteration 56, loss = 0.01276449\n",
            "Iteration 57, loss = 0.01211364\n",
            "Iteration 58, loss = 0.01119332\n",
            "Iteration 59, loss = 0.01052555\n",
            "Iteration 60, loss = 0.01049817\n",
            "Iteration 61, loss = 0.00914778\n",
            "Iteration 62, loss = 0.00848335\n",
            "Iteration 63, loss = 0.00800057\n",
            "Iteration 64, loss = 0.00746241\n",
            "Iteration 65, loss = 0.00705611\n",
            "Iteration 66, loss = 0.00672177\n",
            "Iteration 67, loss = 0.00666743\n",
            "Iteration 68, loss = 0.00626487\n",
            "Iteration 69, loss = 0.00602972\n",
            "Iteration 70, loss = 0.00581557\n",
            "Iteration 71, loss = 0.00553965\n",
            "Iteration 72, loss = 0.00542058\n",
            "Iteration 73, loss = 0.00502971\n",
            "Iteration 74, loss = 0.00490002\n",
            "Iteration 75, loss = 0.00478860\n",
            "Iteration 76, loss = 0.00457074\n",
            "Iteration 77, loss = 0.00446190\n",
            "Iteration 78, loss = 0.00426014\n",
            "Iteration 79, loss = 0.00413721\n",
            "Iteration 80, loss = 0.00402282\n",
            "Iteration 81, loss = 0.00392187\n",
            "Iteration 82, loss = 0.00379382\n",
            "Iteration 83, loss = 0.00372326\n",
            "Iteration 84, loss = 0.00359998\n",
            "Iteration 85, loss = 0.00352933\n",
            "Iteration 86, loss = 0.00344037\n",
            "Iteration 87, loss = 0.00333461\n",
            "Iteration 88, loss = 0.00329608\n",
            "Iteration 89, loss = 0.00319079\n",
            "Iteration 90, loss = 0.00311564\n",
            "Iteration 91, loss = 0.00305432\n",
            "Iteration 92, loss = 0.00297455\n",
            "Iteration 93, loss = 0.00295485\n",
            "Iteration 94, loss = 0.00289813\n",
            "Iteration 95, loss = 0.00283545\n",
            "Iteration 96, loss = 0.00277295\n",
            "Iteration 97, loss = 0.00270620\n",
            "Iteration 98, loss = 0.00267984\n",
            "Iteration 99, loss = 0.00260701\n",
            "Iteration 100, loss = 0.00258728\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.25146739\n",
            "Iteration 2, loss = 1.81765091\n",
            "Iteration 3, loss = 1.65498580\n",
            "Iteration 4, loss = 1.26092351\n",
            "Iteration 5, loss = 1.07901683\n",
            "Iteration 6, loss = 0.85004459\n",
            "Iteration 7, loss = 0.86968030\n",
            "Iteration 8, loss = 1.08589990\n",
            "Iteration 9, loss = 0.67655505\n",
            "Iteration 10, loss = 0.60153852\n",
            "Iteration 11, loss = 0.54859063\n",
            "Iteration 12, loss = 0.54070320\n",
            "Iteration 13, loss = 0.52612436\n",
            "Iteration 14, loss = 0.43304170\n",
            "Iteration 15, loss = 0.43133827\n",
            "Iteration 16, loss = 0.45018685\n",
            "Iteration 17, loss = 0.38571576\n",
            "Iteration 18, loss = 0.33346090\n",
            "Iteration 19, loss = 0.33221181\n",
            "Iteration 20, loss = 0.27881087\n",
            "Iteration 21, loss = 0.27902291\n",
            "Iteration 22, loss = 0.26040468\n",
            "Iteration 23, loss = 0.49798977\n",
            "Iteration 24, loss = 0.30245639\n",
            "Iteration 25, loss = 0.24666861\n",
            "Iteration 26, loss = 0.19301926\n",
            "Iteration 27, loss = 0.17270512\n",
            "Iteration 28, loss = 0.16522256\n",
            "Iteration 29, loss = 0.15009976\n",
            "Iteration 30, loss = 0.15524308\n",
            "Iteration 31, loss = 0.14159637\n",
            "Iteration 32, loss = 0.11160656\n",
            "Iteration 33, loss = 0.10815590\n",
            "Iteration 34, loss = 0.10092479\n",
            "Iteration 35, loss = 0.18155193\n",
            "Iteration 36, loss = 0.46187441\n",
            "Iteration 37, loss = 0.24095970\n",
            "Iteration 38, loss = 0.20303315\n",
            "Iteration 39, loss = 0.11490677\n",
            "Iteration 40, loss = 0.08124156\n",
            "Iteration 41, loss = 0.06960440\n",
            "Iteration 42, loss = 0.06788519\n",
            "Iteration 43, loss = 0.06439728\n",
            "Iteration 44, loss = 0.05233517\n",
            "Iteration 45, loss = 0.04846438\n",
            "Iteration 46, loss = 0.04049011\n",
            "Iteration 47, loss = 0.03653718\n",
            "Iteration 48, loss = 0.03914566\n",
            "Iteration 49, loss = 0.03801527\n",
            "Iteration 50, loss = 0.02639193\n",
            "Iteration 51, loss = 0.02381983\n",
            "Iteration 52, loss = 0.02129768\n",
            "Iteration 53, loss = 0.01724819\n",
            "Iteration 54, loss = 0.01598064\n",
            "Iteration 55, loss = 0.01456772\n",
            "Iteration 56, loss = 0.01341683\n",
            "Iteration 57, loss = 0.01262991\n",
            "Iteration 58, loss = 0.01139967\n",
            "Iteration 59, loss = 0.01079306\n",
            "Iteration 60, loss = 0.01106736\n",
            "Iteration 61, loss = 0.00966009\n",
            "Iteration 62, loss = 0.00900561\n",
            "Iteration 63, loss = 0.00878916\n",
            "Iteration 64, loss = 0.00810128\n",
            "Iteration 65, loss = 0.00793127\n",
            "Iteration 66, loss = 0.00718556\n",
            "Iteration 67, loss = 0.00670147\n",
            "Iteration 68, loss = 0.00649843\n",
            "Iteration 69, loss = 0.00626060\n",
            "Iteration 70, loss = 0.00616816\n",
            "Iteration 71, loss = 0.00597980\n",
            "Iteration 72, loss = 0.00546613\n",
            "Iteration 73, loss = 0.00531606\n",
            "Iteration 74, loss = 0.00501994\n",
            "Iteration 75, loss = 0.00493006\n",
            "Iteration 76, loss = 0.00469817\n",
            "Iteration 77, loss = 0.00453046\n",
            "Iteration 78, loss = 0.00443066\n",
            "Iteration 79, loss = 0.00429516\n",
            "Iteration 80, loss = 0.00414519\n",
            "Iteration 81, loss = 0.00402306\n",
            "Iteration 82, loss = 0.00389651\n",
            "Iteration 83, loss = 0.00380491\n",
            "Iteration 84, loss = 0.00371167\n",
            "Iteration 85, loss = 0.00363488\n",
            "Iteration 86, loss = 0.00351743\n",
            "Iteration 87, loss = 0.00347270\n",
            "Iteration 88, loss = 0.00339434\n",
            "Iteration 89, loss = 0.00329096\n",
            "Iteration 90, loss = 0.00322826\n",
            "Iteration 91, loss = 0.00314261\n",
            "Iteration 92, loss = 0.00306884\n",
            "Iteration 93, loss = 0.00301407\n",
            "Iteration 94, loss = 0.00296033\n",
            "Iteration 95, loss = 0.00289588\n",
            "Iteration 96, loss = 0.00284697\n",
            "Iteration 97, loss = 0.00277263\n",
            "Iteration 98, loss = 0.00278439\n",
            "Iteration 99, loss = 0.00267960\n",
            "Iteration 100, loss = 0.00265098\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.24187460\n",
            "Iteration 2, loss = 1.78825458\n",
            "Iteration 3, loss = 1.69356892\n",
            "Iteration 4, loss = 1.50475084\n",
            "Iteration 5, loss = 1.26221138\n",
            "Iteration 6, loss = 1.01355871\n",
            "Iteration 7, loss = 0.82803981\n",
            "Iteration 8, loss = 0.97406837\n",
            "Iteration 9, loss = 0.92702834\n",
            "Iteration 10, loss = 0.80655861\n",
            "Iteration 11, loss = 0.63991937\n",
            "Iteration 12, loss = 0.57795834\n",
            "Iteration 13, loss = 0.52059609\n",
            "Iteration 14, loss = 0.46155845\n",
            "Iteration 15, loss = 0.42507172\n",
            "Iteration 16, loss = 0.39368974\n",
            "Iteration 17, loss = 0.37272919\n",
            "Iteration 18, loss = 0.31386452\n",
            "Iteration 19, loss = 0.55536201\n",
            "Iteration 20, loss = 0.47232101\n",
            "Iteration 21, loss = 0.57012146\n",
            "Iteration 22, loss = 0.37499563\n",
            "Iteration 23, loss = 0.29095783\n",
            "Iteration 24, loss = 0.26734431\n",
            "Iteration 25, loss = 0.23588801\n",
            "Iteration 26, loss = 0.19264224\n",
            "Iteration 27, loss = 0.19016304\n",
            "Iteration 28, loss = 0.18381136\n",
            "Iteration 29, loss = 0.14478164\n",
            "Iteration 30, loss = 0.12351551\n",
            "Iteration 31, loss = 0.15963720\n",
            "Iteration 32, loss = 0.21309835\n",
            "Iteration 33, loss = 0.31416914\n",
            "Iteration 34, loss = 0.64021868\n",
            "Iteration 35, loss = 0.17256734\n",
            "Iteration 36, loss = 0.15405757\n",
            "Iteration 37, loss = 0.12870527\n",
            "Iteration 38, loss = 0.11268691\n",
            "Iteration 39, loss = 0.09502493\n",
            "Iteration 40, loss = 0.07847029\n",
            "Iteration 41, loss = 0.06735405\n",
            "Iteration 42, loss = 0.06907515\n",
            "Iteration 43, loss = 0.05962993\n",
            "Iteration 44, loss = 0.05657769\n",
            "Iteration 45, loss = 0.04891390\n",
            "Iteration 46, loss = 0.03852413\n",
            "Iteration 47, loss = 0.04265048\n",
            "Iteration 48, loss = 0.05257919\n",
            "Iteration 49, loss = 0.02864085\n",
            "Iteration 50, loss = 0.03009097\n",
            "Iteration 51, loss = 0.04148659\n",
            "Iteration 52, loss = 0.03113481\n",
            "Iteration 53, loss = 0.02430237\n",
            "Iteration 54, loss = 0.01863963\n",
            "Iteration 55, loss = 0.01820546\n",
            "Iteration 56, loss = 0.01459692\n",
            "Iteration 57, loss = 0.01273496\n",
            "Iteration 58, loss = 0.01185094\n",
            "Iteration 59, loss = 0.01175127\n",
            "Iteration 60, loss = 0.01147880\n",
            "Iteration 61, loss = 0.00970122\n",
            "Iteration 62, loss = 0.00950335\n",
            "Iteration 63, loss = 0.00861279\n",
            "Iteration 64, loss = 0.00808488\n",
            "Iteration 65, loss = 0.00758050\n",
            "Iteration 66, loss = 0.00720890\n",
            "Iteration 67, loss = 0.00691900\n",
            "Iteration 68, loss = 0.00648393\n",
            "Iteration 69, loss = 0.00623696\n",
            "Iteration 70, loss = 0.00641844\n",
            "Iteration 71, loss = 0.00598030\n",
            "Iteration 72, loss = 0.00546358\n",
            "Iteration 73, loss = 0.00545711\n",
            "Iteration 74, loss = 0.00500466\n",
            "Iteration 75, loss = 0.00495043\n",
            "Iteration 76, loss = 0.00468261\n",
            "Iteration 77, loss = 0.00450379\n",
            "Iteration 78, loss = 0.00437596\n",
            "Iteration 79, loss = 0.00422271\n",
            "Iteration 80, loss = 0.00411604\n",
            "Iteration 81, loss = 0.00401252\n",
            "Iteration 82, loss = 0.00387260\n",
            "Iteration 83, loss = 0.00378398\n",
            "Iteration 84, loss = 0.00368383\n",
            "Iteration 85, loss = 0.00363289\n",
            "Iteration 86, loss = 0.00347690\n",
            "Iteration 87, loss = 0.00340042\n",
            "Iteration 88, loss = 0.00333493\n",
            "Iteration 89, loss = 0.00322392\n",
            "Iteration 90, loss = 0.00317635\n",
            "Iteration 91, loss = 0.00308944\n",
            "Iteration 92, loss = 0.00301062\n",
            "Iteration 93, loss = 0.00296527\n",
            "Iteration 94, loss = 0.00288543\n",
            "Iteration 95, loss = 0.00282122\n",
            "Iteration 96, loss = 0.00276969\n",
            "Iteration 97, loss = 0.00272669\n",
            "Iteration 98, loss = 0.00266068\n",
            "Iteration 99, loss = 0.00262632\n",
            "Iteration 100, loss = 0.00259701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.27034751\n",
            "Iteration 2, loss = 1.51915586\n",
            "Iteration 3, loss = 1.52929062\n",
            "Iteration 4, loss = 1.17348113\n",
            "Iteration 5, loss = 0.80098939\n",
            "Iteration 6, loss = 0.74082963\n",
            "Iteration 7, loss = 0.65418147\n",
            "Iteration 8, loss = 0.60355727\n",
            "Iteration 9, loss = 0.55248969\n",
            "Iteration 10, loss = 0.49310312\n",
            "Iteration 11, loss = 0.46946387\n",
            "Iteration 12, loss = 0.53838602\n",
            "Iteration 13, loss = 0.43540423\n",
            "Iteration 14, loss = 0.38158026\n",
            "Iteration 15, loss = 0.32040759\n",
            "Iteration 16, loss = 0.33933211\n",
            "Iteration 17, loss = 0.34484977\n",
            "Iteration 18, loss = 0.25406799\n",
            "Iteration 19, loss = 0.25362669\n",
            "Iteration 20, loss = 0.24173084\n",
            "Iteration 21, loss = 0.32528582\n",
            "Iteration 22, loss = 0.19091157\n",
            "Iteration 23, loss = 0.18697310\n",
            "Iteration 24, loss = 0.21186654\n",
            "Iteration 25, loss = 0.14664276\n",
            "Iteration 26, loss = 0.15765870\n",
            "Iteration 27, loss = 0.15230696\n",
            "Iteration 28, loss = 0.16273302\n",
            "Iteration 29, loss = 0.14818373\n",
            "Iteration 30, loss = 0.11421804\n",
            "Iteration 31, loss = 0.10745063\n",
            "Iteration 32, loss = 0.09826842\n",
            "Iteration 33, loss = 0.08068410\n",
            "Iteration 34, loss = 0.09364739\n",
            "Iteration 35, loss = 0.08450510\n",
            "Iteration 36, loss = 0.07604914\n",
            "Iteration 37, loss = 0.06854483\n",
            "Iteration 38, loss = 0.06198278\n",
            "Iteration 39, loss = 0.06210312\n",
            "Iteration 40, loss = 0.05464958\n",
            "Iteration 41, loss = 0.05985002\n",
            "Iteration 42, loss = 0.04574839\n",
            "Iteration 43, loss = 0.04607652\n",
            "Iteration 44, loss = 0.04508522\n",
            "Iteration 45, loss = 0.05896121\n",
            "Iteration 46, loss = 0.03693020\n",
            "Iteration 47, loss = 0.03291176\n",
            "Iteration 48, loss = 0.03262228\n",
            "Iteration 49, loss = 0.03090672\n",
            "Iteration 50, loss = 0.02989968\n",
            "Iteration 51, loss = 0.02948342\n",
            "Iteration 52, loss = 0.02557027\n",
            "Iteration 53, loss = 0.02426082\n",
            "Iteration 54, loss = 0.02259896\n",
            "Iteration 55, loss = 0.02277578\n",
            "Iteration 56, loss = 0.02096106\n",
            "Iteration 57, loss = 0.02114522\n",
            "Iteration 58, loss = 0.01931713\n",
            "Iteration 59, loss = 0.01851132\n",
            "Iteration 60, loss = 0.02018320\n",
            "Iteration 61, loss = 0.01707579\n",
            "Iteration 62, loss = 0.01635156\n",
            "Iteration 63, loss = 0.01625561\n",
            "Iteration 64, loss = 0.01619714\n",
            "Iteration 65, loss = 0.01476448\n",
            "Iteration 66, loss = 0.01415775\n",
            "Iteration 67, loss = 0.01379720\n",
            "Iteration 68, loss = 0.01335603\n",
            "Iteration 69, loss = 0.01283576\n",
            "Iteration 70, loss = 0.01285818\n",
            "Iteration 71, loss = 0.01253300\n",
            "Iteration 72, loss = 0.01210876\n",
            "Iteration 73, loss = 0.01167711\n",
            "Iteration 74, loss = 0.01145134\n",
            "Iteration 75, loss = 0.01113512\n",
            "Iteration 76, loss = 0.01069940\n",
            "Iteration 77, loss = 0.01095891\n",
            "Iteration 78, loss = 0.01034398\n",
            "Iteration 79, loss = 0.01006790\n",
            "Iteration 80, loss = 0.00975602\n",
            "Iteration 81, loss = 0.00948678\n",
            "Iteration 82, loss = 0.00938980\n",
            "Iteration 83, loss = 0.00909459\n",
            "Iteration 84, loss = 0.00899281\n",
            "Iteration 85, loss = 0.00882379\n",
            "Iteration 86, loss = 0.00863187\n",
            "Iteration 87, loss = 0.00855707\n",
            "Iteration 88, loss = 0.00830629\n",
            "Iteration 89, loss = 0.00817878\n",
            "Iteration 90, loss = 0.00807854\n",
            "Iteration 91, loss = 0.00786371\n",
            "Iteration 92, loss = 0.00759499\n",
            "Iteration 93, loss = 0.00742190\n",
            "Iteration 94, loss = 0.00743970\n",
            "Iteration 95, loss = 0.00720977\n",
            "Iteration 96, loss = 0.00708459\n",
            "Iteration 97, loss = 0.00692508\n",
            "Iteration 98, loss = 0.00681336\n",
            "Iteration 99, loss = 0.00672156\n",
            "Iteration 100, loss = 0.00659519\n",
            "Iteration 101, loss = 0.00663688\n",
            "Iteration 102, loss = 0.00643051\n",
            "Iteration 103, loss = 0.00630399\n",
            "Iteration 104, loss = 0.00623600\n",
            "Iteration 105, loss = 0.00614479\n",
            "Iteration 106, loss = 0.00604258\n",
            "Iteration 107, loss = 0.00591876\n",
            "Iteration 108, loss = 0.00586659\n",
            "Iteration 109, loss = 0.00574505\n",
            "Iteration 110, loss = 0.00571282\n",
            "Iteration 111, loss = 0.00558479\n",
            "Iteration 112, loss = 0.00550662\n",
            "Iteration 113, loss = 0.00546969\n",
            "Iteration 114, loss = 0.00544477\n",
            "Iteration 115, loss = 0.00531047\n",
            "Iteration 116, loss = 0.00526840\n",
            "Iteration 117, loss = 0.00518185\n",
            "Iteration 118, loss = 0.00510402\n",
            "Iteration 119, loss = 0.00500696\n",
            "Iteration 120, loss = 0.00497434\n",
            "Iteration 121, loss = 0.00488423\n",
            "Iteration 122, loss = 0.00488505\n",
            "Iteration 123, loss = 0.00477317\n",
            "Iteration 124, loss = 0.00471239\n",
            "Iteration 125, loss = 0.00464363\n",
            "Iteration 126, loss = 0.00460769\n",
            "Iteration 127, loss = 0.00455093\n",
            "Iteration 128, loss = 0.00450013\n",
            "Iteration 129, loss = 0.00447109\n",
            "Iteration 130, loss = 0.00439367\n",
            "Iteration 131, loss = 0.00432834\n",
            "Iteration 132, loss = 0.00427152\n",
            "Iteration 133, loss = 0.00424034\n",
            "Iteration 134, loss = 0.00420625\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=MLPClassifier(hidden_layer_sizes={'hidden_layer_sizes': [(10,),\n",
              "                                                                                (50,),\n",
              "                                                                                (10,\n",
              "                                                                                 10),\n",
              "                                                                                (50,\n",
              "                                                                                 50)]},\n",
              "                                     learning_rate_init=0.1, max_iter=300,\n",
              "                                     random_state=2082157, solver='sgd',\n",
              "                                     verbose=True),\n",
              "             param_grid={'hidden_layer_sizes': [(10,), (50,), (10, 10),\n",
              "                                                (50, 50)]})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "#MLPclassifier requires in input the parameter hidden_layer_sizes, that is a tuple specifying the number of \n",
        "#neurons in the hidden layers; for example: (10,) means that there is only 1 hidden layer with 10 neurons; \n",
        "#(10,50) means that there are 2 hidden layers, the first with 10 neurons, the second with 50 neurons\n",
        "hl_parameters_500 = {'hidden_layer_sizes': [(10,), (50,), (10,10,), (50,50,)]}\n",
        "\n",
        "#MPLClassifier model definition (m_training = 500): default activation function = ReLu\n",
        "mlp_classifier_500 = MLPClassifier(hl_parameters_500, max_iter=300, alpha=1e-4, solver='sgd', tol=1e-4, learning_rate_init=.1, verbose=True, random_state=ID)\n",
        "\n",
        "#find best model using 5-fold CV and train it using all the training data\n",
        "mlp_cv = GridSearchCV(mlp_classifier_500, hl_parameters_500, cv=5, return_train_score=False)\n",
        "\n",
        "#fit the grid search with training data\n",
        "mlp_cv.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfUsgmR3rzIK",
        "outputId": "c1ee99f0-b965-4cb3-929d-84bd994eadfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best MLP classifier estimator (m_training = 500):\n",
            " MLPClassifier(hidden_layer_sizes=(50,), learning_rate_init=0.1, max_iter=300,\n",
            "              random_state=2082157, solver='sgd', verbose=True)\n",
            "\n",
            "RESULTS FOR NN (m_training = 500):\n",
            "\n",
            "Best parameters set found:\n",
            "{'hidden_layer_sizes': (50,)}\n",
            "\n",
            "Score with best parameters:\n",
            "0.792\n",
            "\n",
            "All scores on the grid:\n",
            "Parameters: [{'hidden_layer_sizes': (10,)}, {'hidden_layer_sizes': (50,)}, {'hidden_layer_sizes': (10, 10)}, {'hidden_layer_sizes': (50, 50)}]\n",
            "Scores: [0.748 0.792 0.408 0.79 ]\n"
          ]
        }
      ],
      "source": [
        "#best MPL classifier (m_training = 500): best_estimator_\n",
        "best_mlp_classifier = mlp_cv.best_estimator_\n",
        "print(\"Best MLP classifier estimator (m_training = 500):\\n \"+str(best_mlp_classifier))\n",
        "\n",
        "print ('\\nRESULTS FOR NN (m_training = 500):\\n')\n",
        "\n",
        "print(\"Best parameters set found:\")\n",
        "print(mlp_cv.best_params_)\n",
        "\n",
        "print(\"\\nScore with best parameters:\")\n",
        "print(mlp_cv.best_score_)\n",
        "\n",
        "print(\"\\nAll scores on the grid:\")\n",
        "print(\"Parameters: \"+str(mlp_cv.cv_results_['params']))\n",
        "print(\"Scores: \"+str(mlp_cv.cv_results_['mean_test_score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7xk12q0rzIL"
      },
      "source": [
        "## TO DO 2\n",
        "\n",
        "What do you observe for different architectures and their scores? How do the number of layers and their sizes affect the performances?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxmFp3GkrzIM"
      },
      "source": [
        "[ADD YOUR ANSWER HERE]\n",
        "\n",
        "**ANSWER [David Polzoni]**: In the results above, the model with a single hidden layer of 50 units achieved the highest score, while, for instance, the model with two hidden layers of 10 units each had a much lower score. This could be due to the fact that the model with two hidden layers had a larger capacity and may have overfitted to the training data. On the other hand, the model with a single hidden layer of 50 units may have struck a good balance between capacity and generalization, leading to a better performance on the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sjMx2ZDrzIO"
      },
      "source": [
        "## TO DO 3\n",
        "\n",
        "Now get training and test error (according to the initial split) for a NN with best parameters chosen from the cross-validation above. Use the attribute *best_estimator_* to pick the best architecture already re-trained on the training dataset (more infos in the documentation of GridSearchCV)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVm2XLQUrzIO",
        "outputId": "6b0afb54-b36a-417e-86eb-ad41ed85bb1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULTS FOR BEST NN (m_training = 500):\n",
            "\n",
            "Best NN training error: 0.000000\n",
            "Best NN test error: 0.219076\n"
          ]
        }
      ],
      "source": [
        "#get training and test error for the best NN model from CV\n",
        "\n",
        "#should be equal to best_mlp_classifier defined previously!\n",
        "best_mlp_classifier_500 = mlp_cv.best_estimator_\n",
        "\n",
        "#compute training and test error for best_mlp_classifier_500\n",
        "training_error_mlp_500 = 1. - best_mlp_classifier_500.score(X_train, y_train)\n",
        "test_error_mlp_500 = 1. - best_mlp_classifier_500.score(X_test, y_test)\n",
        "\n",
        "print ('RESULTS FOR BEST NN (m_training = 500):\\n')\n",
        "\n",
        "print (\"Best NN training error: %f\" % training_error_mlp_500)\n",
        "print (\"Best NN test error: %f\" % test_error_mlp_500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ek0XOyrrzIQ"
      },
      "source": [
        "## More data \n",
        "Now let's do the same but using 10000 (or less if it takes too long on your machine) data points for training. Use the same NN architectures as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ZTRjKorzIR",
        "outputId": "52bf1368-6928-4ddf-e09a-1bfa98f2871b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels and frequencies in training dataset: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
              " array([ 966,  996,  998, 1035,  999, 1015,  975,  981,  993, 1042]))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "\n",
        "m_training = 10000\n",
        "\n",
        "X_train, X_test = X[:m_training], X[m_training:]\n",
        "y_train, y_test = y[:m_training], y[m_training:]\n",
        "\n",
        "print(\"Labels and frequencies in training dataset: \")\n",
        "np.unique(y_train, return_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7V_goWzrzIT"
      },
      "source": [
        "## TO DO 4\n",
        "\n",
        "Now train the NNs with the added data points. This time you can run for 100 iterations if you cannot run for 300 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LADo_IDbrzIT",
        "outputId": "b63269eb-8786-4140-808e-849fb9218f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.40035300\n",
            "Iteration 2, loss = 0.72532270\n",
            "Iteration 3, loss = 0.63890478\n",
            "Iteration 4, loss = 0.59051641\n",
            "Iteration 5, loss = 0.54811678\n",
            "Iteration 6, loss = 0.50937693\n",
            "Iteration 7, loss = 0.49656361\n",
            "Iteration 8, loss = 0.49428478\n",
            "Iteration 9, loss = 0.46682201\n",
            "Iteration 10, loss = 0.44747830\n",
            "Iteration 11, loss = 0.44550761\n",
            "Iteration 12, loss = 0.43212535\n",
            "Iteration 13, loss = 0.42270240\n",
            "Iteration 14, loss = 0.41284958\n",
            "Iteration 15, loss = 0.40076623\n",
            "Iteration 16, loss = 0.39811643\n",
            "Iteration 17, loss = 0.39712278\n",
            "Iteration 18, loss = 0.38636573\n",
            "Iteration 19, loss = 0.38918259\n",
            "Iteration 20, loss = 0.37557933\n",
            "Iteration 21, loss = 0.36205182\n",
            "Iteration 22, loss = 0.37170756\n",
            "Iteration 23, loss = 0.35587326\n",
            "Iteration 24, loss = 0.34703760\n",
            "Iteration 25, loss = 0.34093384\n",
            "Iteration 26, loss = 0.33758758\n",
            "Iteration 27, loss = 0.35230740\n",
            "Iteration 28, loss = 0.32493158\n",
            "Iteration 29, loss = 0.33209923\n",
            "Iteration 30, loss = 0.32243446\n",
            "Iteration 31, loss = 0.33105410\n",
            "Iteration 32, loss = 0.31767601\n",
            "Iteration 33, loss = 0.32586263\n",
            "Iteration 34, loss = 0.31799891\n",
            "Iteration 35, loss = 0.32206534\n",
            "Iteration 36, loss = 0.32087341\n",
            "Iteration 37, loss = 0.30537842\n",
            "Iteration 38, loss = 0.30667484\n",
            "Iteration 39, loss = 0.30700963\n",
            "Iteration 40, loss = 0.29375413\n",
            "Iteration 41, loss = 0.30140386\n",
            "Iteration 42, loss = 0.28805963\n",
            "Iteration 43, loss = 0.28650831\n",
            "Iteration 44, loss = 0.29332862\n",
            "Iteration 45, loss = 0.28823156\n",
            "Iteration 46, loss = 0.28976767\n",
            "Iteration 47, loss = 0.28640545\n",
            "Iteration 48, loss = 0.28057560\n",
            "Iteration 49, loss = 0.28328086\n",
            "Iteration 50, loss = 0.26771943\n",
            "Iteration 51, loss = 0.27725579\n",
            "Iteration 52, loss = 0.27501829\n",
            "Iteration 53, loss = 0.27383078\n",
            "Iteration 54, loss = 0.27100828\n",
            "Iteration 55, loss = 0.26492928\n",
            "Iteration 56, loss = 0.26936356\n",
            "Iteration 57, loss = 0.26619342\n",
            "Iteration 58, loss = 0.26034941\n",
            "Iteration 59, loss = 0.26522682\n",
            "Iteration 60, loss = 0.25938580\n",
            "Iteration 61, loss = 0.26183995\n",
            "Iteration 62, loss = 0.25740303\n",
            "Iteration 63, loss = 0.25931465\n",
            "Iteration 64, loss = 0.26431336\n",
            "Iteration 65, loss = 0.25803745\n",
            "Iteration 66, loss = 0.24873018\n",
            "Iteration 67, loss = 0.25587287\n",
            "Iteration 68, loss = 0.24944328\n",
            "Iteration 69, loss = 0.23958384\n",
            "Iteration 70, loss = 0.25362926\n",
            "Iteration 71, loss = 0.25255049\n",
            "Iteration 72, loss = 0.24397198\n",
            "Iteration 73, loss = 0.24909812\n",
            "Iteration 74, loss = 0.24185169\n",
            "Iteration 75, loss = 0.23491951\n",
            "Iteration 76, loss = 0.23082395\n",
            "Iteration 77, loss = 0.23816247\n",
            "Iteration 78, loss = 0.23930930\n",
            "Iteration 79, loss = 0.25106691\n",
            "Iteration 80, loss = 0.23089227\n",
            "Iteration 81, loss = 0.22226191\n",
            "Iteration 82, loss = 0.23128140\n",
            "Iteration 83, loss = 0.22255483\n",
            "Iteration 84, loss = 0.25456151\n",
            "Iteration 85, loss = 0.21913964\n",
            "Iteration 86, loss = 0.22388753\n",
            "Iteration 87, loss = 0.22659852\n",
            "Iteration 88, loss = 0.22421758\n",
            "Iteration 89, loss = 0.22741187\n",
            "Iteration 90, loss = 0.22223506\n",
            "Iteration 91, loss = 0.22593237\n",
            "Iteration 92, loss = 0.21588215\n",
            "Iteration 93, loss = 0.22884958\n",
            "Iteration 94, loss = 0.22136816\n",
            "Iteration 95, loss = 0.22279271\n",
            "Iteration 96, loss = 0.21310475\n",
            "Iteration 97, loss = 0.20510913\n",
            "Iteration 98, loss = 0.22075071\n",
            "Iteration 99, loss = 0.21832099\n",
            "Iteration 100, loss = 0.21588764\n",
            "Iteration 101, loss = 0.20325569\n",
            "Iteration 102, loss = 0.21334244\n",
            "Iteration 103, loss = 0.20708450\n",
            "Iteration 104, loss = 0.20962630\n",
            "Iteration 105, loss = 0.20672607\n",
            "Iteration 106, loss = 0.21933087\n",
            "Iteration 107, loss = 0.21256021\n",
            "Iteration 108, loss = 0.20734539\n",
            "Iteration 109, loss = 0.19395754\n",
            "Iteration 110, loss = 0.19356146\n",
            "Iteration 111, loss = 0.19841396\n",
            "Iteration 112, loss = 0.19246832\n",
            "Iteration 113, loss = 0.19903597\n",
            "Iteration 114, loss = 0.19701046\n",
            "Iteration 115, loss = 0.19827722\n",
            "Iteration 116, loss = 0.19389935\n",
            "Iteration 117, loss = 0.19896828\n",
            "Iteration 118, loss = 0.19221857\n",
            "Iteration 119, loss = 0.19944913\n",
            "Iteration 120, loss = 0.18415154\n",
            "Iteration 121, loss = 0.20176008\n",
            "Iteration 122, loss = 0.19516750\n",
            "Iteration 123, loss = 0.23677305\n",
            "Iteration 124, loss = 0.18958223\n",
            "Iteration 125, loss = 0.18947392\n",
            "Iteration 126, loss = 0.23174008\n",
            "Iteration 127, loss = 0.20028904\n",
            "Iteration 128, loss = 0.19440664\n",
            "Iteration 129, loss = 0.18838267\n",
            "Iteration 130, loss = 0.19927626\n",
            "Iteration 131, loss = 0.19902007\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30358593\n",
            "Iteration 2, loss = 0.75763826\n",
            "Iteration 3, loss = 0.62071177\n",
            "Iteration 4, loss = 0.57777362\n",
            "Iteration 5, loss = 0.53535217\n",
            "Iteration 6, loss = 0.53254907\n",
            "Iteration 7, loss = 0.48680359\n",
            "Iteration 8, loss = 0.49960781\n",
            "Iteration 9, loss = 0.48124284\n",
            "Iteration 10, loss = 0.47077997\n",
            "Iteration 11, loss = 0.45974410\n",
            "Iteration 12, loss = 0.45149069\n",
            "Iteration 13, loss = 0.44727820\n",
            "Iteration 14, loss = 0.43501801\n",
            "Iteration 15, loss = 0.45693091\n",
            "Iteration 16, loss = 0.43133573\n",
            "Iteration 17, loss = 0.41970250\n",
            "Iteration 18, loss = 0.42399408\n",
            "Iteration 19, loss = 0.41648513\n",
            "Iteration 20, loss = 0.40469412\n",
            "Iteration 21, loss = 0.41126396\n",
            "Iteration 22, loss = 0.39682726\n",
            "Iteration 23, loss = 0.42001032\n",
            "Iteration 24, loss = 0.38428185\n",
            "Iteration 25, loss = 0.39586546\n",
            "Iteration 26, loss = 0.39211042\n",
            "Iteration 27, loss = 0.37502830\n",
            "Iteration 28, loss = 0.37325829\n",
            "Iteration 29, loss = 0.36235879\n",
            "Iteration 30, loss = 0.36044945\n",
            "Iteration 31, loss = 0.36679496\n",
            "Iteration 32, loss = 0.36095171\n",
            "Iteration 33, loss = 0.34353441\n",
            "Iteration 34, loss = 0.35319101\n",
            "Iteration 35, loss = 0.33601598\n",
            "Iteration 36, loss = 0.34340027\n",
            "Iteration 37, loss = 0.33890574\n",
            "Iteration 38, loss = 0.33781532\n",
            "Iteration 39, loss = 0.33762858\n",
            "Iteration 40, loss = 0.31620697\n",
            "Iteration 41, loss = 0.31508510\n",
            "Iteration 42, loss = 0.32789583\n",
            "Iteration 43, loss = 0.31218435\n",
            "Iteration 44, loss = 0.32642324\n",
            "Iteration 45, loss = 0.31548288\n",
            "Iteration 46, loss = 0.31904537\n",
            "Iteration 47, loss = 0.31411157\n",
            "Iteration 48, loss = 0.29828872\n",
            "Iteration 49, loss = 0.30202093\n",
            "Iteration 50, loss = 0.29344608\n",
            "Iteration 51, loss = 0.29407039\n",
            "Iteration 52, loss = 0.29024437\n",
            "Iteration 53, loss = 0.29445265\n",
            "Iteration 54, loss = 0.29606504\n",
            "Iteration 55, loss = 0.28625351\n",
            "Iteration 56, loss = 0.29829156\n",
            "Iteration 57, loss = 0.29347589\n",
            "Iteration 58, loss = 0.29093770\n",
            "Iteration 59, loss = 0.27563987\n",
            "Iteration 60, loss = 0.27722240\n",
            "Iteration 61, loss = 0.27297351\n",
            "Iteration 62, loss = 0.27851602\n",
            "Iteration 63, loss = 0.30253955\n",
            "Iteration 64, loss = 0.26491907\n",
            "Iteration 65, loss = 0.28674894\n",
            "Iteration 66, loss = 0.26436460\n",
            "Iteration 67, loss = 0.26897286\n",
            "Iteration 68, loss = 0.27088367\n",
            "Iteration 69, loss = 0.26134404\n",
            "Iteration 70, loss = 0.28178016\n",
            "Iteration 71, loss = 0.27269597\n",
            "Iteration 72, loss = 0.26275010\n",
            "Iteration 73, loss = 0.26502230\n",
            "Iteration 74, loss = 0.26213001\n",
            "Iteration 75, loss = 0.26052657\n",
            "Iteration 76, loss = 0.25282041\n",
            "Iteration 77, loss = 0.25244704\n",
            "Iteration 78, loss = 0.25814171\n",
            "Iteration 79, loss = 0.26443951\n",
            "Iteration 80, loss = 0.24885717\n",
            "Iteration 81, loss = 0.25648249\n",
            "Iteration 82, loss = 0.25759679\n",
            "Iteration 83, loss = 0.24184200\n",
            "Iteration 84, loss = 0.23305538\n",
            "Iteration 85, loss = 0.25252135\n",
            "Iteration 86, loss = 0.24930668\n",
            "Iteration 87, loss = 0.23366824\n",
            "Iteration 88, loss = 0.24810967\n",
            "Iteration 89, loss = 0.25832017\n",
            "Iteration 90, loss = 0.25343636\n",
            "Iteration 91, loss = 0.23798566\n",
            "Iteration 92, loss = 0.22562431\n",
            "Iteration 93, loss = 0.24949461\n",
            "Iteration 94, loss = 0.23679664\n",
            "Iteration 95, loss = 0.24616676\n",
            "Iteration 96, loss = 0.22923374\n",
            "Iteration 97, loss = 0.22116779\n",
            "Iteration 98, loss = 0.22267457\n",
            "Iteration 99, loss = 0.23151811\n",
            "Iteration 100, loss = 0.25841033\n",
            "Iteration 101, loss = 0.23929427\n",
            "Iteration 102, loss = 0.23414632\n",
            "Iteration 103, loss = 0.22627203\n",
            "Iteration 104, loss = 0.23113805\n",
            "Iteration 105, loss = 0.22211660\n",
            "Iteration 106, loss = 0.24668092\n",
            "Iteration 107, loss = 0.22216353\n",
            "Iteration 108, loss = 0.21729025\n",
            "Iteration 109, loss = 0.21276381\n",
            "Iteration 110, loss = 0.21149073\n",
            "Iteration 111, loss = 0.24145487\n",
            "Iteration 112, loss = 0.22061831\n",
            "Iteration 113, loss = 0.22381533\n",
            "Iteration 114, loss = 0.21745674\n",
            "Iteration 115, loss = 0.21574206\n",
            "Iteration 116, loss = 0.20420408\n",
            "Iteration 117, loss = 0.22254474\n",
            "Iteration 118, loss = 0.22012935\n",
            "Iteration 119, loss = 0.22144191\n",
            "Iteration 120, loss = 0.22800511\n",
            "Iteration 121, loss = 0.22414002\n",
            "Iteration 122, loss = 0.20798051\n",
            "Iteration 123, loss = 0.19951992\n",
            "Iteration 124, loss = 0.20407961\n",
            "Iteration 125, loss = 0.21127678\n",
            "Iteration 126, loss = 0.21335628\n",
            "Iteration 127, loss = 0.22406212\n",
            "Iteration 128, loss = 0.23009414\n",
            "Iteration 129, loss = 0.21658462\n",
            "Iteration 130, loss = 0.20272727\n",
            "Iteration 131, loss = 0.22036103\n",
            "Iteration 132, loss = 0.20446728\n",
            "Iteration 133, loss = 0.19978190\n",
            "Iteration 134, loss = 0.20454590\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30744089\n",
            "Iteration 2, loss = 0.72592605\n",
            "Iteration 3, loss = 0.63103258\n",
            "Iteration 4, loss = 0.55777009\n",
            "Iteration 5, loss = 0.52211705\n",
            "Iteration 6, loss = 0.47399848\n",
            "Iteration 7, loss = 0.45233317\n",
            "Iteration 8, loss = 0.44550285\n",
            "Iteration 9, loss = 0.42535798\n",
            "Iteration 10, loss = 0.41853483\n",
            "Iteration 11, loss = 0.40147154\n",
            "Iteration 12, loss = 0.39284295\n",
            "Iteration 13, loss = 0.39002550\n",
            "Iteration 14, loss = 0.38279516\n",
            "Iteration 15, loss = 0.38955952\n",
            "Iteration 16, loss = 0.37038003\n",
            "Iteration 17, loss = 0.36000543\n",
            "Iteration 18, loss = 0.36476365\n",
            "Iteration 19, loss = 0.35131542\n",
            "Iteration 20, loss = 0.34686669\n",
            "Iteration 21, loss = 0.35063005\n",
            "Iteration 22, loss = 0.33864718\n",
            "Iteration 23, loss = 0.33465570\n",
            "Iteration 24, loss = 0.32410123\n",
            "Iteration 25, loss = 0.33906767\n",
            "Iteration 26, loss = 0.33238440\n",
            "Iteration 27, loss = 0.32517292\n",
            "Iteration 28, loss = 0.31169558\n",
            "Iteration 29, loss = 0.30960371\n",
            "Iteration 30, loss = 0.32092723\n",
            "Iteration 31, loss = 0.30746723\n",
            "Iteration 32, loss = 0.30740715\n",
            "Iteration 33, loss = 0.30958675\n",
            "Iteration 34, loss = 0.30297851\n",
            "Iteration 35, loss = 0.30862373\n",
            "Iteration 36, loss = 0.29000917\n",
            "Iteration 37, loss = 0.29488017\n",
            "Iteration 38, loss = 0.29000557\n",
            "Iteration 39, loss = 0.29312006\n",
            "Iteration 40, loss = 0.27996220\n",
            "Iteration 41, loss = 0.28555463\n",
            "Iteration 42, loss = 0.27746650\n",
            "Iteration 43, loss = 0.27151359\n",
            "Iteration 44, loss = 0.28506845\n",
            "Iteration 45, loss = 0.28227398\n",
            "Iteration 46, loss = 0.26315852\n",
            "Iteration 47, loss = 0.27758340\n",
            "Iteration 48, loss = 0.26868879\n",
            "Iteration 49, loss = 0.28093353\n",
            "Iteration 50, loss = 0.25795219\n",
            "Iteration 51, loss = 0.26886053\n",
            "Iteration 52, loss = 0.27507981\n",
            "Iteration 53, loss = 0.26465739\n",
            "Iteration 54, loss = 0.27018708\n",
            "Iteration 55, loss = 0.25812779\n",
            "Iteration 56, loss = 0.26169524\n",
            "Iteration 57, loss = 0.26394683\n",
            "Iteration 58, loss = 0.27021922\n",
            "Iteration 59, loss = 0.25635758\n",
            "Iteration 60, loss = 0.25409226\n",
            "Iteration 61, loss = 0.24998244\n",
            "Iteration 62, loss = 0.24896889\n",
            "Iteration 63, loss = 0.25902307\n",
            "Iteration 64, loss = 0.24168909\n",
            "Iteration 65, loss = 0.24821509\n",
            "Iteration 66, loss = 0.25282177\n",
            "Iteration 67, loss = 0.24967370\n",
            "Iteration 68, loss = 0.24516574\n",
            "Iteration 69, loss = 0.24025064\n",
            "Iteration 70, loss = 0.24336168\n",
            "Iteration 71, loss = 0.23951951\n",
            "Iteration 72, loss = 0.24141297\n",
            "Iteration 73, loss = 0.25162984\n",
            "Iteration 74, loss = 0.23021893\n",
            "Iteration 75, loss = 0.25121988\n",
            "Iteration 76, loss = 0.24134462\n",
            "Iteration 77, loss = 0.22493739\n",
            "Iteration 78, loss = 0.26276649\n",
            "Iteration 79, loss = 0.23152436\n",
            "Iteration 80, loss = 0.23009748\n",
            "Iteration 81, loss = 0.25027106\n",
            "Iteration 82, loss = 0.23717138\n",
            "Iteration 83, loss = 0.22702656\n",
            "Iteration 84, loss = 0.22126993\n",
            "Iteration 85, loss = 0.22257100\n",
            "Iteration 86, loss = 0.22957612\n",
            "Iteration 87, loss = 0.22885941\n",
            "Iteration 88, loss = 0.21887704\n",
            "Iteration 89, loss = 0.21025147\n",
            "Iteration 90, loss = 0.22300009\n",
            "Iteration 91, loss = 0.21589664\n",
            "Iteration 92, loss = 0.20948654\n",
            "Iteration 93, loss = 0.23397355\n",
            "Iteration 94, loss = 0.22017439\n",
            "Iteration 95, loss = 0.22245002\n",
            "Iteration 96, loss = 0.21619401\n",
            "Iteration 97, loss = 0.21075078\n",
            "Iteration 98, loss = 0.20210355\n",
            "Iteration 99, loss = 0.23942461\n",
            "Iteration 100, loss = 0.22265994\n",
            "Iteration 101, loss = 0.21127924\n",
            "Iteration 102, loss = 0.20734918\n",
            "Iteration 103, loss = 0.19452700\n",
            "Iteration 104, loss = 0.21633222\n",
            "Iteration 105, loss = 0.20232719\n",
            "Iteration 106, loss = 0.21528731\n",
            "Iteration 107, loss = 0.21298375\n",
            "Iteration 108, loss = 0.21537070\n",
            "Iteration 109, loss = 0.19779578\n",
            "Iteration 110, loss = 0.19455263\n",
            "Iteration 111, loss = 0.23409937\n",
            "Iteration 112, loss = 0.20563549\n",
            "Iteration 113, loss = 0.19102313\n",
            "Iteration 114, loss = 0.21356756\n",
            "Iteration 115, loss = 0.19443876\n",
            "Iteration 116, loss = 0.19123253\n",
            "Iteration 117, loss = 0.20816053\n",
            "Iteration 118, loss = 0.19564901\n",
            "Iteration 119, loss = 0.19962632\n",
            "Iteration 120, loss = 0.21564986\n",
            "Iteration 121, loss = 0.20849355\n",
            "Iteration 122, loss = 0.19951190\n",
            "Iteration 123, loss = 0.18690955\n",
            "Iteration 124, loss = 0.18648719\n",
            "Iteration 125, loss = 0.20035646\n",
            "Iteration 126, loss = 0.21188585\n",
            "Iteration 127, loss = 0.19951564\n",
            "Iteration 128, loss = 0.21194997\n",
            "Iteration 129, loss = 0.21250214\n",
            "Iteration 130, loss = 0.19078017\n",
            "Iteration 131, loss = 0.19706091\n",
            "Iteration 132, loss = 0.21540191\n",
            "Iteration 133, loss = 0.18979766\n",
            "Iteration 134, loss = 0.19740159\n",
            "Iteration 135, loss = 0.17679954\n",
            "Iteration 136, loss = 0.17182917\n",
            "Iteration 137, loss = 0.18453317\n",
            "Iteration 138, loss = 0.19358695\n",
            "Iteration 139, loss = 0.19870047\n",
            "Iteration 140, loss = 0.17902628\n",
            "Iteration 141, loss = 0.22059365\n",
            "Iteration 142, loss = 0.20420147\n",
            "Iteration 143, loss = 0.17145294\n",
            "Iteration 144, loss = 0.18705168\n",
            "Iteration 145, loss = 0.19405280\n",
            "Iteration 146, loss = 0.18919577\n",
            "Iteration 147, loss = 0.21841349\n",
            "Iteration 148, loss = 0.18335924\n",
            "Iteration 149, loss = 0.16880183\n",
            "Iteration 150, loss = 0.18024122\n",
            "Iteration 151, loss = 0.16961702\n",
            "Iteration 152, loss = 0.16872302\n",
            "Iteration 153, loss = 0.16306810\n",
            "Iteration 154, loss = 0.17319893\n",
            "Iteration 155, loss = 0.16638769\n",
            "Iteration 156, loss = 0.21081817\n",
            "Iteration 157, loss = 0.19652641\n",
            "Iteration 158, loss = 0.16318711\n",
            "Iteration 159, loss = 0.16238871\n",
            "Iteration 160, loss = 0.16569120\n",
            "Iteration 161, loss = 0.16035660\n",
            "Iteration 162, loss = 0.16715995\n",
            "Iteration 163, loss = 0.18385532\n",
            "Iteration 164, loss = 0.16587662\n",
            "Iteration 165, loss = 0.15939254\n",
            "Iteration 166, loss = 0.18035702\n",
            "Iteration 167, loss = 0.16621310\n",
            "Iteration 168, loss = 0.16617602\n",
            "Iteration 169, loss = 0.19631925\n",
            "Iteration 170, loss = 0.17274860\n",
            "Iteration 171, loss = 0.21445433\n",
            "Iteration 172, loss = 0.16706543\n",
            "Iteration 173, loss = 0.20706172\n",
            "Iteration 174, loss = 0.16676118\n",
            "Iteration 175, loss = 0.16027339\n",
            "Iteration 176, loss = 0.16892008\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36945968\n",
            "Iteration 2, loss = 0.74914670\n",
            "Iteration 3, loss = 0.63382698\n",
            "Iteration 4, loss = 0.60086764\n",
            "Iteration 5, loss = 0.57654230\n",
            "Iteration 6, loss = 0.53656695\n",
            "Iteration 7, loss = 0.50119405\n",
            "Iteration 8, loss = 0.51396992\n",
            "Iteration 9, loss = 0.48308349\n",
            "Iteration 10, loss = 0.46459152\n",
            "Iteration 11, loss = 0.44579251\n",
            "Iteration 12, loss = 0.44114898\n",
            "Iteration 13, loss = 0.42544777\n",
            "Iteration 14, loss = 0.41909622\n",
            "Iteration 15, loss = 0.41529359\n",
            "Iteration 16, loss = 0.40594696\n",
            "Iteration 17, loss = 0.40960420\n",
            "Iteration 18, loss = 0.39231738\n",
            "Iteration 19, loss = 0.39273618\n",
            "Iteration 20, loss = 0.37274639\n",
            "Iteration 21, loss = 0.37430606\n",
            "Iteration 22, loss = 0.37162273\n",
            "Iteration 23, loss = 0.37012433\n",
            "Iteration 24, loss = 0.36799812\n",
            "Iteration 25, loss = 0.36322580\n",
            "Iteration 26, loss = 0.35062348\n",
            "Iteration 27, loss = 0.35444261\n",
            "Iteration 28, loss = 0.34732911\n",
            "Iteration 29, loss = 0.34230739\n",
            "Iteration 30, loss = 0.34226382\n",
            "Iteration 31, loss = 0.33421531\n",
            "Iteration 32, loss = 0.33629991\n",
            "Iteration 33, loss = 0.32882033\n",
            "Iteration 34, loss = 0.33520710\n",
            "Iteration 35, loss = 0.32781724\n",
            "Iteration 36, loss = 0.32391360\n",
            "Iteration 37, loss = 0.32402489\n",
            "Iteration 38, loss = 0.30743669\n",
            "Iteration 39, loss = 0.32288360\n",
            "Iteration 40, loss = 0.30884432\n",
            "Iteration 41, loss = 0.31894248\n",
            "Iteration 42, loss = 0.30604221\n",
            "Iteration 43, loss = 0.30086109\n",
            "Iteration 44, loss = 0.30608983\n",
            "Iteration 45, loss = 0.31598063\n",
            "Iteration 46, loss = 0.29243707\n",
            "Iteration 47, loss = 0.30057297\n",
            "Iteration 48, loss = 0.30497983\n",
            "Iteration 49, loss = 0.31263965\n",
            "Iteration 50, loss = 0.28928232\n",
            "Iteration 51, loss = 0.31790877\n",
            "Iteration 52, loss = 0.28411995\n",
            "Iteration 53, loss = 0.28444275\n",
            "Iteration 54, loss = 0.28274780\n",
            "Iteration 55, loss = 0.28347476\n",
            "Iteration 56, loss = 0.29125937\n",
            "Iteration 57, loss = 0.29095450\n",
            "Iteration 58, loss = 0.28801423\n",
            "Iteration 59, loss = 0.28106044\n",
            "Iteration 60, loss = 0.28016289\n",
            "Iteration 61, loss = 0.29284730\n",
            "Iteration 62, loss = 0.28547453\n",
            "Iteration 63, loss = 0.27172709\n",
            "Iteration 64, loss = 0.27648215\n",
            "Iteration 65, loss = 0.26548849\n",
            "Iteration 66, loss = 0.27876725\n",
            "Iteration 67, loss = 0.25912128\n",
            "Iteration 68, loss = 0.26031342\n",
            "Iteration 69, loss = 0.27610467\n",
            "Iteration 70, loss = 0.29214352\n",
            "Iteration 71, loss = 0.25532976\n",
            "Iteration 72, loss = 0.26482269\n",
            "Iteration 73, loss = 0.25531916\n",
            "Iteration 74, loss = 0.25349677\n",
            "Iteration 75, loss = 0.25571126\n",
            "Iteration 76, loss = 0.25022298\n",
            "Iteration 77, loss = 0.25602641\n",
            "Iteration 78, loss = 0.24842718\n",
            "Iteration 79, loss = 0.25812259\n",
            "Iteration 80, loss = 0.24558389\n",
            "Iteration 81, loss = 0.25702858\n",
            "Iteration 82, loss = 0.27171721\n",
            "Iteration 83, loss = 0.26456747\n",
            "Iteration 84, loss = 0.24036984\n",
            "Iteration 85, loss = 0.24239726\n",
            "Iteration 86, loss = 0.26018380\n",
            "Iteration 87, loss = 0.24122619\n",
            "Iteration 88, loss = 0.24407384\n",
            "Iteration 89, loss = 0.24483143\n",
            "Iteration 90, loss = 0.23927851\n",
            "Iteration 91, loss = 0.26366695\n",
            "Iteration 92, loss = 0.23056114\n",
            "Iteration 93, loss = 0.25950691\n",
            "Iteration 94, loss = 0.23772797\n",
            "Iteration 95, loss = 0.23889674\n",
            "Iteration 96, loss = 0.23207428\n",
            "Iteration 97, loss = 0.24830831\n",
            "Iteration 98, loss = 0.23344309\n",
            "Iteration 99, loss = 0.25771263\n",
            "Iteration 100, loss = 0.25225569\n",
            "Iteration 101, loss = 0.23044611\n",
            "Iteration 102, loss = 0.23145786\n",
            "Iteration 103, loss = 0.22529636\n",
            "Iteration 104, loss = 0.24156931\n",
            "Iteration 105, loss = 0.22937288\n",
            "Iteration 106, loss = 0.22488861\n",
            "Iteration 107, loss = 0.22094766\n",
            "Iteration 108, loss = 0.22264423\n",
            "Iteration 109, loss = 0.22250570\n",
            "Iteration 110, loss = 0.22401221\n",
            "Iteration 111, loss = 0.22748085\n",
            "Iteration 112, loss = 0.22475040\n",
            "Iteration 113, loss = 0.21235388\n",
            "Iteration 114, loss = 0.25492367\n",
            "Iteration 115, loss = 0.22926717\n",
            "Iteration 116, loss = 0.20685190\n",
            "Iteration 117, loss = 0.22881414\n",
            "Iteration 118, loss = 0.22103363\n",
            "Iteration 119, loss = 0.34111185\n",
            "Iteration 120, loss = 0.23496054\n",
            "Iteration 121, loss = 0.22728623\n",
            "Iteration 122, loss = 0.22048944\n",
            "Iteration 123, loss = 0.20667545\n",
            "Iteration 124, loss = 0.21416282\n",
            "Iteration 125, loss = 0.22091307\n",
            "Iteration 126, loss = 0.20560494\n",
            "Iteration 127, loss = 0.19932400\n",
            "Iteration 128, loss = 0.21761827\n",
            "Iteration 129, loss = 0.21092105\n",
            "Iteration 130, loss = 0.20489616\n",
            "Iteration 131, loss = 0.20679754\n",
            "Iteration 132, loss = 0.20481056\n",
            "Iteration 133, loss = 0.20718054\n",
            "Iteration 134, loss = 0.22478472\n",
            "Iteration 135, loss = 0.20312877\n",
            "Iteration 136, loss = 0.20763611\n",
            "Iteration 137, loss = 0.19982832\n",
            "Iteration 138, loss = 0.21084811\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29562854\n",
            "Iteration 2, loss = 0.65468674\n",
            "Iteration 3, loss = 0.55293816\n",
            "Iteration 4, loss = 0.50645057\n",
            "Iteration 5, loss = 0.48430024\n",
            "Iteration 6, loss = 0.47915804\n",
            "Iteration 7, loss = 0.44185351\n",
            "Iteration 8, loss = 0.44862963\n",
            "Iteration 9, loss = 0.43807923\n",
            "Iteration 10, loss = 0.41276662\n",
            "Iteration 11, loss = 0.40355915\n",
            "Iteration 12, loss = 0.40792979\n",
            "Iteration 13, loss = 0.39006805\n",
            "Iteration 14, loss = 0.38831771\n",
            "Iteration 15, loss = 0.37745941\n",
            "Iteration 16, loss = 0.37210879\n",
            "Iteration 17, loss = 0.36107664\n",
            "Iteration 18, loss = 0.36460872\n",
            "Iteration 19, loss = 0.35094102\n",
            "Iteration 20, loss = 0.35053506\n",
            "Iteration 21, loss = 0.35082760\n",
            "Iteration 22, loss = 0.34883038\n",
            "Iteration 23, loss = 0.33259075\n",
            "Iteration 24, loss = 0.33671578\n",
            "Iteration 25, loss = 0.33017720\n",
            "Iteration 26, loss = 0.32547326\n",
            "Iteration 27, loss = 0.33255527\n",
            "Iteration 28, loss = 0.31690050\n",
            "Iteration 29, loss = 0.32416152\n",
            "Iteration 30, loss = 0.31915732\n",
            "Iteration 31, loss = 0.31032105\n",
            "Iteration 32, loss = 0.31281517\n",
            "Iteration 33, loss = 0.30691378\n",
            "Iteration 34, loss = 0.31099437\n",
            "Iteration 35, loss = 0.30900444\n",
            "Iteration 36, loss = 0.30032836\n",
            "Iteration 37, loss = 0.30113417\n",
            "Iteration 38, loss = 0.29288916\n",
            "Iteration 39, loss = 0.30855163\n",
            "Iteration 40, loss = 0.28960734\n",
            "Iteration 41, loss = 0.28712054\n",
            "Iteration 42, loss = 0.28671225\n",
            "Iteration 43, loss = 0.29544158\n",
            "Iteration 44, loss = 0.28778126\n",
            "Iteration 45, loss = 0.28762336\n",
            "Iteration 46, loss = 0.28210914\n",
            "Iteration 47, loss = 0.27083205\n",
            "Iteration 48, loss = 0.27731593\n",
            "Iteration 49, loss = 0.27638527\n",
            "Iteration 50, loss = 0.26710170\n",
            "Iteration 51, loss = 0.29308764\n",
            "Iteration 52, loss = 0.26258938\n",
            "Iteration 53, loss = 0.26096640\n",
            "Iteration 54, loss = 0.26440775\n",
            "Iteration 55, loss = 0.27507177\n",
            "Iteration 56, loss = 0.25211502\n",
            "Iteration 57, loss = 0.26345007\n",
            "Iteration 58, loss = 0.24659616\n",
            "Iteration 59, loss = 0.25592957\n",
            "Iteration 60, loss = 0.26316285\n",
            "Iteration 61, loss = 0.25765799\n",
            "Iteration 62, loss = 0.25392921\n",
            "Iteration 63, loss = 0.25259119\n",
            "Iteration 64, loss = 0.25129691\n",
            "Iteration 65, loss = 0.23670321\n",
            "Iteration 66, loss = 0.26530281\n",
            "Iteration 67, loss = 0.23359220\n",
            "Iteration 68, loss = 0.23677233\n",
            "Iteration 69, loss = 0.24204980\n",
            "Iteration 70, loss = 0.23852696\n",
            "Iteration 71, loss = 0.23986243\n",
            "Iteration 72, loss = 0.24246536\n",
            "Iteration 73, loss = 0.24553842\n",
            "Iteration 74, loss = 0.23830903\n",
            "Iteration 75, loss = 0.23181959\n",
            "Iteration 76, loss = 0.23026432\n",
            "Iteration 77, loss = 0.24722955\n",
            "Iteration 78, loss = 0.24342836\n",
            "Iteration 79, loss = 0.23785989\n",
            "Iteration 80, loss = 0.24066023\n",
            "Iteration 81, loss = 0.22926483\n",
            "Iteration 82, loss = 0.22763688\n",
            "Iteration 83, loss = 0.22890801\n",
            "Iteration 84, loss = 0.21692081\n",
            "Iteration 85, loss = 0.23278109\n",
            "Iteration 86, loss = 0.21438042\n",
            "Iteration 87, loss = 0.22053693\n",
            "Iteration 88, loss = 0.23238811\n",
            "Iteration 89, loss = 0.22723284\n",
            "Iteration 90, loss = 0.21417443\n",
            "Iteration 91, loss = 0.21736369\n",
            "Iteration 92, loss = 0.21245655\n",
            "Iteration 93, loss = 0.21919482\n",
            "Iteration 94, loss = 0.20246372\n",
            "Iteration 95, loss = 0.20675653\n",
            "Iteration 96, loss = 0.21941861\n",
            "Iteration 97, loss = 0.20654051\n",
            "Iteration 98, loss = 0.22355612\n",
            "Iteration 99, loss = 0.23296346\n",
            "Iteration 100, loss = 0.20720508\n",
            "Iteration 101, loss = 0.20202868\n",
            "Iteration 102, loss = 0.20745944\n",
            "Iteration 103, loss = 0.21982246\n",
            "Iteration 104, loss = 0.22683605\n",
            "Iteration 105, loss = 0.20916437\n",
            "Iteration 106, loss = 0.20670984\n",
            "Iteration 107, loss = 0.20580508\n",
            "Iteration 108, loss = 0.21820354\n",
            "Iteration 109, loss = 0.20196712\n",
            "Iteration 110, loss = 0.19842446\n",
            "Iteration 111, loss = 0.21009452\n",
            "Iteration 112, loss = 0.19805081\n",
            "Iteration 113, loss = 0.19443793\n",
            "Iteration 114, loss = 0.19013705\n",
            "Iteration 115, loss = 0.18364261\n",
            "Iteration 116, loss = 0.21837172\n",
            "Iteration 117, loss = 0.18379687\n",
            "Iteration 118, loss = 0.18603982\n",
            "Iteration 119, loss = 0.18535488\n",
            "Iteration 120, loss = 0.19897002\n",
            "Iteration 121, loss = 0.20509878\n",
            "Iteration 122, loss = 0.20096213\n",
            "Iteration 123, loss = 0.18853195\n",
            "Iteration 124, loss = 0.19387174\n",
            "Iteration 125, loss = 0.19530666\n",
            "Iteration 126, loss = 0.19253954\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.99812784\n",
            "Iteration 2, loss = 0.54314138\n",
            "Iteration 3, loss = 0.47542468\n",
            "Iteration 4, loss = 0.42907843\n",
            "Iteration 5, loss = 0.40269636\n",
            "Iteration 6, loss = 0.38279144\n",
            "Iteration 7, loss = 0.35736594\n",
            "Iteration 8, loss = 0.34852350\n",
            "Iteration 9, loss = 0.32545331\n",
            "Iteration 10, loss = 0.31388792\n",
            "Iteration 11, loss = 0.29993887\n",
            "Iteration 12, loss = 0.30213345\n",
            "Iteration 13, loss = 0.28345928\n",
            "Iteration 14, loss = 0.27747885\n",
            "Iteration 15, loss = 0.25748809\n",
            "Iteration 16, loss = 0.25606084\n",
            "Iteration 17, loss = 0.24864077\n",
            "Iteration 18, loss = 0.23406634\n",
            "Iteration 19, loss = 0.23126783\n",
            "Iteration 20, loss = 0.22840823\n",
            "Iteration 21, loss = 0.22268352\n",
            "Iteration 22, loss = 0.22739707\n",
            "Iteration 23, loss = 0.22270628\n",
            "Iteration 24, loss = 0.18702194\n",
            "Iteration 25, loss = 0.19046347\n",
            "Iteration 26, loss = 0.18074238\n",
            "Iteration 27, loss = 0.18821168\n",
            "Iteration 28, loss = 0.18553055\n",
            "Iteration 29, loss = 0.16446221\n",
            "Iteration 30, loss = 0.16175133\n",
            "Iteration 31, loss = 0.15069175\n",
            "Iteration 32, loss = 0.17792707\n",
            "Iteration 33, loss = 0.15414432\n",
            "Iteration 34, loss = 0.14939369\n",
            "Iteration 35, loss = 0.13956953\n",
            "Iteration 36, loss = 0.13951200\n",
            "Iteration 37, loss = 0.15520306\n",
            "Iteration 38, loss = 0.13026847\n",
            "Iteration 39, loss = 0.12547029\n",
            "Iteration 40, loss = 0.11706679\n",
            "Iteration 41, loss = 0.13144728\n",
            "Iteration 42, loss = 0.12134395\n",
            "Iteration 43, loss = 0.12741043\n",
            "Iteration 44, loss = 0.12406260\n",
            "Iteration 45, loss = 0.11450955\n",
            "Iteration 46, loss = 0.09861309\n",
            "Iteration 47, loss = 0.10603519\n",
            "Iteration 48, loss = 0.10542270\n",
            "Iteration 49, loss = 0.10081564\n",
            "Iteration 50, loss = 0.09592012\n",
            "Iteration 51, loss = 0.08444771\n",
            "Iteration 52, loss = 0.10587865\n",
            "Iteration 53, loss = 0.08988313\n",
            "Iteration 54, loss = 0.07419375\n",
            "Iteration 55, loss = 0.07257392\n",
            "Iteration 56, loss = 0.07595491\n",
            "Iteration 57, loss = 0.09295072\n",
            "Iteration 58, loss = 0.08738966\n",
            "Iteration 59, loss = 0.07604859\n",
            "Iteration 60, loss = 0.07661370\n",
            "Iteration 61, loss = 0.06269360\n",
            "Iteration 62, loss = 0.07704223\n",
            "Iteration 63, loss = 0.09227750\n",
            "Iteration 64, loss = 0.06304298\n",
            "Iteration 65, loss = 0.06033316\n",
            "Iteration 66, loss = 0.05439322\n",
            "Iteration 67, loss = 0.06146135\n",
            "Iteration 68, loss = 0.05852883\n",
            "Iteration 69, loss = 0.05239806\n",
            "Iteration 70, loss = 0.04799003\n",
            "Iteration 71, loss = 0.04320478\n",
            "Iteration 72, loss = 0.06608951\n",
            "Iteration 73, loss = 0.05187493\n",
            "Iteration 74, loss = 0.03853454\n",
            "Iteration 75, loss = 0.05508627\n",
            "Iteration 76, loss = 0.04346600\n",
            "Iteration 77, loss = 0.04161052\n",
            "Iteration 78, loss = 0.05461932\n",
            "Iteration 79, loss = 0.04995089\n",
            "Iteration 80, loss = 0.04029967\n",
            "Iteration 81, loss = 0.03256572\n",
            "Iteration 82, loss = 0.03109252\n",
            "Iteration 83, loss = 0.03263200\n",
            "Iteration 84, loss = 0.02981054\n",
            "Iteration 85, loss = 0.02512573\n",
            "Iteration 86, loss = 0.05046561\n",
            "Iteration 87, loss = 0.06451371\n",
            "Iteration 88, loss = 0.03345278\n",
            "Iteration 89, loss = 0.02893646\n",
            "Iteration 90, loss = 0.02315180\n",
            "Iteration 91, loss = 0.02824467\n",
            "Iteration 92, loss = 0.04340189\n",
            "Iteration 93, loss = 0.02762833\n",
            "Iteration 94, loss = 0.02045111\n",
            "Iteration 95, loss = 0.01957326\n",
            "Iteration 96, loss = 0.01475296\n",
            "Iteration 97, loss = 0.01781881\n",
            "Iteration 98, loss = 0.02202598\n",
            "Iteration 99, loss = 0.02332664\n",
            "Iteration 100, loss = 0.02342073\n",
            "Iteration 101, loss = 0.01938434\n",
            "Iteration 102, loss = 0.02074551\n",
            "Iteration 103, loss = 0.02032785\n",
            "Iteration 104, loss = 0.01118314\n",
            "Iteration 105, loss = 0.00944141\n",
            "Iteration 106, loss = 0.00910924\n",
            "Iteration 107, loss = 0.00839077\n",
            "Iteration 108, loss = 0.01003493\n",
            "Iteration 109, loss = 0.01032705\n",
            "Iteration 110, loss = 0.00911650\n",
            "Iteration 111, loss = 0.00770338\n",
            "Iteration 112, loss = 0.00813563\n",
            "Iteration 113, loss = 0.00770487\n",
            "Iteration 114, loss = 0.00948110\n",
            "Iteration 115, loss = 0.00859385\n",
            "Iteration 116, loss = 0.00624585\n",
            "Iteration 117, loss = 0.00612311\n",
            "Iteration 118, loss = 0.00561042\n",
            "Iteration 119, loss = 0.02568664\n",
            "Iteration 120, loss = 0.15261998\n",
            "Iteration 121, loss = 0.06696477\n",
            "Iteration 122, loss = 0.02539868\n",
            "Iteration 123, loss = 0.02046628\n",
            "Iteration 124, loss = 0.05660469\n",
            "Iteration 125, loss = 0.02916695\n",
            "Iteration 126, loss = 0.02409295\n",
            "Iteration 127, loss = 0.02123569\n",
            "Iteration 128, loss = 0.01438084\n",
            "Iteration 129, loss = 0.01138021\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.97447892\n",
            "Iteration 2, loss = 0.57345668\n",
            "Iteration 3, loss = 0.49816147\n",
            "Iteration 4, loss = 0.42969094\n",
            "Iteration 5, loss = 0.41108731\n",
            "Iteration 6, loss = 0.38632155\n",
            "Iteration 7, loss = 0.37115563\n",
            "Iteration 8, loss = 0.35200099\n",
            "Iteration 9, loss = 0.32913331\n",
            "Iteration 10, loss = 0.32091941\n",
            "Iteration 11, loss = 0.31356305\n",
            "Iteration 12, loss = 0.30077634\n",
            "Iteration 13, loss = 0.28635145\n",
            "Iteration 14, loss = 0.27586693\n",
            "Iteration 15, loss = 0.26649377\n",
            "Iteration 16, loss = 0.26067695\n",
            "Iteration 17, loss = 0.26755102\n",
            "Iteration 18, loss = 0.27181094\n",
            "Iteration 19, loss = 0.24632820\n",
            "Iteration 20, loss = 0.23540952\n",
            "Iteration 21, loss = 0.23891216\n",
            "Iteration 22, loss = 0.21697190\n",
            "Iteration 23, loss = 0.21595874\n",
            "Iteration 24, loss = 0.20104808\n",
            "Iteration 25, loss = 0.20538823\n",
            "Iteration 26, loss = 0.18812456\n",
            "Iteration 27, loss = 0.19080826\n",
            "Iteration 28, loss = 0.18960561\n",
            "Iteration 29, loss = 0.18203891\n",
            "Iteration 30, loss = 0.17486959\n",
            "Iteration 31, loss = 0.16564604\n",
            "Iteration 32, loss = 0.16121943\n",
            "Iteration 33, loss = 0.15470485\n",
            "Iteration 34, loss = 0.15321747\n",
            "Iteration 35, loss = 0.15254199\n",
            "Iteration 36, loss = 0.14531997\n",
            "Iteration 37, loss = 0.14495270\n",
            "Iteration 38, loss = 0.13142765\n",
            "Iteration 39, loss = 0.14621509\n",
            "Iteration 40, loss = 0.12362125\n",
            "Iteration 41, loss = 0.14924106\n",
            "Iteration 42, loss = 0.12056963\n",
            "Iteration 43, loss = 0.11328450\n",
            "Iteration 44, loss = 0.12052782\n",
            "Iteration 45, loss = 0.10305856\n",
            "Iteration 46, loss = 0.09936785\n",
            "Iteration 47, loss = 0.11156898\n",
            "Iteration 48, loss = 0.12120479\n",
            "Iteration 49, loss = 0.09726012\n",
            "Iteration 50, loss = 0.10063203\n",
            "Iteration 51, loss = 0.08752502\n",
            "Iteration 52, loss = 0.09108852\n",
            "Iteration 53, loss = 0.08478878\n",
            "Iteration 54, loss = 0.10890574\n",
            "Iteration 55, loss = 0.08971255\n",
            "Iteration 56, loss = 0.08487194\n",
            "Iteration 57, loss = 0.08775588\n",
            "Iteration 58, loss = 0.09585714\n",
            "Iteration 59, loss = 0.07797433\n",
            "Iteration 60, loss = 0.07868691\n",
            "Iteration 61, loss = 0.06910333\n",
            "Iteration 62, loss = 0.08691140\n",
            "Iteration 63, loss = 0.07129899\n",
            "Iteration 64, loss = 0.05622345\n",
            "Iteration 65, loss = 0.06241604\n",
            "Iteration 66, loss = 0.09238744\n",
            "Iteration 67, loss = 0.07046484\n",
            "Iteration 68, loss = 0.07394660\n",
            "Iteration 69, loss = 0.06380631\n",
            "Iteration 70, loss = 0.06406549\n",
            "Iteration 71, loss = 0.06199592\n",
            "Iteration 72, loss = 0.07451053\n",
            "Iteration 73, loss = 0.05237926\n",
            "Iteration 74, loss = 0.05294935\n",
            "Iteration 75, loss = 0.05147191\n",
            "Iteration 76, loss = 0.05830247\n",
            "Iteration 77, loss = 0.05131381\n",
            "Iteration 78, loss = 0.04637352\n",
            "Iteration 79, loss = 0.04873327\n",
            "Iteration 80, loss = 0.04190201\n",
            "Iteration 81, loss = 0.03678693\n",
            "Iteration 82, loss = 0.03983005\n",
            "Iteration 83, loss = 0.04308782\n",
            "Iteration 84, loss = 0.04136997\n",
            "Iteration 85, loss = 0.03724766\n",
            "Iteration 86, loss = 0.06831543\n",
            "Iteration 87, loss = 0.04069365\n",
            "Iteration 88, loss = 0.03492496\n",
            "Iteration 89, loss = 0.04178545\n",
            "Iteration 90, loss = 0.03391657\n",
            "Iteration 91, loss = 0.02961718\n",
            "Iteration 92, loss = 0.02658287\n",
            "Iteration 93, loss = 0.02297062\n",
            "Iteration 94, loss = 0.03874246\n",
            "Iteration 95, loss = 0.02480368\n",
            "Iteration 96, loss = 0.02615896\n",
            "Iteration 97, loss = 0.02526769\n",
            "Iteration 98, loss = 0.03053715\n",
            "Iteration 99, loss = 0.03024585\n",
            "Iteration 100, loss = 0.02321516\n",
            "Iteration 101, loss = 0.02092213\n",
            "Iteration 102, loss = 0.02013099\n",
            "Iteration 103, loss = 0.02061623\n",
            "Iteration 104, loss = 0.02063356\n",
            "Iteration 105, loss = 0.01845989\n",
            "Iteration 106, loss = 0.01709574\n",
            "Iteration 107, loss = 0.03084804\n",
            "Iteration 108, loss = 0.01990340\n",
            "Iteration 109, loss = 0.02312184\n",
            "Iteration 110, loss = 0.01576482\n",
            "Iteration 111, loss = 0.06837181\n",
            "Iteration 112, loss = 0.37929096\n",
            "Iteration 113, loss = 0.20810290\n",
            "Iteration 114, loss = 0.11816427\n",
            "Iteration 115, loss = 0.07561949\n",
            "Iteration 116, loss = 0.07227209\n",
            "Iteration 117, loss = 0.06535490\n",
            "Iteration 118, loss = 0.06810665\n",
            "Iteration 119, loss = 0.04568280\n",
            "Iteration 120, loss = 0.03874078\n",
            "Iteration 121, loss = 0.03738847\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.98117678\n",
            "Iteration 2, loss = 0.53713145\n",
            "Iteration 3, loss = 0.47107823\n",
            "Iteration 4, loss = 0.43463378\n",
            "Iteration 5, loss = 0.40158450\n",
            "Iteration 6, loss = 0.37901226\n",
            "Iteration 7, loss = 0.35771539\n",
            "Iteration 8, loss = 0.34085098\n",
            "Iteration 9, loss = 0.32517136\n",
            "Iteration 10, loss = 0.31896084\n",
            "Iteration 11, loss = 0.29946069\n",
            "Iteration 12, loss = 0.29278873\n",
            "Iteration 13, loss = 0.28582233\n",
            "Iteration 14, loss = 0.27279479\n",
            "Iteration 15, loss = 0.25536113\n",
            "Iteration 16, loss = 0.25065141\n",
            "Iteration 17, loss = 0.23974400\n",
            "Iteration 18, loss = 0.24679400\n",
            "Iteration 19, loss = 0.22798983\n",
            "Iteration 20, loss = 0.22042706\n",
            "Iteration 21, loss = 0.22016064\n",
            "Iteration 22, loss = 0.20401016\n",
            "Iteration 23, loss = 0.20625870\n",
            "Iteration 24, loss = 0.19642344\n",
            "Iteration 25, loss = 0.18767046\n",
            "Iteration 26, loss = 0.18214830\n",
            "Iteration 27, loss = 0.18351133\n",
            "Iteration 28, loss = 0.18118628\n",
            "Iteration 29, loss = 0.16160002\n",
            "Iteration 30, loss = 0.16315126\n",
            "Iteration 31, loss = 0.16037088\n",
            "Iteration 32, loss = 0.16650352\n",
            "Iteration 33, loss = 0.14806007\n",
            "Iteration 34, loss = 0.14548825\n",
            "Iteration 35, loss = 0.14410568\n",
            "Iteration 36, loss = 0.12813470\n",
            "Iteration 37, loss = 0.15435174\n",
            "Iteration 38, loss = 0.12679360\n",
            "Iteration 39, loss = 0.12418821\n",
            "Iteration 40, loss = 0.13335466\n",
            "Iteration 41, loss = 0.13606665\n",
            "Iteration 42, loss = 0.11288149\n",
            "Iteration 43, loss = 0.11821546\n",
            "Iteration 44, loss = 0.11530115\n",
            "Iteration 45, loss = 0.10644519\n",
            "Iteration 46, loss = 0.10548715\n",
            "Iteration 47, loss = 0.10515125\n",
            "Iteration 48, loss = 0.10515626\n",
            "Iteration 49, loss = 0.10808268\n",
            "Iteration 50, loss = 0.09899874\n",
            "Iteration 51, loss = 0.09537822\n",
            "Iteration 52, loss = 0.09752291\n",
            "Iteration 53, loss = 0.09729441\n",
            "Iteration 54, loss = 0.09295625\n",
            "Iteration 55, loss = 0.08527127\n",
            "Iteration 56, loss = 0.09094072\n",
            "Iteration 57, loss = 0.08119773\n",
            "Iteration 58, loss = 0.08257207\n",
            "Iteration 59, loss = 0.07444891\n",
            "Iteration 60, loss = 0.07233992\n",
            "Iteration 61, loss = 0.07183746\n",
            "Iteration 62, loss = 0.08913434\n",
            "Iteration 63, loss = 0.08198301\n",
            "Iteration 64, loss = 0.05851476\n",
            "Iteration 65, loss = 0.05362811\n",
            "Iteration 66, loss = 0.06542520\n",
            "Iteration 67, loss = 0.05378546\n",
            "Iteration 68, loss = 0.05790258\n",
            "Iteration 69, loss = 0.05687763\n",
            "Iteration 70, loss = 0.05825721\n",
            "Iteration 71, loss = 0.04796053\n",
            "Iteration 72, loss = 0.04707721\n",
            "Iteration 73, loss = 0.05262976\n",
            "Iteration 74, loss = 0.06411412\n",
            "Iteration 75, loss = 0.04818324\n",
            "Iteration 76, loss = 0.04155882\n",
            "Iteration 77, loss = 0.04400927\n",
            "Iteration 78, loss = 0.03907384\n",
            "Iteration 79, loss = 0.03810345\n",
            "Iteration 80, loss = 0.06121536\n",
            "Iteration 81, loss = 0.06676903\n",
            "Iteration 82, loss = 0.04853088\n",
            "Iteration 83, loss = 0.04103237\n",
            "Iteration 84, loss = 0.03507119\n",
            "Iteration 85, loss = 0.05611516\n",
            "Iteration 86, loss = 0.03710510\n",
            "Iteration 87, loss = 0.03382803\n",
            "Iteration 88, loss = 0.03476710\n",
            "Iteration 89, loss = 0.02802893\n",
            "Iteration 90, loss = 0.03235087\n",
            "Iteration 91, loss = 0.03272111\n",
            "Iteration 92, loss = 0.02798593\n",
            "Iteration 93, loss = 0.03451678\n",
            "Iteration 94, loss = 0.02810036\n",
            "Iteration 95, loss = 0.03351785\n",
            "Iteration 96, loss = 0.03153493\n",
            "Iteration 97, loss = 0.02592979\n",
            "Iteration 98, loss = 0.02260534\n",
            "Iteration 99, loss = 0.02647952\n",
            "Iteration 100, loss = 0.02010451\n",
            "Iteration 101, loss = 0.01677935\n",
            "Iteration 102, loss = 0.01786983\n",
            "Iteration 103, loss = 0.01934891\n",
            "Iteration 104, loss = 0.01533014\n",
            "Iteration 105, loss = 0.02127230\n",
            "Iteration 106, loss = 0.01309668\n",
            "Iteration 107, loss = 0.01156319\n",
            "Iteration 108, loss = 0.01319664\n",
            "Iteration 109, loss = 0.01384707\n",
            "Iteration 110, loss = 0.01485909\n",
            "Iteration 111, loss = 0.01766308\n",
            "Iteration 112, loss = 0.01549587\n",
            "Iteration 113, loss = 0.01287833\n",
            "Iteration 114, loss = 0.01108588\n",
            "Iteration 115, loss = 0.00919723\n",
            "Iteration 116, loss = 0.00889334\n",
            "Iteration 117, loss = 0.00816834\n",
            "Iteration 118, loss = 0.00796432\n",
            "Iteration 119, loss = 0.00632382\n",
            "Iteration 120, loss = 0.00891341\n",
            "Iteration 121, loss = 0.02851925\n",
            "Iteration 122, loss = 0.09146789\n",
            "Iteration 123, loss = 0.03288034\n",
            "Iteration 124, loss = 0.06209216\n",
            "Iteration 125, loss = 0.04978285\n",
            "Iteration 126, loss = 0.01651220\n",
            "Iteration 127, loss = 0.01436571\n",
            "Iteration 128, loss = 0.01036471\n",
            "Iteration 129, loss = 0.01212299\n",
            "Iteration 130, loss = 0.01786948\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.97593986\n",
            "Iteration 2, loss = 0.54126565\n",
            "Iteration 3, loss = 0.46867542\n",
            "Iteration 4, loss = 0.42375677\n",
            "Iteration 5, loss = 0.39473059\n",
            "Iteration 6, loss = 0.37902573\n",
            "Iteration 7, loss = 0.34907282\n",
            "Iteration 8, loss = 0.33772010\n",
            "Iteration 9, loss = 0.32294653\n",
            "Iteration 10, loss = 0.31046736\n",
            "Iteration 11, loss = 0.29471138\n",
            "Iteration 12, loss = 0.29588354\n",
            "Iteration 13, loss = 0.27617486\n",
            "Iteration 14, loss = 0.26166503\n",
            "Iteration 15, loss = 0.26052740\n",
            "Iteration 16, loss = 0.24897210\n",
            "Iteration 17, loss = 0.23233712\n",
            "Iteration 18, loss = 0.23889020\n",
            "Iteration 19, loss = 0.22781664\n",
            "Iteration 20, loss = 0.20964158\n",
            "Iteration 21, loss = 0.20390777\n",
            "Iteration 22, loss = 0.20612056\n",
            "Iteration 23, loss = 0.19641421\n",
            "Iteration 24, loss = 0.19013143\n",
            "Iteration 25, loss = 0.18541116\n",
            "Iteration 26, loss = 0.18300745\n",
            "Iteration 27, loss = 0.16574113\n",
            "Iteration 28, loss = 0.16603591\n",
            "Iteration 29, loss = 0.16405137\n",
            "Iteration 30, loss = 0.16408901\n",
            "Iteration 31, loss = 0.14849011\n",
            "Iteration 32, loss = 0.14361534\n",
            "Iteration 33, loss = 0.14465425\n",
            "Iteration 34, loss = 0.14015637\n",
            "Iteration 35, loss = 0.12876336\n",
            "Iteration 36, loss = 0.13193729\n",
            "Iteration 37, loss = 0.12716805\n",
            "Iteration 38, loss = 0.11964739\n",
            "Iteration 39, loss = 0.11942350\n",
            "Iteration 40, loss = 0.11020418\n",
            "Iteration 41, loss = 0.11825349\n",
            "Iteration 42, loss = 0.10710901\n",
            "Iteration 43, loss = 0.10825947\n",
            "Iteration 44, loss = 0.10657786\n",
            "Iteration 45, loss = 0.09995869\n",
            "Iteration 46, loss = 0.09219909\n",
            "Iteration 47, loss = 0.09396035\n",
            "Iteration 48, loss = 0.09199397\n",
            "Iteration 49, loss = 0.09260813\n",
            "Iteration 50, loss = 0.07677585\n",
            "Iteration 51, loss = 0.12086155\n",
            "Iteration 52, loss = 0.10138600\n",
            "Iteration 53, loss = 0.10186208\n",
            "Iteration 54, loss = 0.08470620\n",
            "Iteration 55, loss = 0.07447754\n",
            "Iteration 56, loss = 0.07260266\n",
            "Iteration 57, loss = 0.07485189\n",
            "Iteration 58, loss = 0.06421728\n",
            "Iteration 59, loss = 0.08118806\n",
            "Iteration 60, loss = 0.06840690\n",
            "Iteration 61, loss = 0.06450326\n",
            "Iteration 62, loss = 0.05997239\n",
            "Iteration 63, loss = 0.05779436\n",
            "Iteration 64, loss = 0.05889082\n",
            "Iteration 65, loss = 0.05397345\n",
            "Iteration 66, loss = 0.05828188\n",
            "Iteration 67, loss = 0.05263734\n",
            "Iteration 68, loss = 0.05451699\n",
            "Iteration 69, loss = 0.05132688\n",
            "Iteration 70, loss = 0.04766793\n",
            "Iteration 71, loss = 0.04221765\n",
            "Iteration 72, loss = 0.04497922\n",
            "Iteration 73, loss = 0.05830158\n",
            "Iteration 74, loss = 0.04617265\n",
            "Iteration 75, loss = 0.05879083\n",
            "Iteration 76, loss = 0.05750093\n",
            "Iteration 77, loss = 0.06668935\n",
            "Iteration 78, loss = 0.03757650\n",
            "Iteration 79, loss = 0.03178429\n",
            "Iteration 80, loss = 0.03329542\n",
            "Iteration 81, loss = 0.03759822\n",
            "Iteration 82, loss = 0.02976094\n",
            "Iteration 83, loss = 0.03894621\n",
            "Iteration 84, loss = 0.03110392\n",
            "Iteration 85, loss = 0.03439314\n",
            "Iteration 86, loss = 0.03346473\n",
            "Iteration 87, loss = 0.02802709\n",
            "Iteration 88, loss = 0.02384634\n",
            "Iteration 89, loss = 0.01990042\n",
            "Iteration 90, loss = 0.02157175\n",
            "Iteration 91, loss = 0.03140223\n",
            "Iteration 92, loss = 0.01921125\n",
            "Iteration 93, loss = 0.02232565\n",
            "Iteration 94, loss = 0.02150041\n",
            "Iteration 95, loss = 0.01758128\n",
            "Iteration 96, loss = 0.01889111\n",
            "Iteration 97, loss = 0.02575126\n",
            "Iteration 98, loss = 0.01989902\n",
            "Iteration 99, loss = 0.01297171\n",
            "Iteration 100, loss = 0.01297845\n",
            "Iteration 101, loss = 0.01159479\n",
            "Iteration 102, loss = 0.01169909\n",
            "Iteration 103, loss = 0.01443488\n",
            "Iteration 104, loss = 0.01354330\n",
            "Iteration 105, loss = 0.01217130\n",
            "Iteration 106, loss = 0.01488470\n",
            "Iteration 107, loss = 0.01090671\n",
            "Iteration 108, loss = 0.00940699\n",
            "Iteration 109, loss = 0.01280643\n",
            "Iteration 110, loss = 0.01035279\n",
            "Iteration 111, loss = 0.00882776\n",
            "Iteration 112, loss = 0.01406441\n",
            "Iteration 113, loss = 0.01480591\n",
            "Iteration 114, loss = 0.03143993\n",
            "Iteration 115, loss = 0.01981027\n",
            "Iteration 116, loss = 0.01270064\n",
            "Iteration 117, loss = 0.01054752\n",
            "Iteration 118, loss = 0.00754117\n",
            "Iteration 119, loss = 0.00761088\n",
            "Iteration 120, loss = 0.01981662\n",
            "Iteration 121, loss = 0.02006772\n",
            "Iteration 122, loss = 0.00962123\n",
            "Iteration 123, loss = 0.00870859\n",
            "Iteration 124, loss = 0.01060283\n",
            "Iteration 125, loss = 0.00766000\n",
            "Iteration 126, loss = 0.00642910\n",
            "Iteration 127, loss = 0.00883242\n",
            "Iteration 128, loss = 0.00727458\n",
            "Iteration 129, loss = 0.00674710\n",
            "Iteration 130, loss = 0.00586679\n",
            "Iteration 131, loss = 0.00498698\n",
            "Iteration 132, loss = 0.00462415\n",
            "Iteration 133, loss = 0.00435029\n",
            "Iteration 134, loss = 0.00425274\n",
            "Iteration 135, loss = 0.00356948\n",
            "Iteration 136, loss = 0.00361966\n",
            "Iteration 137, loss = 0.00468596\n",
            "Iteration 138, loss = 0.00378431\n",
            "Iteration 139, loss = 0.00335684\n",
            "Iteration 140, loss = 0.00330011\n",
            "Iteration 141, loss = 0.00317260\n",
            "Iteration 142, loss = 0.00535248\n",
            "Iteration 143, loss = 0.00401411\n",
            "Iteration 144, loss = 0.00313310\n",
            "Iteration 145, loss = 0.00344332\n",
            "Iteration 146, loss = 0.00274411\n",
            "Iteration 147, loss = 0.00275288\n",
            "Iteration 148, loss = 0.00309148\n",
            "Iteration 149, loss = 0.00285087\n",
            "Iteration 150, loss = 0.00322687\n",
            "Iteration 151, loss = 0.00255871\n",
            "Iteration 152, loss = 0.00242393\n",
            "Iteration 153, loss = 0.00247168\n",
            "Iteration 154, loss = 0.00237886\n",
            "Iteration 155, loss = 0.00242136\n",
            "Iteration 156, loss = 0.00308005\n",
            "Iteration 157, loss = 0.00283772\n",
            "Iteration 158, loss = 0.00227855\n",
            "Iteration 159, loss = 0.00219977\n",
            "Iteration 160, loss = 0.00234173\n",
            "Iteration 161, loss = 0.00232630\n",
            "Iteration 162, loss = 0.00223989\n",
            "Iteration 163, loss = 0.00309912\n",
            "Iteration 164, loss = 0.00230489\n",
            "Iteration 165, loss = 0.00308508\n",
            "Iteration 166, loss = 0.00221729\n",
            "Iteration 167, loss = 0.00216239\n",
            "Iteration 168, loss = 0.00200004\n",
            "Iteration 169, loss = 0.00231647\n",
            "Iteration 170, loss = 0.00198216\n",
            "Iteration 171, loss = 0.00190754\n",
            "Iteration 172, loss = 0.00193720\n",
            "Iteration 173, loss = 0.00185995\n",
            "Iteration 174, loss = 0.00184720\n",
            "Iteration 175, loss = 0.00184324\n",
            "Iteration 176, loss = 0.00185482\n",
            "Iteration 177, loss = 0.00188970\n",
            "Iteration 178, loss = 0.00177212\n",
            "Iteration 179, loss = 0.00179015\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.95544792\n",
            "Iteration 2, loss = 0.54779794\n",
            "Iteration 3, loss = 0.47757886\n",
            "Iteration 4, loss = 0.43547215\n",
            "Iteration 5, loss = 0.40580464\n",
            "Iteration 6, loss = 0.38588859\n",
            "Iteration 7, loss = 0.36265644\n",
            "Iteration 8, loss = 0.34517053\n",
            "Iteration 9, loss = 0.33871202\n",
            "Iteration 10, loss = 0.31831737\n",
            "Iteration 11, loss = 0.30309005\n",
            "Iteration 12, loss = 0.29953181\n",
            "Iteration 13, loss = 0.27994734\n",
            "Iteration 14, loss = 0.27198391\n",
            "Iteration 15, loss = 0.26869810\n",
            "Iteration 16, loss = 0.25678727\n",
            "Iteration 17, loss = 0.25067295\n",
            "Iteration 18, loss = 0.25001005\n",
            "Iteration 19, loss = 0.24384424\n",
            "Iteration 20, loss = 0.22304305\n",
            "Iteration 21, loss = 0.22160489\n",
            "Iteration 22, loss = 0.20853727\n",
            "Iteration 23, loss = 0.20281531\n",
            "Iteration 24, loss = 0.19849580\n",
            "Iteration 25, loss = 0.19330455\n",
            "Iteration 26, loss = 0.19190381\n",
            "Iteration 27, loss = 0.18857493\n",
            "Iteration 28, loss = 0.16732604\n",
            "Iteration 29, loss = 0.16881954\n",
            "Iteration 30, loss = 0.16890430\n",
            "Iteration 31, loss = 0.15855992\n",
            "Iteration 32, loss = 0.14847763\n",
            "Iteration 33, loss = 0.14568925\n",
            "Iteration 34, loss = 0.16036252\n",
            "Iteration 35, loss = 0.14174044\n",
            "Iteration 36, loss = 0.14858229\n",
            "Iteration 37, loss = 0.13211460\n",
            "Iteration 38, loss = 0.12543027\n",
            "Iteration 39, loss = 0.12440099\n",
            "Iteration 40, loss = 0.14004871\n",
            "Iteration 41, loss = 0.13345374\n",
            "Iteration 42, loss = 0.11306669\n",
            "Iteration 43, loss = 0.10967355\n",
            "Iteration 44, loss = 0.11048296\n",
            "Iteration 45, loss = 0.10282802\n",
            "Iteration 46, loss = 0.11178856\n",
            "Iteration 47, loss = 0.10673786\n",
            "Iteration 48, loss = 0.09736045\n",
            "Iteration 49, loss = 0.09359811\n",
            "Iteration 50, loss = 0.08548288\n",
            "Iteration 51, loss = 0.08331591\n",
            "Iteration 52, loss = 0.08362836\n",
            "Iteration 53, loss = 0.08395192\n",
            "Iteration 54, loss = 0.11524164\n",
            "Iteration 55, loss = 0.07670697\n",
            "Iteration 56, loss = 0.07420505\n",
            "Iteration 57, loss = 0.07312257\n",
            "Iteration 58, loss = 0.06856840\n",
            "Iteration 59, loss = 0.06712928\n",
            "Iteration 60, loss = 0.08347685\n",
            "Iteration 61, loss = 0.06857950\n",
            "Iteration 62, loss = 0.08151231\n",
            "Iteration 63, loss = 0.07132799\n",
            "Iteration 64, loss = 0.05971367\n",
            "Iteration 65, loss = 0.05497815\n",
            "Iteration 66, loss = 0.06007787\n",
            "Iteration 67, loss = 0.05759791\n",
            "Iteration 68, loss = 0.04607490\n",
            "Iteration 69, loss = 0.05215556\n",
            "Iteration 70, loss = 0.04885007\n",
            "Iteration 71, loss = 0.04266342\n",
            "Iteration 72, loss = 0.06114662\n",
            "Iteration 73, loss = 0.04616029\n",
            "Iteration 74, loss = 0.05467597\n",
            "Iteration 75, loss = 0.04273790\n",
            "Iteration 76, loss = 0.04976775\n",
            "Iteration 77, loss = 0.07450352\n",
            "Iteration 78, loss = 0.07073252\n",
            "Iteration 79, loss = 0.05109071\n",
            "Iteration 80, loss = 0.04100954\n",
            "Iteration 81, loss = 0.03575360\n",
            "Iteration 82, loss = 0.03703733\n",
            "Iteration 83, loss = 0.03859438\n",
            "Iteration 84, loss = 0.02986001\n",
            "Iteration 85, loss = 0.02834210\n",
            "Iteration 86, loss = 0.05561529\n",
            "Iteration 87, loss = 0.03535554\n",
            "Iteration 88, loss = 0.02648126\n",
            "Iteration 89, loss = 0.03282430\n",
            "Iteration 90, loss = 0.03414484\n",
            "Iteration 91, loss = 0.03340127\n",
            "Iteration 92, loss = 0.02329103\n",
            "Iteration 93, loss = 0.02773521\n",
            "Iteration 94, loss = 0.02358698\n",
            "Iteration 95, loss = 0.01877034\n",
            "Iteration 96, loss = 0.02049798\n",
            "Iteration 97, loss = 0.01835587\n",
            "Iteration 98, loss = 0.02748511\n",
            "Iteration 99, loss = 0.01960648\n",
            "Iteration 100, loss = 0.01621113\n",
            "Iteration 101, loss = 0.02675300\n",
            "Iteration 102, loss = 0.02110115\n",
            "Iteration 103, loss = 0.03132209\n",
            "Iteration 104, loss = 0.02471699\n",
            "Iteration 105, loss = 0.02081605\n",
            "Iteration 106, loss = 0.01594855\n",
            "Iteration 107, loss = 0.01455013\n",
            "Iteration 108, loss = 0.01644349\n",
            "Iteration 109, loss = 0.01428958\n",
            "Iteration 110, loss = 0.01840479\n",
            "Iteration 111, loss = 0.00979722\n",
            "Iteration 112, loss = 0.01225514\n",
            "Iteration 113, loss = 0.00996045\n",
            "Iteration 114, loss = 0.01025715\n",
            "Iteration 115, loss = 0.01237330\n",
            "Iteration 116, loss = 0.01134424\n",
            "Iteration 117, loss = 0.00795180\n",
            "Iteration 118, loss = 0.00676776\n",
            "Iteration 119, loss = 0.00953539\n",
            "Iteration 120, loss = 0.01062162\n",
            "Iteration 121, loss = 0.00729866\n",
            "Iteration 122, loss = 0.00912716\n",
            "Iteration 123, loss = 0.00803674\n",
            "Iteration 124, loss = 0.00722789\n",
            "Iteration 125, loss = 0.00779832\n",
            "Iteration 126, loss = 0.00632424\n",
            "Iteration 127, loss = 0.00571489\n",
            "Iteration 128, loss = 0.00806083\n",
            "Iteration 129, loss = 0.00572835\n",
            "Iteration 130, loss = 0.00590980\n",
            "Iteration 131, loss = 0.00476217\n",
            "Iteration 132, loss = 0.00681210\n",
            "Iteration 133, loss = 0.00462722\n",
            "Iteration 134, loss = 0.00478635\n",
            "Iteration 135, loss = 0.00453175\n",
            "Iteration 136, loss = 0.00598410\n",
            "Iteration 137, loss = 0.00604686\n",
            "Iteration 138, loss = 0.01044584\n",
            "Iteration 139, loss = 0.01033153\n",
            "Iteration 140, loss = 0.00480181\n",
            "Iteration 141, loss = 0.00444050\n",
            "Iteration 142, loss = 0.00597430\n",
            "Iteration 143, loss = 0.00415902\n",
            "Iteration 144, loss = 0.00421681\n",
            "Iteration 145, loss = 0.00689941\n",
            "Iteration 146, loss = 0.00945265\n",
            "Iteration 147, loss = 0.00375161\n",
            "Iteration 148, loss = 0.00348288\n",
            "Iteration 149, loss = 0.00332142\n",
            "Iteration 150, loss = 0.00373121\n",
            "Iteration 151, loss = 0.00377141\n",
            "Iteration 152, loss = 0.00356084\n",
            "Iteration 153, loss = 0.00306022\n",
            "Iteration 154, loss = 0.00291622\n",
            "Iteration 155, loss = 0.00272281\n",
            "Iteration 156, loss = 0.00266165\n",
            "Iteration 157, loss = 0.00256711\n",
            "Iteration 158, loss = 0.00266024\n",
            "Iteration 159, loss = 0.00246518\n",
            "Iteration 160, loss = 0.00241175\n",
            "Iteration 161, loss = 0.00262477\n",
            "Iteration 162, loss = 0.00271039\n",
            "Iteration 163, loss = 0.00236230\n",
            "Iteration 164, loss = 0.00236205\n",
            "Iteration 165, loss = 0.00236235\n",
            "Iteration 166, loss = 0.01796812\n",
            "Iteration 167, loss = 0.01870502\n",
            "Iteration 168, loss = 0.00853578\n",
            "Iteration 169, loss = 0.01841312\n",
            "Iteration 170, loss = 0.95106673\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.71692774\n",
            "Iteration 2, loss = 0.81159046\n",
            "Iteration 3, loss = 0.63994225\n",
            "Iteration 4, loss = 0.58001135\n",
            "Iteration 5, loss = 0.54939075\n",
            "Iteration 6, loss = 0.53217843\n",
            "Iteration 7, loss = 0.49467214\n",
            "Iteration 8, loss = 0.49918285\n",
            "Iteration 9, loss = 0.47966301\n",
            "Iteration 10, loss = 0.47556226\n",
            "Iteration 11, loss = 0.46271081\n",
            "Iteration 12, loss = 0.46060512\n",
            "Iteration 13, loss = 0.43166986\n",
            "Iteration 14, loss = 0.46831947\n",
            "Iteration 15, loss = 0.42559177\n",
            "Iteration 16, loss = 0.42052069\n",
            "Iteration 17, loss = 0.40878736\n",
            "Iteration 18, loss = 0.40256978\n",
            "Iteration 19, loss = 0.41056118\n",
            "Iteration 20, loss = 0.40660165\n",
            "Iteration 21, loss = 0.39683703\n",
            "Iteration 22, loss = 0.40199698\n",
            "Iteration 23, loss = 0.38906705\n",
            "Iteration 24, loss = 0.38982381\n",
            "Iteration 25, loss = 0.38587164\n",
            "Iteration 26, loss = 0.38131944\n",
            "Iteration 27, loss = 0.37082412\n",
            "Iteration 28, loss = 0.39776410\n",
            "Iteration 29, loss = 0.42930150\n",
            "Iteration 30, loss = 0.37133869\n",
            "Iteration 31, loss = 0.36777626\n",
            "Iteration 32, loss = 0.36439811\n",
            "Iteration 33, loss = 0.36375606\n",
            "Iteration 34, loss = 0.36635047\n",
            "Iteration 35, loss = 0.35637199\n",
            "Iteration 36, loss = 0.36354928\n",
            "Iteration 37, loss = 0.36421985\n",
            "Iteration 38, loss = 0.37114252\n",
            "Iteration 39, loss = 0.34685558\n",
            "Iteration 40, loss = 0.34643419\n",
            "Iteration 41, loss = 0.34125994\n",
            "Iteration 42, loss = 0.34369049\n",
            "Iteration 43, loss = 0.34958274\n",
            "Iteration 44, loss = 0.35091018\n",
            "Iteration 45, loss = 0.36102644\n",
            "Iteration 46, loss = 0.33184904\n",
            "Iteration 47, loss = 0.34926257\n",
            "Iteration 48, loss = 0.32240752\n",
            "Iteration 49, loss = 0.35147623\n",
            "Iteration 50, loss = 0.34796788\n",
            "Iteration 51, loss = 0.34779968\n",
            "Iteration 52, loss = 0.32819637\n",
            "Iteration 53, loss = 0.32579246\n",
            "Iteration 54, loss = 0.33720783\n",
            "Iteration 55, loss = 0.32285582\n",
            "Iteration 56, loss = 0.32387748\n",
            "Iteration 57, loss = 0.32760382\n",
            "Iteration 58, loss = 0.31503175\n",
            "Iteration 59, loss = 0.33782321\n",
            "Iteration 60, loss = 0.32790161\n",
            "Iteration 61, loss = 0.32977717\n",
            "Iteration 62, loss = 0.31143342\n",
            "Iteration 63, loss = 0.31638039\n",
            "Iteration 64, loss = 0.31734863\n",
            "Iteration 65, loss = 0.32880928\n",
            "Iteration 66, loss = 0.31552104\n",
            "Iteration 67, loss = 0.32470646\n",
            "Iteration 68, loss = 0.32960846\n",
            "Iteration 69, loss = 0.31251822\n",
            "Iteration 70, loss = 0.31911506\n",
            "Iteration 71, loss = 0.33270397\n",
            "Iteration 72, loss = 0.31321456\n",
            "Iteration 73, loss = 0.31689861\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.74838399\n",
            "Iteration 2, loss = 1.22307970\n",
            "Iteration 3, loss = 0.78062557\n",
            "Iteration 4, loss = 0.81083758\n",
            "Iteration 5, loss = 0.84101977\n",
            "Iteration 6, loss = 0.71499864\n",
            "Iteration 7, loss = 0.63362663\n",
            "Iteration 8, loss = 0.61977025\n",
            "Iteration 9, loss = 0.60979285\n",
            "Iteration 10, loss = 0.58754160\n",
            "Iteration 11, loss = 0.91180926\n",
            "Iteration 12, loss = 0.79571013\n",
            "Iteration 13, loss = 0.71767326\n",
            "Iteration 14, loss = 0.65242270\n",
            "Iteration 15, loss = 0.69311006\n",
            "Iteration 16, loss = 0.62254677\n",
            "Iteration 17, loss = 0.60216229\n",
            "Iteration 18, loss = 0.67846594\n",
            "Iteration 19, loss = 0.59281628\n",
            "Iteration 20, loss = 0.56962421\n",
            "Iteration 21, loss = 0.58831033\n",
            "Iteration 22, loss = 0.57482314\n",
            "Iteration 23, loss = 0.55876738\n",
            "Iteration 24, loss = 0.54566574\n",
            "Iteration 25, loss = 0.57595697\n",
            "Iteration 26, loss = 0.52651310\n",
            "Iteration 27, loss = 0.54344269\n",
            "Iteration 28, loss = 0.65603012\n",
            "Iteration 29, loss = 0.51405836\n",
            "Iteration 30, loss = 0.51247926\n",
            "Iteration 31, loss = 0.52559428\n",
            "Iteration 32, loss = 0.51041042\n",
            "Iteration 33, loss = 0.50678852\n",
            "Iteration 34, loss = 0.50446449\n",
            "Iteration 35, loss = 0.51439903\n",
            "Iteration 36, loss = 0.53538507\n",
            "Iteration 37, loss = 0.48819148\n",
            "Iteration 38, loss = 0.48531891\n",
            "Iteration 39, loss = 0.50565548\n",
            "Iteration 40, loss = 0.49597399\n",
            "Iteration 41, loss = 0.73083398\n",
            "Iteration 42, loss = 0.58339233\n",
            "Iteration 43, loss = 0.56829272\n",
            "Iteration 44, loss = 0.55012143\n",
            "Iteration 45, loss = 0.54485687\n",
            "Iteration 46, loss = 0.52586205\n",
            "Iteration 47, loss = 0.51734620\n",
            "Iteration 48, loss = 0.47529392\n",
            "Iteration 49, loss = 0.46114508\n",
            "Iteration 50, loss = 0.47558788\n",
            "Iteration 51, loss = 0.47304715\n",
            "Iteration 52, loss = 0.48266060\n",
            "Iteration 53, loss = 0.47743096\n",
            "Iteration 54, loss = 0.45432271\n",
            "Iteration 55, loss = 0.48932709\n",
            "Iteration 56, loss = 0.45167570\n",
            "Iteration 57, loss = 0.45586803\n",
            "Iteration 58, loss = 0.46364501\n",
            "Iteration 59, loss = 0.44117646\n",
            "Iteration 60, loss = 0.44267826\n",
            "Iteration 61, loss = 0.45363554\n",
            "Iteration 62, loss = 0.45210841\n",
            "Iteration 63, loss = 0.48309464\n",
            "Iteration 64, loss = 0.44536936\n",
            "Iteration 65, loss = 0.43232890\n",
            "Iteration 66, loss = 0.42233213\n",
            "Iteration 67, loss = 0.44123075\n",
            "Iteration 68, loss = 0.43901306\n",
            "Iteration 69, loss = 0.44996850\n",
            "Iteration 70, loss = 0.46049018\n",
            "Iteration 71, loss = 0.43021752\n",
            "Iteration 72, loss = 0.42401193\n",
            "Iteration 73, loss = 0.44592757\n",
            "Iteration 74, loss = 0.40931287\n",
            "Iteration 75, loss = 0.41831627\n",
            "Iteration 76, loss = 0.41781988\n",
            "Iteration 77, loss = 0.40667481\n",
            "Iteration 78, loss = 0.44496687\n",
            "Iteration 79, loss = 0.42038082\n",
            "Iteration 80, loss = 0.40767485\n",
            "Iteration 81, loss = 0.40658311\n",
            "Iteration 82, loss = 0.39614145\n",
            "Iteration 83, loss = 0.42622140\n",
            "Iteration 84, loss = 0.38862816\n",
            "Iteration 85, loss = 0.42012345\n",
            "Iteration 86, loss = 0.40360572\n",
            "Iteration 87, loss = 0.41825885\n",
            "Iteration 88, loss = 0.39541409\n",
            "Iteration 89, loss = 0.40475232\n",
            "Iteration 90, loss = 0.39197863\n",
            "Iteration 91, loss = 0.38800412\n",
            "Iteration 92, loss = 0.38743916\n",
            "Iteration 93, loss = 0.39156526\n",
            "Iteration 94, loss = 0.38591098\n",
            "Iteration 95, loss = 0.40908728\n",
            "Iteration 96, loss = 0.39041706\n",
            "Iteration 97, loss = 0.39006337\n",
            "Iteration 98, loss = 0.37832064\n",
            "Iteration 99, loss = 0.39675143\n",
            "Iteration 100, loss = 0.38866170\n",
            "Iteration 101, loss = 0.37616572\n",
            "Iteration 102, loss = 0.39687682\n",
            "Iteration 103, loss = 0.38873323\n",
            "Iteration 104, loss = 0.38070221\n",
            "Iteration 105, loss = 0.38639409\n",
            "Iteration 106, loss = 0.37320879\n",
            "Iteration 107, loss = 0.37457556\n",
            "Iteration 108, loss = 0.37660522\n",
            "Iteration 109, loss = 0.40836202\n",
            "Iteration 110, loss = 0.39020946\n",
            "Iteration 111, loss = 0.38628018\n",
            "Iteration 112, loss = 0.37924306\n",
            "Iteration 113, loss = 0.39223970\n",
            "Iteration 114, loss = 0.36537363\n",
            "Iteration 115, loss = 0.38258834\n",
            "Iteration 116, loss = 0.38031254\n",
            "Iteration 117, loss = 0.37384372\n",
            "Iteration 118, loss = 0.37831075\n",
            "Iteration 119, loss = 0.35679819\n",
            "Iteration 120, loss = 0.37024953\n",
            "Iteration 121, loss = 0.36953412\n",
            "Iteration 122, loss = 0.36846190\n",
            "Iteration 123, loss = 0.38117976\n",
            "Iteration 124, loss = 0.37827457\n",
            "Iteration 125, loss = 0.40115314\n",
            "Iteration 126, loss = 0.37072820\n",
            "Iteration 127, loss = 0.35183861\n",
            "Iteration 128, loss = 0.41543637\n",
            "Iteration 129, loss = 0.37796389\n",
            "Iteration 130, loss = 0.36592478\n",
            "Iteration 131, loss = 0.36130726\n",
            "Iteration 132, loss = 0.34930017\n",
            "Iteration 133, loss = 0.34716952\n",
            "Iteration 134, loss = 0.35265190\n",
            "Iteration 135, loss = 0.34546502\n",
            "Iteration 136, loss = 0.38044430\n",
            "Iteration 137, loss = 0.37877088\n",
            "Iteration 138, loss = 0.35308429\n",
            "Iteration 139, loss = 0.36735719\n",
            "Iteration 140, loss = 0.35170563\n",
            "Iteration 141, loss = 0.40441536\n",
            "Iteration 142, loss = 0.36535051\n",
            "Iteration 143, loss = 0.35086854\n",
            "Iteration 144, loss = 0.34917643\n",
            "Iteration 145, loss = 0.37481985\n",
            "Iteration 146, loss = 0.36365320\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.28575512\n",
            "Iteration 2, loss = 2.30491596\n",
            "Iteration 3, loss = 2.30397359\n",
            "Iteration 4, loss = 2.30396858\n",
            "Iteration 5, loss = 2.30398054\n",
            "Iteration 6, loss = 2.30378086\n",
            "Iteration 7, loss = 2.30371208\n",
            "Iteration 8, loss = 2.30388590\n",
            "Iteration 9, loss = 2.30358182\n",
            "Iteration 10, loss = 2.30371086\n",
            "Iteration 11, loss = 2.30341577\n",
            "Iteration 12, loss = 2.30365304\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.80609620\n",
            "Iteration 2, loss = 1.11285336\n",
            "Iteration 3, loss = 0.85785359\n",
            "Iteration 4, loss = 0.78656886\n",
            "Iteration 5, loss = 0.73557045\n",
            "Iteration 6, loss = 0.71167563\n",
            "Iteration 7, loss = 0.67289113\n",
            "Iteration 8, loss = 0.65367100\n",
            "Iteration 9, loss = 0.64128336\n",
            "Iteration 10, loss = 0.62782973\n",
            "Iteration 11, loss = 0.60063693\n",
            "Iteration 12, loss = 0.59776853\n",
            "Iteration 13, loss = 0.58280326\n",
            "Iteration 14, loss = 0.58164361\n",
            "Iteration 15, loss = 0.56303205\n",
            "Iteration 16, loss = 0.58268468\n",
            "Iteration 17, loss = 0.54947915\n",
            "Iteration 18, loss = 0.55287809\n",
            "Iteration 19, loss = 0.53944507\n",
            "Iteration 20, loss = 0.53516272\n",
            "Iteration 21, loss = 0.53304770\n",
            "Iteration 22, loss = 0.52877564\n",
            "Iteration 23, loss = 0.53080871\n",
            "Iteration 24, loss = 0.55897571\n",
            "Iteration 25, loss = 0.52430418\n",
            "Iteration 26, loss = 0.50784031\n",
            "Iteration 27, loss = 0.52284506\n",
            "Iteration 28, loss = 0.52868311\n",
            "Iteration 29, loss = 0.51839276\n",
            "Iteration 30, loss = 0.51642528\n",
            "Iteration 31, loss = 0.53737919\n",
            "Iteration 32, loss = 0.49712012\n",
            "Iteration 33, loss = 0.52335488\n",
            "Iteration 34, loss = 0.49255687\n",
            "Iteration 35, loss = 0.51367327\n",
            "Iteration 36, loss = 0.50121044\n",
            "Iteration 37, loss = 0.50737927\n",
            "Iteration 38, loss = 0.48719360\n",
            "Iteration 39, loss = 0.49559032\n",
            "Iteration 40, loss = 0.49107931\n",
            "Iteration 41, loss = 0.48691613\n",
            "Iteration 42, loss = 0.48029707\n",
            "Iteration 43, loss = 0.47125949\n",
            "Iteration 44, loss = 0.49766563\n",
            "Iteration 45, loss = 0.49792342\n",
            "Iteration 46, loss = 0.48631656\n",
            "Iteration 47, loss = 0.47115526\n",
            "Iteration 48, loss = 0.47064364\n",
            "Iteration 49, loss = 0.47508974\n",
            "Iteration 50, loss = 0.47677117\n",
            "Iteration 51, loss = 0.47528385\n",
            "Iteration 52, loss = 0.47575501\n",
            "Iteration 53, loss = 0.48703927\n",
            "Iteration 54, loss = 0.46049998\n",
            "Iteration 55, loss = 0.46811153\n",
            "Iteration 56, loss = 0.47277657\n",
            "Iteration 57, loss = 0.44644813\n",
            "Iteration 58, loss = 0.46200370\n",
            "Iteration 59, loss = 0.46400895\n",
            "Iteration 60, loss = 0.46073843\n",
            "Iteration 61, loss = 0.47220774\n",
            "Iteration 62, loss = 0.47118372\n",
            "Iteration 63, loss = 0.47556621\n",
            "Iteration 64, loss = 0.47132124\n",
            "Iteration 65, loss = 0.46483503\n",
            "Iteration 66, loss = 0.45723481\n",
            "Iteration 67, loss = 0.44706207\n",
            "Iteration 68, loss = 0.55061318\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.60067882\n",
            "Iteration 2, loss = 0.80818279\n",
            "Iteration 3, loss = 0.65904758\n",
            "Iteration 4, loss = 0.60193084\n",
            "Iteration 5, loss = 0.56105922\n",
            "Iteration 6, loss = 0.53490610\n",
            "Iteration 7, loss = 0.51198289\n",
            "Iteration 8, loss = 0.49259281\n",
            "Iteration 9, loss = 0.48264203\n",
            "Iteration 10, loss = 0.45134316\n",
            "Iteration 11, loss = 0.48048818\n",
            "Iteration 12, loss = 0.43977113\n",
            "Iteration 13, loss = 0.46057947\n",
            "Iteration 14, loss = 0.44237882\n",
            "Iteration 15, loss = 0.43664987\n",
            "Iteration 16, loss = 0.40931332\n",
            "Iteration 17, loss = 0.42352386\n",
            "Iteration 18, loss = 0.40982980\n",
            "Iteration 19, loss = 0.41811761\n",
            "Iteration 20, loss = 0.41284550\n",
            "Iteration 21, loss = 0.39028209\n",
            "Iteration 22, loss = 0.38597265\n",
            "Iteration 23, loss = 0.39808931\n",
            "Iteration 24, loss = 0.38645137\n",
            "Iteration 25, loss = 0.40560617\n",
            "Iteration 26, loss = 0.38183274\n",
            "Iteration 27, loss = 0.37470250\n",
            "Iteration 28, loss = 0.37131082\n",
            "Iteration 29, loss = 0.38091769\n",
            "Iteration 30, loss = 0.37572617\n",
            "Iteration 31, loss = 0.36793960\n",
            "Iteration 32, loss = 0.39882506\n",
            "Iteration 33, loss = 0.37794485\n",
            "Iteration 34, loss = 0.35984680\n",
            "Iteration 35, loss = 0.35141307\n",
            "Iteration 36, loss = 0.36424203\n",
            "Iteration 37, loss = 0.34105040\n",
            "Iteration 38, loss = 0.34529186\n",
            "Iteration 39, loss = 0.37023429\n",
            "Iteration 40, loss = 0.35870767\n",
            "Iteration 41, loss = 0.35314103\n",
            "Iteration 42, loss = 0.36427629\n",
            "Iteration 43, loss = 0.34129571\n",
            "Iteration 44, loss = 0.34197641\n",
            "Iteration 45, loss = 0.34480538\n",
            "Iteration 46, loss = 0.34967410\n",
            "Iteration 47, loss = 0.35854165\n",
            "Iteration 48, loss = 0.33936292\n",
            "Iteration 49, loss = 0.34270785\n",
            "Iteration 50, loss = 0.34617828\n",
            "Iteration 51, loss = 0.33602363\n",
            "Iteration 52, loss = 0.33363608\n",
            "Iteration 53, loss = 0.33105203\n",
            "Iteration 54, loss = 0.33094094\n",
            "Iteration 55, loss = 0.34720747\n",
            "Iteration 56, loss = 0.38264739\n",
            "Iteration 57, loss = 0.33555499\n",
            "Iteration 58, loss = 0.32251241\n",
            "Iteration 59, loss = 0.33594578\n",
            "Iteration 60, loss = 0.31692325\n",
            "Iteration 61, loss = 0.33379837\n",
            "Iteration 62, loss = 0.32668802\n",
            "Iteration 63, loss = 0.32077980\n",
            "Iteration 64, loss = 0.35040847\n",
            "Iteration 65, loss = 0.32881908\n",
            "Iteration 66, loss = 0.31755293\n",
            "Iteration 67, loss = 0.34187512\n",
            "Iteration 68, loss = 0.33081061\n",
            "Iteration 69, loss = 0.29748796\n",
            "Iteration 70, loss = 0.30952109\n",
            "Iteration 71, loss = 0.32179586\n",
            "Iteration 72, loss = 0.33267872\n",
            "Iteration 73, loss = 0.31188578\n",
            "Iteration 74, loss = 0.32966721\n",
            "Iteration 75, loss = 0.31111259\n",
            "Iteration 76, loss = 0.33423549\n",
            "Iteration 77, loss = 0.31971744\n",
            "Iteration 78, loss = 0.32278204\n",
            "Iteration 79, loss = 0.31214890\n",
            "Iteration 80, loss = 0.32800704\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.03664793\n",
            "Iteration 2, loss = 0.54489620\n",
            "Iteration 3, loss = 0.46845955\n",
            "Iteration 4, loss = 0.41378440\n",
            "Iteration 5, loss = 0.38672365\n",
            "Iteration 6, loss = 0.37207835\n",
            "Iteration 7, loss = 0.34647368\n",
            "Iteration 8, loss = 0.33142070\n",
            "Iteration 9, loss = 0.32377978\n",
            "Iteration 10, loss = 0.29938132\n",
            "Iteration 11, loss = 0.29165173\n",
            "Iteration 12, loss = 0.27659207\n",
            "Iteration 13, loss = 0.26454361\n",
            "Iteration 14, loss = 0.25964469\n",
            "Iteration 15, loss = 0.25055360\n",
            "Iteration 16, loss = 0.25897243\n",
            "Iteration 17, loss = 0.22750408\n",
            "Iteration 18, loss = 0.23649166\n",
            "Iteration 19, loss = 0.23004434\n",
            "Iteration 20, loss = 0.20607972\n",
            "Iteration 21, loss = 0.19423379\n",
            "Iteration 22, loss = 0.21445150\n",
            "Iteration 23, loss = 0.21254538\n",
            "Iteration 24, loss = 0.18767729\n",
            "Iteration 25, loss = 0.17154807\n",
            "Iteration 26, loss = 0.19417021\n",
            "Iteration 27, loss = 0.17679909\n",
            "Iteration 28, loss = 0.16449784\n",
            "Iteration 29, loss = 0.16117132\n",
            "Iteration 30, loss = 0.15437023\n",
            "Iteration 31, loss = 0.15620003\n",
            "Iteration 32, loss = 0.14067858\n",
            "Iteration 33, loss = 0.13456984\n",
            "Iteration 34, loss = 0.14691902\n",
            "Iteration 35, loss = 0.15270597\n",
            "Iteration 36, loss = 0.14055310\n",
            "Iteration 37, loss = 0.12883371\n",
            "Iteration 38, loss = 0.11137795\n",
            "Iteration 39, loss = 0.10160087\n",
            "Iteration 40, loss = 0.11913735\n",
            "Iteration 41, loss = 0.12027643\n",
            "Iteration 42, loss = 0.11936235\n",
            "Iteration 43, loss = 0.12930060\n",
            "Iteration 44, loss = 0.09905468\n",
            "Iteration 45, loss = 0.10577310\n",
            "Iteration 46, loss = 0.09882355\n",
            "Iteration 47, loss = 0.10718706\n",
            "Iteration 48, loss = 0.08981630\n",
            "Iteration 49, loss = 0.07275114\n",
            "Iteration 50, loss = 0.14892960\n",
            "Iteration 51, loss = 0.09918911\n",
            "Iteration 52, loss = 0.10121098\n",
            "Iteration 53, loss = 0.08271402\n",
            "Iteration 54, loss = 0.07901456\n",
            "Iteration 55, loss = 0.07865127\n",
            "Iteration 56, loss = 0.09356925\n",
            "Iteration 57, loss = 0.07002726\n",
            "Iteration 58, loss = 0.09532265\n",
            "Iteration 59, loss = 0.10463405\n",
            "Iteration 60, loss = 0.09885038\n",
            "Iteration 61, loss = 0.07200757\n",
            "Iteration 62, loss = 0.06663484\n",
            "Iteration 63, loss = 0.06303397\n",
            "Iteration 64, loss = 0.07551826\n",
            "Iteration 65, loss = 0.09182767\n",
            "Iteration 66, loss = 0.08750092\n",
            "Iteration 67, loss = 0.09677464\n",
            "Iteration 68, loss = 0.06417382\n",
            "Iteration 69, loss = 0.05406403\n",
            "Iteration 70, loss = 0.05696121\n",
            "Iteration 71, loss = 0.08829649\n",
            "Iteration 72, loss = 0.07052669\n",
            "Iteration 73, loss = 0.05630446\n",
            "Iteration 74, loss = 0.04195267\n",
            "Iteration 75, loss = 0.04332983\n",
            "Iteration 76, loss = 0.04636744\n",
            "Iteration 77, loss = 0.05189056\n",
            "Iteration 78, loss = 0.04032389\n",
            "Iteration 79, loss = 0.05672004\n",
            "Iteration 80, loss = 0.06131346\n",
            "Iteration 81, loss = 0.04152908\n",
            "Iteration 82, loss = 0.06216717\n",
            "Iteration 83, loss = 0.05896604\n",
            "Iteration 84, loss = 0.06224958\n",
            "Iteration 85, loss = 0.04560774\n",
            "Iteration 86, loss = 0.04094275\n",
            "Iteration 87, loss = 0.03265619\n",
            "Iteration 88, loss = 0.03247779\n",
            "Iteration 89, loss = 0.03778668\n",
            "Iteration 90, loss = 0.02991989\n",
            "Iteration 91, loss = 0.04593240\n",
            "Iteration 92, loss = 0.04224855\n",
            "Iteration 93, loss = 0.03541722\n",
            "Iteration 94, loss = 0.03199780\n",
            "Iteration 95, loss = 0.02761050\n",
            "Iteration 96, loss = 0.02233501\n",
            "Iteration 97, loss = 0.12406428\n",
            "Iteration 98, loss = 0.13604748\n",
            "Iteration 99, loss = 0.05889288\n",
            "Iteration 100, loss = 0.05003906\n",
            "Iteration 101, loss = 0.02926373\n",
            "Iteration 102, loss = 0.02738592\n",
            "Iteration 103, loss = 0.02058969\n",
            "Iteration 104, loss = 0.02069310\n",
            "Iteration 105, loss = 0.02926164\n",
            "Iteration 106, loss = 0.03805690\n",
            "Iteration 107, loss = 0.02365870\n",
            "Iteration 108, loss = 0.02606879\n",
            "Iteration 109, loss = 0.02508169\n",
            "Iteration 110, loss = 0.09377877\n",
            "Iteration 111, loss = 0.03960260\n",
            "Iteration 112, loss = 0.02369974\n",
            "Iteration 113, loss = 0.01976769\n",
            "Iteration 114, loss = 0.01640936\n",
            "Iteration 115, loss = 0.01167536\n",
            "Iteration 116, loss = 0.00756632\n",
            "Iteration 117, loss = 0.00453681\n",
            "Iteration 118, loss = 0.00286586\n",
            "Iteration 119, loss = 0.00362754\n",
            "Iteration 120, loss = 0.00377644\n",
            "Iteration 121, loss = 0.00312147\n",
            "Iteration 122, loss = 0.00213075\n",
            "Iteration 123, loss = 0.05395574\n",
            "Iteration 124, loss = 0.07814235\n",
            "Iteration 125, loss = 0.04310137\n",
            "Iteration 126, loss = 0.02044736\n",
            "Iteration 127, loss = 0.01368577\n",
            "Iteration 128, loss = 0.02115152\n",
            "Iteration 129, loss = 0.01455150\n",
            "Iteration 130, loss = 0.01048176\n",
            "Iteration 131, loss = 0.00551794\n",
            "Iteration 132, loss = 0.00428632\n",
            "Iteration 133, loss = 0.00389403\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.01189438\n",
            "Iteration 2, loss = 0.55364785\n",
            "Iteration 3, loss = 0.47756735\n",
            "Iteration 4, loss = 0.43007552\n",
            "Iteration 5, loss = 0.40061701\n",
            "Iteration 6, loss = 0.37203231\n",
            "Iteration 7, loss = 0.34993832\n",
            "Iteration 8, loss = 0.33503565\n",
            "Iteration 9, loss = 0.31814851\n",
            "Iteration 10, loss = 0.31344412\n",
            "Iteration 11, loss = 0.31315396\n",
            "Iteration 12, loss = 0.28659898\n",
            "Iteration 13, loss = 0.28170644\n",
            "Iteration 14, loss = 0.26819084\n",
            "Iteration 15, loss = 0.26083124\n",
            "Iteration 16, loss = 0.25574716\n",
            "Iteration 17, loss = 0.23667407\n",
            "Iteration 18, loss = 0.23394629\n",
            "Iteration 19, loss = 0.22392864\n",
            "Iteration 20, loss = 0.21475920\n",
            "Iteration 21, loss = 0.21070644\n",
            "Iteration 22, loss = 0.21374570\n",
            "Iteration 23, loss = 0.21308487\n",
            "Iteration 24, loss = 0.18481471\n",
            "Iteration 25, loss = 0.18850018\n",
            "Iteration 26, loss = 0.17022185\n",
            "Iteration 27, loss = 0.18508613\n",
            "Iteration 28, loss = 0.17510326\n",
            "Iteration 29, loss = 0.16014426\n",
            "Iteration 30, loss = 0.16226580\n",
            "Iteration 31, loss = 0.15668033\n",
            "Iteration 32, loss = 0.14053797\n",
            "Iteration 33, loss = 0.15100378\n",
            "Iteration 34, loss = 0.14285251\n",
            "Iteration 35, loss = 0.14348064\n",
            "Iteration 36, loss = 0.12958319\n",
            "Iteration 37, loss = 0.12062133\n",
            "Iteration 38, loss = 0.13517972\n",
            "Iteration 39, loss = 0.12798565\n",
            "Iteration 40, loss = 0.12777523\n",
            "Iteration 41, loss = 0.12258073\n",
            "Iteration 42, loss = 0.10242518\n",
            "Iteration 43, loss = 0.11455140\n",
            "Iteration 44, loss = 0.13096227\n",
            "Iteration 45, loss = 0.11288256\n",
            "Iteration 46, loss = 0.10220123\n",
            "Iteration 47, loss = 0.09640485\n",
            "Iteration 48, loss = 0.09600792\n",
            "Iteration 49, loss = 0.15662152\n",
            "Iteration 50, loss = 0.15907689\n",
            "Iteration 51, loss = 0.22349626\n",
            "Iteration 52, loss = 0.19100350\n",
            "Iteration 53, loss = 0.13559706\n",
            "Iteration 54, loss = 0.09732224\n",
            "Iteration 55, loss = 0.09700803\n",
            "Iteration 56, loss = 0.09232777\n",
            "Iteration 57, loss = 0.07736096\n",
            "Iteration 58, loss = 0.10564516\n",
            "Iteration 59, loss = 0.08134510\n",
            "Iteration 60, loss = 0.09166656\n",
            "Iteration 61, loss = 0.08035593\n",
            "Iteration 62, loss = 0.06367351\n",
            "Iteration 63, loss = 0.06983918\n",
            "Iteration 64, loss = 0.07407740\n",
            "Iteration 65, loss = 0.09161358\n",
            "Iteration 66, loss = 0.07177301\n",
            "Iteration 67, loss = 0.11472001\n",
            "Iteration 68, loss = 0.07934122\n",
            "Iteration 69, loss = 0.09340430\n",
            "Iteration 70, loss = 0.08054483\n",
            "Iteration 71, loss = 0.08176003\n",
            "Iteration 72, loss = 0.07927666\n",
            "Iteration 73, loss = 0.05777182\n",
            "Iteration 74, loss = 0.05314245\n",
            "Iteration 75, loss = 0.06369385\n",
            "Iteration 76, loss = 0.05679631\n",
            "Iteration 77, loss = 0.05320661\n",
            "Iteration 78, loss = 0.05889026\n",
            "Iteration 79, loss = 0.04909801\n",
            "Iteration 80, loss = 0.05702327\n",
            "Iteration 81, loss = 0.04209357\n",
            "Iteration 82, loss = 0.03636234\n",
            "Iteration 83, loss = 0.05673567\n",
            "Iteration 84, loss = 0.05102624\n",
            "Iteration 85, loss = 0.03717949\n",
            "Iteration 86, loss = 0.05229870\n",
            "Iteration 87, loss = 0.05806413\n",
            "Iteration 88, loss = 0.06747783\n",
            "Iteration 89, loss = 0.06337462\n",
            "Iteration 90, loss = 0.05514399\n",
            "Iteration 91, loss = 0.10134057\n",
            "Iteration 92, loss = 0.05983159\n",
            "Iteration 93, loss = 0.05285943\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.05322778\n",
            "Iteration 2, loss = 0.56522686\n",
            "Iteration 3, loss = 0.48962708\n",
            "Iteration 4, loss = 0.43703248\n",
            "Iteration 5, loss = 0.39574604\n",
            "Iteration 6, loss = 0.38309785\n",
            "Iteration 7, loss = 0.35812325\n",
            "Iteration 8, loss = 0.34755652\n",
            "Iteration 9, loss = 0.32826876\n",
            "Iteration 10, loss = 0.32168487\n",
            "Iteration 11, loss = 0.30200561\n",
            "Iteration 12, loss = 0.28895371\n",
            "Iteration 13, loss = 0.28111656\n",
            "Iteration 14, loss = 0.27181516\n",
            "Iteration 15, loss = 0.25941110\n",
            "Iteration 16, loss = 0.25811087\n",
            "Iteration 17, loss = 0.23451351\n",
            "Iteration 18, loss = 0.24060245\n",
            "Iteration 19, loss = 0.22933298\n",
            "Iteration 20, loss = 0.22162564\n",
            "Iteration 21, loss = 0.22089407\n",
            "Iteration 22, loss = 0.21394996\n",
            "Iteration 23, loss = 0.20897788\n",
            "Iteration 24, loss = 0.18910480\n",
            "Iteration 25, loss = 0.19499115\n",
            "Iteration 26, loss = 0.19510513\n",
            "Iteration 27, loss = 0.19217285\n",
            "Iteration 28, loss = 0.17142111\n",
            "Iteration 29, loss = 0.19010594\n",
            "Iteration 30, loss = 0.16385767\n",
            "Iteration 31, loss = 0.15511898\n",
            "Iteration 32, loss = 0.16556736\n",
            "Iteration 33, loss = 0.14744412\n",
            "Iteration 34, loss = 0.16206880\n",
            "Iteration 35, loss = 0.17805148\n",
            "Iteration 36, loss = 0.13957695\n",
            "Iteration 37, loss = 0.13060401\n",
            "Iteration 38, loss = 0.12979511\n",
            "Iteration 39, loss = 0.15582801\n",
            "Iteration 40, loss = 0.14568822\n",
            "Iteration 41, loss = 0.11735345\n",
            "Iteration 42, loss = 0.11954735\n",
            "Iteration 43, loss = 0.11993367\n",
            "Iteration 44, loss = 0.10906883\n",
            "Iteration 45, loss = 0.10105277\n",
            "Iteration 46, loss = 0.11072523\n",
            "Iteration 47, loss = 0.09946241\n",
            "Iteration 48, loss = 0.09378591\n",
            "Iteration 49, loss = 0.09716271\n",
            "Iteration 50, loss = 0.10520436\n",
            "Iteration 51, loss = 0.12769651\n",
            "Iteration 52, loss = 0.10042085\n",
            "Iteration 53, loss = 0.11174742\n",
            "Iteration 54, loss = 0.09929088\n",
            "Iteration 55, loss = 0.08665115\n",
            "Iteration 56, loss = 0.09875502\n",
            "Iteration 57, loss = 0.07986264\n",
            "Iteration 58, loss = 0.09126368\n",
            "Iteration 59, loss = 0.08145008\n",
            "Iteration 60, loss = 0.07220422\n",
            "Iteration 61, loss = 0.07780541\n",
            "Iteration 62, loss = 0.06848543\n",
            "Iteration 63, loss = 0.08239842\n",
            "Iteration 64, loss = 0.06543930\n",
            "Iteration 65, loss = 0.06532577\n",
            "Iteration 66, loss = 0.07976579\n",
            "Iteration 67, loss = 0.08020937\n",
            "Iteration 68, loss = 0.06682129\n",
            "Iteration 69, loss = 0.04829235\n",
            "Iteration 70, loss = 0.05317936\n",
            "Iteration 71, loss = 0.05219065\n",
            "Iteration 72, loss = 0.04345783\n",
            "Iteration 73, loss = 0.05201610\n",
            "Iteration 74, loss = 0.05041015\n",
            "Iteration 75, loss = 0.04974858\n",
            "Iteration 76, loss = 0.05054941\n",
            "Iteration 77, loss = 0.06484356\n",
            "Iteration 78, loss = 0.07947373\n",
            "Iteration 79, loss = 0.05058027\n",
            "Iteration 80, loss = 0.03635870\n",
            "Iteration 81, loss = 0.07938595\n",
            "Iteration 82, loss = 0.04768012\n",
            "Iteration 83, loss = 0.03174296\n",
            "Iteration 84, loss = 0.03150149\n",
            "Iteration 85, loss = 0.03686531\n",
            "Iteration 86, loss = 0.05519426\n",
            "Iteration 87, loss = 0.05749983\n",
            "Iteration 88, loss = 0.05642232\n",
            "Iteration 89, loss = 0.03941418\n",
            "Iteration 90, loss = 0.09352707\n",
            "Iteration 91, loss = 0.07633134\n",
            "Iteration 92, loss = 0.04360109\n",
            "Iteration 93, loss = 0.04174074\n",
            "Iteration 94, loss = 0.02429757\n",
            "Iteration 95, loss = 0.02967718\n",
            "Iteration 96, loss = 0.02998122\n",
            "Iteration 97, loss = 0.04224377\n",
            "Iteration 98, loss = 0.02505432\n",
            "Iteration 99, loss = 0.03240831\n",
            "Iteration 100, loss = 0.05942834\n",
            "Iteration 101, loss = 0.03474221\n",
            "Iteration 102, loss = 0.02623456\n",
            "Iteration 103, loss = 0.03154731\n",
            "Iteration 104, loss = 0.03101006\n",
            "Iteration 105, loss = 0.02824603\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.00418550\n",
            "Iteration 2, loss = 0.52921170\n",
            "Iteration 3, loss = 0.46966559\n",
            "Iteration 4, loss = 0.42083829\n",
            "Iteration 5, loss = 0.37972608\n",
            "Iteration 6, loss = 0.35954324\n",
            "Iteration 7, loss = 0.34525826\n",
            "Iteration 8, loss = 0.32866570\n",
            "Iteration 9, loss = 0.30704237\n",
            "Iteration 10, loss = 0.29995733\n",
            "Iteration 11, loss = 0.28194858\n",
            "Iteration 12, loss = 0.27190902\n",
            "Iteration 13, loss = 0.26153720\n",
            "Iteration 14, loss = 0.24946237\n",
            "Iteration 15, loss = 0.23730991\n",
            "Iteration 16, loss = 0.24179091\n",
            "Iteration 17, loss = 0.23573913\n",
            "Iteration 18, loss = 0.21102834\n",
            "Iteration 19, loss = 0.21027248\n",
            "Iteration 20, loss = 0.21269537\n",
            "Iteration 21, loss = 0.20057052\n",
            "Iteration 22, loss = 0.18283978\n",
            "Iteration 23, loss = 0.18576780\n",
            "Iteration 24, loss = 0.17481489\n",
            "Iteration 25, loss = 0.17525084\n",
            "Iteration 26, loss = 0.16951839\n",
            "Iteration 27, loss = 0.15910944\n",
            "Iteration 28, loss = 0.15286611\n",
            "Iteration 29, loss = 0.14263667\n",
            "Iteration 30, loss = 0.14398562\n",
            "Iteration 31, loss = 0.14497341\n",
            "Iteration 32, loss = 0.12677258\n",
            "Iteration 33, loss = 0.12997809\n",
            "Iteration 34, loss = 0.13782252\n",
            "Iteration 35, loss = 0.13076363\n",
            "Iteration 36, loss = 0.13450416\n",
            "Iteration 37, loss = 0.11814238\n",
            "Iteration 38, loss = 0.11443985\n",
            "Iteration 39, loss = 0.11218898\n",
            "Iteration 40, loss = 0.13426707\n",
            "Iteration 41, loss = 0.10149910\n",
            "Iteration 42, loss = 0.09100393\n",
            "Iteration 43, loss = 0.10632986\n",
            "Iteration 44, loss = 0.10923328\n",
            "Iteration 45, loss = 0.08511500\n",
            "Iteration 46, loss = 0.09060121\n",
            "Iteration 47, loss = 0.15555275\n",
            "Iteration 48, loss = 0.12202318\n",
            "Iteration 49, loss = 0.09810940\n",
            "Iteration 50, loss = 0.09128833\n",
            "Iteration 51, loss = 0.08729514\n",
            "Iteration 52, loss = 0.07698969\n",
            "Iteration 53, loss = 0.07394604\n",
            "Iteration 54, loss = 0.09677425\n",
            "Iteration 55, loss = 0.11721649\n",
            "Iteration 56, loss = 0.07578321\n",
            "Iteration 57, loss = 0.06507695\n",
            "Iteration 58, loss = 0.06697977\n",
            "Iteration 59, loss = 0.05753389\n",
            "Iteration 60, loss = 0.07152010\n",
            "Iteration 61, loss = 0.06096426\n",
            "Iteration 62, loss = 0.05461093\n",
            "Iteration 63, loss = 0.05873080\n",
            "Iteration 64, loss = 0.04678766\n",
            "Iteration 65, loss = 0.09489498\n",
            "Iteration 66, loss = 0.07141050\n",
            "Iteration 67, loss = 0.06850402\n",
            "Iteration 68, loss = 0.04810332\n",
            "Iteration 69, loss = 0.04361089\n",
            "Iteration 70, loss = 0.04525492\n",
            "Iteration 71, loss = 0.04299901\n",
            "Iteration 72, loss = 0.06246179\n",
            "Iteration 73, loss = 0.06723367\n",
            "Iteration 74, loss = 0.04700892\n",
            "Iteration 75, loss = 0.06270100\n",
            "Iteration 76, loss = 0.04567523\n",
            "Iteration 77, loss = 0.07112612\n",
            "Iteration 78, loss = 0.06576610\n",
            "Iteration 79, loss = 0.04981002\n",
            "Iteration 80, loss = 0.04959404\n",
            "Iteration 81, loss = 0.03547652\n",
            "Iteration 82, loss = 0.03423064\n",
            "Iteration 83, loss = 0.04526746\n",
            "Iteration 84, loss = 0.03209089\n",
            "Iteration 85, loss = 0.01970149\n",
            "Iteration 86, loss = 0.05875750\n",
            "Iteration 87, loss = 0.04881003\n",
            "Iteration 88, loss = 0.04517032\n",
            "Iteration 89, loss = 0.04006670\n",
            "Iteration 90, loss = 0.04002027\n",
            "Iteration 91, loss = 0.02248988\n",
            "Iteration 92, loss = 0.03249673\n",
            "Iteration 93, loss = 0.05685985\n",
            "Iteration 94, loss = 0.03093793\n",
            "Iteration 95, loss = 0.02992049\n",
            "Iteration 96, loss = 0.01979461\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.08233825\n",
            "Iteration 2, loss = 0.58562122\n",
            "Iteration 3, loss = 0.49261625\n",
            "Iteration 4, loss = 0.44601666\n",
            "Iteration 5, loss = 0.40649401\n",
            "Iteration 6, loss = 0.38140630\n",
            "Iteration 7, loss = 0.36968635\n",
            "Iteration 8, loss = 0.35249097\n",
            "Iteration 9, loss = 0.33244437\n",
            "Iteration 10, loss = 0.32347347\n",
            "Iteration 11, loss = 0.30305400\n",
            "Iteration 12, loss = 0.29155766\n",
            "Iteration 13, loss = 0.28048325\n",
            "Iteration 14, loss = 0.27197121\n",
            "Iteration 15, loss = 0.26302511\n",
            "Iteration 16, loss = 0.25645048\n",
            "Iteration 17, loss = 0.24918959\n",
            "Iteration 18, loss = 0.23496127\n",
            "Iteration 19, loss = 0.24014961\n",
            "Iteration 20, loss = 0.23029009\n",
            "Iteration 21, loss = 0.22768095\n",
            "Iteration 22, loss = 0.22027920\n",
            "Iteration 23, loss = 0.20726728\n",
            "Iteration 24, loss = 0.19378115\n",
            "Iteration 25, loss = 0.21162788\n",
            "Iteration 26, loss = 0.19490901\n",
            "Iteration 27, loss = 0.17824429\n",
            "Iteration 28, loss = 0.18266273\n",
            "Iteration 29, loss = 0.15588046\n",
            "Iteration 30, loss = 0.16203478\n",
            "Iteration 31, loss = 0.15888993\n",
            "Iteration 32, loss = 0.13775142\n",
            "Iteration 33, loss = 0.15813240\n",
            "Iteration 34, loss = 0.13868569\n",
            "Iteration 35, loss = 0.13355473\n",
            "Iteration 36, loss = 0.13611578\n",
            "Iteration 37, loss = 0.14723429\n",
            "Iteration 38, loss = 0.13996579\n",
            "Iteration 39, loss = 0.13002590\n",
            "Iteration 40, loss = 0.13454246\n",
            "Iteration 41, loss = 0.12748772\n",
            "Iteration 42, loss = 0.12190184\n",
            "Iteration 43, loss = 0.11421403\n",
            "Iteration 44, loss = 0.11756526\n",
            "Iteration 45, loss = 0.11610891\n",
            "Iteration 46, loss = 0.10040270\n",
            "Iteration 47, loss = 0.08928341\n",
            "Iteration 48, loss = 0.09354063\n",
            "Iteration 49, loss = 0.09612558\n",
            "Iteration 50, loss = 0.09575255\n",
            "Iteration 51, loss = 0.07771195\n",
            "Iteration 52, loss = 0.07611247\n",
            "Iteration 53, loss = 0.18437556\n",
            "Iteration 54, loss = 0.16618205\n",
            "Iteration 55, loss = 0.11504942\n",
            "Iteration 56, loss = 0.10540044\n",
            "Iteration 57, loss = 0.07954506\n",
            "Iteration 58, loss = 0.07391757\n",
            "Iteration 59, loss = 0.05749436\n",
            "Iteration 60, loss = 0.09290374\n",
            "Iteration 61, loss = 0.08636359\n",
            "Iteration 62, loss = 0.05404991\n",
            "Iteration 63, loss = 0.06518190\n",
            "Iteration 64, loss = 0.06168609\n",
            "Iteration 65, loss = 0.06268399\n",
            "Iteration 66, loss = 0.06488584\n",
            "Iteration 67, loss = 0.07358841\n",
            "Iteration 68, loss = 0.05815587\n",
            "Iteration 69, loss = 0.04641396\n",
            "Iteration 70, loss = 0.04746971\n",
            "Iteration 71, loss = 0.04038307\n",
            "Iteration 72, loss = 0.07757143\n",
            "Iteration 73, loss = 0.08152244\n",
            "Iteration 74, loss = 0.06170953\n",
            "Iteration 75, loss = 0.07374638\n",
            "Iteration 76, loss = 0.05388308\n",
            "Iteration 77, loss = 0.04940051\n",
            "Iteration 78, loss = 0.10444809\n",
            "Iteration 79, loss = 0.06941488\n",
            "Iteration 80, loss = 0.07164347\n",
            "Iteration 81, loss = 0.05525546\n",
            "Iteration 82, loss = 0.05441584\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.96184370\n",
            "Iteration 2, loss = 0.51794994\n",
            "Iteration 3, loss = 0.44278488\n",
            "Iteration 4, loss = 0.41018336\n",
            "Iteration 5, loss = 0.38202896\n",
            "Iteration 6, loss = 0.35457201\n",
            "Iteration 7, loss = 0.34288648\n",
            "Iteration 8, loss = 0.32741808\n",
            "Iteration 9, loss = 0.31123967\n",
            "Iteration 10, loss = 0.30584390\n",
            "Iteration 11, loss = 0.29227363\n",
            "Iteration 12, loss = 0.26933827\n",
            "Iteration 13, loss = 0.27563948\n",
            "Iteration 14, loss = 0.25722025\n",
            "Iteration 15, loss = 0.24499045\n",
            "Iteration 16, loss = 0.23935356\n",
            "Iteration 17, loss = 0.22657080\n",
            "Iteration 18, loss = 0.22691961\n",
            "Iteration 19, loss = 0.21815552\n",
            "Iteration 20, loss = 0.21124349\n",
            "Iteration 21, loss = 0.20207777\n",
            "Iteration 22, loss = 0.19022920\n",
            "Iteration 23, loss = 0.19441852\n",
            "Iteration 24, loss = 0.18985109\n",
            "Iteration 25, loss = 0.17665885\n",
            "Iteration 26, loss = 0.17013155\n",
            "Iteration 27, loss = 0.18380580\n",
            "Iteration 28, loss = 0.16886201\n",
            "Iteration 29, loss = 0.16584900\n",
            "Iteration 30, loss = 0.14628652\n",
            "Iteration 31, loss = 0.14967559\n",
            "Iteration 32, loss = 0.15423686\n",
            "Iteration 33, loss = 0.15051467\n",
            "Iteration 34, loss = 0.14848641\n",
            "Iteration 35, loss = 0.13790729\n",
            "Iteration 36, loss = 0.13518823\n",
            "Iteration 37, loss = 0.12048435\n",
            "Iteration 38, loss = 0.12498619\n",
            "Iteration 39, loss = 0.11892766\n",
            "Iteration 40, loss = 0.12658316\n",
            "Iteration 41, loss = 0.11063313\n",
            "Iteration 42, loss = 0.13302315\n",
            "Iteration 43, loss = 0.14070107\n",
            "Iteration 44, loss = 0.12289886\n",
            "Iteration 45, loss = 0.11611824\n",
            "Iteration 46, loss = 0.10749886\n",
            "Iteration 47, loss = 0.11680426\n",
            "Iteration 48, loss = 0.11058550\n",
            "Iteration 49, loss = 0.10017302\n",
            "Iteration 50, loss = 0.11453874\n",
            "Iteration 51, loss = 0.09943209\n",
            "Iteration 52, loss = 0.12097713\n",
            "Iteration 53, loss = 0.08391487\n",
            "Iteration 54, loss = 0.08287024\n",
            "Iteration 55, loss = 0.07402341\n",
            "Iteration 56, loss = 0.08490524\n",
            "Iteration 57, loss = 0.07284352\n",
            "Iteration 58, loss = 0.07363683\n",
            "Iteration 59, loss = 0.09975303\n",
            "Iteration 60, loss = 0.07389322\n",
            "Iteration 61, loss = 0.07624700\n",
            "Iteration 62, loss = 0.08250016\n",
            "Iteration 63, loss = 0.07067836\n",
            "Iteration 64, loss = 0.06736970\n",
            "Iteration 65, loss = 0.08911631\n",
            "Iteration 66, loss = 0.08939739\n",
            "Iteration 67, loss = 0.08908288\n",
            "Iteration 68, loss = 0.07892052\n",
            "Iteration 69, loss = 0.07836312\n",
            "Iteration 70, loss = 0.05127174\n",
            "Iteration 71, loss = 0.06115624\n",
            "Iteration 72, loss = 0.07304104\n",
            "Iteration 73, loss = 0.06024249\n",
            "Iteration 74, loss = 0.08718208\n",
            "Iteration 75, loss = 0.05040245\n",
            "Iteration 76, loss = 0.06063247\n",
            "Iteration 77, loss = 0.07403450\n",
            "Iteration 78, loss = 0.05714805\n",
            "Iteration 79, loss = 0.05637360\n",
            "Iteration 80, loss = 0.05510566\n",
            "Iteration 81, loss = 0.06083923\n",
            "Iteration 82, loss = 0.06350324\n",
            "Iteration 83, loss = 0.04932641\n",
            "Iteration 84, loss = 0.05439241\n",
            "Iteration 85, loss = 0.05070417\n",
            "Iteration 86, loss = 0.03656696\n",
            "Iteration 87, loss = 0.02689754\n",
            "Iteration 88, loss = 0.04922142\n",
            "Iteration 89, loss = 0.06600462\n",
            "Iteration 90, loss = 0.06905124\n",
            "Iteration 91, loss = 0.04763535\n",
            "Iteration 92, loss = 0.03708773\n",
            "Iteration 93, loss = 0.03811742\n",
            "Iteration 94, loss = 0.08060794\n",
            "Iteration 95, loss = 0.09813820\n",
            "Iteration 96, loss = 0.05069672\n",
            "Iteration 97, loss = 0.03894891\n",
            "Iteration 98, loss = 0.03729998\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=MLPClassifier(hidden_layer_sizes={'hidden_layer_sizes': [(10,),\n",
              "                                                                                (50,),\n",
              "                                                                                (10,\n",
              "                                                                                 10),\n",
              "                                                                                (50,\n",
              "                                                                                 50)]},\n",
              "                                     learning_rate_init=0.1, max_iter=300,\n",
              "                                     random_state=2082157, solver='sgd',\n",
              "                                     verbose=True),\n",
              "             param_grid={'hidden_layer_sizes': [(10,), (50,), (10, 10),\n",
              "                                                (50, 50)]})"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "#for NN we try the same architectures as before\n",
        "hl_parameters_10000 = {'hidden_layer_sizes': [(10,), (50,), (10,10,), (50,50,)]}\n",
        "\n",
        "#MPLClassifier model definition (m_training = 10000): default activation function = ReLu\n",
        "mlp_classifier_10000 = MLPClassifier(hl_parameters_10000, max_iter=300, alpha=1e-4, solver='sgd', tol=1e-4, learning_rate_init=.1, verbose=True, random_state=ID)\n",
        "\n",
        "#find best model using 5-fold CV and train it using all the training data\n",
        "mlp_large_cv = GridSearchCV(mlp_classifier_10000, hl_parameters_10000, cv=5, return_train_score=False)\n",
        "\n",
        "#fit the grid search with training data\n",
        "mlp_large_cv.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7nZpfQvrzIU",
        "outputId": "67b7a058-5046-41e9-ee48-82c392ff66a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best MLP classifier estimator (m_training = 10000):\n",
            " MLPClassifier(hidden_layer_sizes=(50, 50), learning_rate_init=0.1, max_iter=300,\n",
            "              random_state=2082157, solver='sgd', verbose=True)\n",
            "\n",
            "RESULTS FOR NN (m_training = 10000):\n",
            "\n",
            "Best parameters set found:\n",
            "{'hidden_layer_sizes': (50, 50)}\n",
            "\n",
            "Score with best parameters:\n",
            "0.8484\n",
            "\n",
            "All scores on the grid:\n",
            "Parameters: [{'hidden_layer_sizes': (10,)}, {'hidden_layer_sizes': (50,)}, {'hidden_layer_sizes': (10, 10)}, {'hidden_layer_sizes': (50, 50)}]\n",
            "Scores: [0.8029 0.8161 0.6481 0.8484]\n"
          ]
        }
      ],
      "source": [
        "#best MPL classifier (m_training = 10000): best_estimator_\n",
        "best_mlp_large_classifier = mlp_large_cv.best_estimator_\n",
        "print(\"Best MLP classifier estimator (m_training = 10000):\\n \"+str(best_mlp_large_classifier))\n",
        "\n",
        "print ('\\nRESULTS FOR NN (m_training = 10000):\\n')\n",
        "\n",
        "print(\"Best parameters set found:\")\n",
        "print(mlp_large_cv.best_params_)\n",
        "\n",
        "print(\"\\nScore with best parameters:\")\n",
        "print(mlp_large_cv.best_score_)\n",
        "\n",
        "print(\"\\nAll scores on the grid:\")\n",
        "print(\"Parameters: \"+str(mlp_large_cv.cv_results_['params']))\n",
        "print(\"Scores: \"+str(mlp_large_cv.cv_results_['mean_test_score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR09gdpBrzIV"
      },
      "source": [
        "## TO DO 5\n",
        "Describe your architecture choices and the results you observe with respect to the architectures you used.\n",
        "\n",
        "**ANSWER [David Polzoni]**: The architecture choices I made for the MLP classifier were the number of hidden layers and the number of units in each hidden layer. The results of the grid search indicate that the best performing architecture was a neural network with two hidden layers, each containing 50 units (for $m_{training} = 10000$). This configuration achieved the highest score of 0.8484. Other architectures, that were considered, were a single hidden layer model with 10 or 50 units and a two hidden layers model each with 10 units. The single hidden layer model with 50 units achieved a score of 0.8161, which is slightly lower than the best architecture. The model with two hidden layers of 10 units each had a much lower score, particularly when the training set was smaller ($m_{training} = 500$). In this particular case, the best architecture was a two hidden layers model, but with a small amount of data, a single hidden layer model works better. In the case of the model trained with a larger amount of data, the scores obtained are higher w.r.t. the MLP classifier trained on 500 data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_28cucvArzIW"
      },
      "source": [
        "## TO DO 6\n",
        "\n",
        "Get the train and test error for the best NN you obtained with 10000 points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWgOsjJwrzIW",
        "outputId": "2558910d-fdb4-4980-f160-cbcd26cb2f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULTS FOR BEST NN (m_training = 10000):\n",
            "\n",
            "Best NN training error: 0.019800\n",
            "Best NN test error: 0.152540\n"
          ]
        }
      ],
      "source": [
        "#get training and test error for the best NN model from CV\n",
        "\n",
        "#should be equal to best_mlp_large_classifier defined previously!\n",
        "best_mlp_classifier_10000 = mlp_large_cv.best_estimator_\n",
        "\n",
        "#compute training and test error for best_mlp_classifier_10000\n",
        "training_error_mlp_10000 = 1. - best_mlp_classifier_10000.score(X_train, y_train)\n",
        "test_error_mlp_10000 = 1. - best_mlp_classifier_10000.score(X_test, y_test)\n",
        "\n",
        "print ('RESULTS FOR BEST NN (m_training = 10000):\\n')\n",
        "\n",
        "print (\"Best NN training error: %f\" % training_error_mlp_10000)\n",
        "print (\"Best NN test error: %f\" % test_error_mlp_10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X5ZmmorrzIY"
      },
      "source": [
        "## TO DO 7\n",
        "\n",
        "Compare the train and test error you got with a large number of samples with the best one you obtained with only 500 data points. Are the architectures the same or do they differ? What about the errors you get?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjnDSyRIrzIY"
      },
      "source": [
        "[ADD YOUR ANSWER HERE]\n",
        "\n",
        "**ANSWER [David Polzoni]**:\n",
        "In the case of $m_{training} = 500$, the best NN architecture was a single hidden layer model with 50 units.\n",
        "* training error: 0;\n",
        "* test error: 0.219076.\n",
        "\n",
        "While in the case of $m_{training} = 10000$, the best NN architecture was a two hidden layers model, each containing 50 units.\n",
        "* training error: 0.0198;\n",
        "* test error: 0.152540.\n",
        "\n",
        "Based on the scores obtained, the two hidden layer architecture might be more suitable for this classification problem when enough data is available.\n",
        "The main difference between the two errors is that the test error is much higher when the amount of data available for training is small ($m_{training} = 500$) as compared to when there is a large amount of data available ($m_{training} = 10000$). This is likely due to the fact that the model is overfitting to the training data when there is a small amount of data available, as it is too complex to be trained with such a small quantity of data. In conclusion, the performance of the model increases with the increase in the amount of data available for training. And also, the model with a two hidden layers architecture performs better than a single hidden layer architecture when enough data is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37l2KLm9rzIZ"
      },
      "source": [
        "## TO DO 8\n",
        "\n",
        "Plot an image that was misclassified by NN with m=500 training data points and it is now instead correctly classified by NN with m=10000 training data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "GKWD2se1rzIZ",
        "outputId": "3d7f0b3c-bac7-4a4b-9be4-71eb9870875b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LABEL: 3\n",
            "\n",
            "MLP classifier (m_training = 500) prediction: [8], true label: 3\n",
            "MLP classifier (m_training = 10000) prediction: [3], true label: 3\n"
          ]
        }
      ],
      "source": [
        "#find the index of a point that is misclassified by best_mlp_classifier_500 and correctly classified by best_mlp_classifier_10000\n",
        "misclassified_index = None\n",
        "for idx, (x_t, y_t) in enumerate(zip(X_test, y_test)):\n",
        "  prediction_NN_500 = best_mlp_classifier_500.predict(x_t.reshape(1, -1))\n",
        "  prediction_NN_10000 = best_mlp_classifier_10000.predict(x_t.reshape(1, -1))\n",
        "  if prediction_NN_500 != y_t and prediction_NN_10000 == y_t:\n",
        "    misclassified_index = idx\n",
        "    break\n",
        "\n",
        "#plot the image with the selected index and compare the predictions by the two NNs\n",
        "if misclassified_index is not None:\n",
        "  plot_input(X_test, y_test, misclassified_index)\n",
        "  print(f\"\\nMLP classifier (m_training = 500) prediction: {prediction_NN_500}, true label: {y_t}\")\n",
        "  print(f\"MLP classifier (m_training = 10000) prediction: {prediction_NN_10000}, true label: {y_t}\")\n",
        "else:\n",
        "  print(\"No misclassified points found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL9GDwcqrzIa"
      },
      "source": [
        "Let's plot some of the weigths of the multi-layer perceptron classifier, for the best NN we get with 500 data points and with 10000 data points. The code below plots the weights in a matrix form, where a figure represents all the weights of the edges entering in a hidden node. Notice that the code assumes that the NNs are called \"mlp\" and \"best_mlp_large\": you may need to replace such variables with your variable names. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "IexRvyyMrzIb",
        "outputId": "b3ed51e1-883e-43f8-f439-2b813755522e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights with 500 data points:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights with 10000 data points:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"Weights with 500 data points:\")\n",
        "\n",
        "fig, axes = plt.subplots(4, 4)\n",
        "vmin, vmax = best_mlp_classifier_500.coefs_[0].min(), best_mlp_classifier_500.coefs_[0].max()\n",
        "for coef, ax in zip(best_mlp_classifier_500.coefs_[0].T, axes.ravel()):\n",
        "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5*vmin, vmax=.5*vmax)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Weights with 10000 data points:\")\n",
        "\n",
        "fig, axes = plt.subplots(4, 4)\n",
        "vmin, vmax = best_mlp_classifier_10000.coefs_[0].min(), best_mlp_classifier_10000.coefs_[0].max()\n",
        "for coef, ax in zip(best_mlp_classifier_10000.coefs_[0].T, axes.ravel()):\n",
        "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5*vmin, vmax=.5*vmax)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feS-KJZmrzIc"
      },
      "source": [
        "## TO DO 9\n",
        "\n",
        "Describe what do you observe by looking at the weights.\n",
        "\n",
        "[ADD YOUR ANSWER HERE]\n",
        "\n",
        "**ANSWER [David Polzoni]**: In this case, the neural network trained with fewer data samples ($m_{training} = 500$) seems trained to be more specific in its recognition task, compared to the neural network created with a larger number of data samples ($m_{training} = 10000$). This make the weights of the first neural network easier to interpret for the human eye, as (probably) they have been optimized to recognize a more limited number of objects or features. Additionally, the interpretability of a neural network by the human eye also depends on its architecture and the number of layers and units per layer. For example, a neural network with a higher number of layers may be less interpretable than a neural network with a lower number of layers (as in this case)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN0q8LVLrzIc"
      },
      "source": [
        "## TO DO 10\n",
        "\n",
        "Pick another classifier among the ones we have seen previously (SVM or something else). Report the training and test error for such classifier with 10000 samples in the training set, if possible; if the classifier cannot run with so many data sample reduce the number of samples.\n",
        "\n",
        "*Note*: if there are parameters to be optimized use cross-validation. If you choose SVM, you can decide if you want to use a single kernel or use the best among many; in the latter case, you need to pick the best kernel using cross-validation (using the functions available in sklearn).\n",
        "\n",
        "**[WRITE HERE WHAT CLASSIFIER YOU ARE CHOOSING AND WHAT PARAMETERS YOU NEED TO SET]**\n",
        "\n",
        "**ANSWER [David Polzoni]**:\n",
        "\n",
        "**Classifier**: logistic regression model;\n",
        "\n",
        "**Parameters to set**:\n",
        "* *penalty*: this parameter specifies the type of regularization to use (for our purposes, the default value for this parameter is fine);\n",
        "* *C*: key parameter which specifies the inverse of the regularization strength. A smaller value of C means a stronger regularization, which can help to reduce overfitting. This parameter is the one to be optimized using cross-validation;\n",
        "* *solver*: this parameter specifies the algorithm to use for optimization (for our purposes, the default value for this parameter is fine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bxgmTvSrzId",
        "outputId": "57d04e3d-64dc-4533-9432-a5562f70256a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULTS FOR OTHER CLASSIFIER (Logistic Regression):\n",
            "\n",
            "Best training error (other model): 0.087900\n",
            "Best test error (other model): 0.159680\n"
          ]
        }
      ],
      "source": [
        "#classifier: logistic regression\n",
        "#parameter to set: penalty, C, solver\n",
        "\n",
        "#load the required packages\n",
        "from sklearn import linear_model\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "#simple filter to ignore ConvergenceWarning (introduced only for 'printing' purposes)\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "#use 5-fold CV to find the best choice of the parameter, than train the model on the entire training set\n",
        "log_regression_cv = linear_model.LogisticRegressionCV(cv=5, random_state=ID).fit(X_train, y_train)\n",
        "\n",
        "#compute training and test error for the log_regression model with the optimized parameter\n",
        "training_error_log_regression_cv = 1. - log_regression_cv.score(X_train, y_train)\n",
        "test_error_log_regression_cv = 1. - log_regression_cv.score(X_test, y_test)\n",
        "\n",
        "print('RESULTS FOR OTHER CLASSIFIER (Logistic Regression):\\n')\n",
        "\n",
        "print(\"Best training error (other model): %f\" % training_error_log_regression_cv)\n",
        "print(\"Best test error (other model): %f\" % test_error_log_regression_cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUZwYXJerzId"
      },
      "source": [
        "## TO DO 11\n",
        "Compare the results of NN and of the other classifier you have chosen above. Which classifier would you preferer? Provide a brief explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfu8tvczrzIe"
      },
      "source": [
        "[ADD YOUR ANSWER HERE]\n",
        "\n",
        "**ANSWER [David Polzoni]**: Based on the results above, it appears that the neural network trained on the larger training set has the lowest test error of the three models. This suggests that this neural network may be the best performing model for this particular classification task. However, logistic regression model is much simpler than a neural network model. In addition, the performance obtained does not differ much from that of the neural network cited above. For this specific task, the best choice, in terms of trade-off between model complexity and performances, is the previously mentioned logistic regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm603J0orzIe"
      },
      "source": [
        "# Clustering with K-means\n",
        "\n",
        "Clustering is a useful technique for *unsupervised* learning. We are now going to cluster 2000 images in the fashion MNIST dataset, and try to understand if the clusters we obtain correspond to the true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fC9wgWm_rzIe"
      },
      "outputs": [],
      "source": [
        "#load the required packages\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DIIMGMWrzIf"
      },
      "source": [
        "(Note that the code below assumes that the data has already been transformed as in the NN part of the notebook, so make sure to run the code for the transformation even if you do not complete the part on NN)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "da8P_VhfrzIf"
      },
      "outputs": [],
      "source": [
        "#let's consider only 2000 data points\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "\n",
        "m_training = 2000\n",
        "\n",
        "X_train, X_test = X[:m_training], X[m_training:]\n",
        "y_train, y_test = y[:m_training], y[m_training:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMk090Q9rzIg"
      },
      "source": [
        "## TO DO 12\n",
        "Cluster the points using the KMeans() and fit() functions (see the userguide for details). For Kmeans, set: n_clusters=10 as number of clusters; n_init=10 as the number of times the algorithm will be run with different centroid seeds; random_state=ID. You can use the default setting for the other parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f2xe6s4rzIg",
        "outputId": "6b0d88a5-4189-40e9-cd1a-39f9c0c49aec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(n_clusters=10, random_state=2082157)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "#KMeans: default settings for other parameters (e.g. algorithm='lloyd')\n",
        "kmeans = KMeans(n_clusters=10, n_init=10, random_state=ID)\n",
        "\n",
        "#fit the model to the data\n",
        "kmeans.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvNnSaGMrzIg"
      },
      "source": [
        "## Comparison of clusters with true labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM4vcRgfrzIh"
      },
      "source": [
        "## TO DO 13\n",
        "Now compare the obtained clusters with the true labels, using the function sklearn.metrics.cluster.contingency_matrix() (see the userguide for details). The function prints a matrix $A$ such that entry $A_{i,j}$ is is the number of samples in true class $i$ and in predicted class $j$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "ZvYL9oRkrzIh",
        "outputId": "1e8df373-e23b-47c5-85ff-bd3e65543e43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c617f0850>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "#compute and print the contingency matrix for the true labels vs the clustering assignments\n",
        "#use seaborn to plot \"nicely\" the contingency matrix\n",
        "cm = contingency_matrix(y_train, kmeans.labels_)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sn.heatmap(cm, annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEDHlfyzrzIi"
      },
      "source": [
        "## TO DO 14\n",
        "Based on the matrix shown above, comment on the results of clustering in terms of adherence to the true labels.\n",
        "\n",
        "[ADD YOUR ANSWER HERE]\n",
        "\n",
        "**ANSWER [David Polzoni]**: In a contingency matrix, each row corresponds to a true class and each column corresponds to a predicted class. If the clustering algorithm is able to accurately group the observations into clusters that correspond to the true classes, then the elements of the contingency matrix will be mostly on the diagonal, indicating that the true and predicted classes are the same for most observations. However, in this specific case, there are many elements off the diagonal, indicating that the predicted classes do not match the true classes for many observations. This suggests that the clustering algorithm is not performing well in terms of adherence to the true labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcEEqWAKrzIi"
      },
      "source": [
        "## Choice of k with silhoutte coefficient\n",
        "In many real applications it is unclear what is the correct value of $k$ to use. In practice one tries different values of $k$ and then uses some external score to choose a value of $k$. One such score is the silhoutte coefficient, that can be computed with metrics.silhouette_score(). See the definition of the silhoutte coefficient in the userguide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uTSG91TrzIi"
      },
      "source": [
        "## TO DO 15\n",
        "Compute the clustering for $k=2,3,...,15$ (other parameters as above) and print the silhoutte coefficient for each such clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USu2zBKwrzIj",
        "outputId": "b3b19aca-da58-497a-89fe-30c0ca0d9b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhoutte coefficient for number of clusters=2: 0.19528765431552908\n",
            "Silhoutte coefficient for number of clusters=3: 0.19173193790632329\n",
            "Silhoutte coefficient for number of clusters=4: 0.18987906065269963\n",
            "Silhoutte coefficient for number of clusters=5: 0.16869861516762905\n",
            "Silhoutte coefficient for number of clusters=6: 0.1597955091688836\n",
            "Silhoutte coefficient for number of clusters=7: 0.17039933579401262\n",
            "Silhoutte coefficient for number of clusters=8: 0.15811723022613663\n",
            "Silhoutte coefficient for number of clusters=9: 0.14256145235202858\n",
            "Silhoutte coefficient for number of clusters=10: 0.1475960831426483\n",
            "Silhoutte coefficient for number of clusters=11: 0.14205719321387736\n",
            "Silhoutte coefficient for number of clusters=12: 0.14077731069437044\n",
            "Silhoutte coefficient for number of clusters=13: 0.13449065081223313\n",
            "Silhoutte coefficient for number of clusters=14: 0.1328616154387225\n",
            "Silhoutte coefficient for number of clusters=15: 0.12186468475593568\n"
          ]
        }
      ],
      "source": [
        "#import silhoutte_score from sklearn.metrics\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "#run k-means with 10 choices of initial centroids for a range of values of n_clusters\n",
        "list_of_silhouttes = []\n",
        "for k in range(2, 16):\n",
        "    kmeans_k = KMeans(n_clusters=k, n_init=10, random_state=ID)\n",
        "    kmeans_k.fit(X_train)\n",
        "    silhoutte_score = silhouette_score(X_train, kmeans_k.labels_)\n",
        "    list_of_silhouttes.append(silhoutte_score)\n",
        "    print(\"Silhoutte coefficient for number of clusters=\"+str(k)+\": \"+str(silhoutte_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-z_4RdxrzIj"
      },
      "source": [
        "Let us now plot how the silhouette coefficients change with respect to $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "oEyDUA1SrzIj",
        "outputId": "07f5717c-4c66-4e04-c15c-5cb7721d2911"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Silhouette score per number of clusters')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot([k for k in range(2, 16)], list_of_silhouttes, \"o-\")\n",
        "plt.gca().set_ylim(bottom=0)\n",
        "plt.title(\"Silhouette score per number of clusters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-BbXh6SrzIk"
      },
      "source": [
        "## TO DO 16\n",
        "\n",
        "Based on the silhoutte score, which $k$ would you pick? Motivate your choice. Does your choice match what you know about the data? If yes, explain why you think this is the case; if no, explain what you think may be the reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6SgDnKGrzIk"
      },
      "source": [
        "[ADD YOUR ANSWER HERE]\n",
        "\n",
        "**ANSWER [David Polzoni]**: Based on the silhoutte scores above, it appears that the clustering algorithm is not performing particularly well for this classification task. The scores are relatively low, with the highest score being around 0.195 for $k=2$ clusters. In general, a silhoutte score close to 1 indicates that the clusters are well-separated and each cluster contains similar observations, while a score close to -1 indicates that the clusters are poorly separated and overlap significantly. A score close to 0 indicates that the clusters are neither well separated nor overlapped. The value of k that would give the best performance is $k=2$, as it has the highest silhouette score among all the values of $k$ that were tried. However, this number of clusters is significantly different from the number of true labels in the data. Infact, as we know that there are a specific number of true labels in the data and that number is considerably different from 2 (e.g. in this case, there are 10 true labels in the data), then it may be the case that the silhouette score is not the most appropriate metric to use to evaluate the performance of the clustering algorithm."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}